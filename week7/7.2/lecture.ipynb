{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"tocheading\">Speed Layer</h1>\n",
    "> The speed layer compensates for the high latency of the batch layer to enable up-to-date results for queries.  \n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives\n",
    "===============================================================\n",
    "\n",
    "By the end of today's lesson, you will be able to:  \n",
    "\n",
    "* Enqueue page-views in Kafka\n",
    "* Dedupe and normalize using Spark Streaming\n",
    "* Store Pageviews over time in HBase\n",
    "* Expire data in HBase as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realtime views\n",
    "===============================================================\n",
    "\n",
    "> This section covers:\n",
    ">  \n",
    "* The theoretical model of the speed layer\n",
    "* How the batch layer eases the responsibilities of the speed layer\n",
    "* Using random-write databases for realtime views\n",
    "* The CAP theorem and its implications\n",
    "* The challenges of incremental computation\n",
    "* Expiring data from the speed layer\n",
    "\n",
    "![Figure 12.1](images/12fig01_alt.jpg)\n",
    "\n",
    "> **The speed layer allows the Lambda Architecture to serve low-latency queries over up-to-date data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing realtime views\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### Strategy: realtime view = function(recent data)\n",
    "\n",
    "![Figure 12.2](images/12fig02_alt.jpg)\n",
    "\n",
    "### Incremental strategy: realtime view = function(new data, previous realtime view)\n",
    "\n",
    "![Figure 12.3](images/12fig03_alt.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing realtime views\n",
    "---------------------------------------------------------------------\n",
    "> \n",
    "* *Random reads*—A realtime view should support fast random reads to answer queries quickly. This means the data it contains must be indexed.\n",
    "* *Random writes*—To support incremental algorithms, it must also be possible to modify a realtime view with low latency.\n",
    "* *Scalability*—As with the serving layer views, the realtime views should scale with the amount of data they store and the read/write rates required by the applica- tion. Typically this implies that realtime views can be distributed across many machines.\n",
    "* *Fault tolerance*—If a disk or a machine crashes, a realtime view should continue to function normally. Fault tolerance is accomplished by replicating data across machines so there are backups should a single machine fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eventual accuracy\n",
    "\n",
    "> Because all data is eventually represented in the batch and serving layer views, any approximations you make in the speed layer are continually corrected.\n",
    "\n",
    "### Amount of state stored in the speed layer\n",
    "> \n",
    "* *Online compaction*—As a read/write database receives updates, parts of the disk index become unused, wasted space. Periodically the database must perform compaction to reclaim space. Compaction is a resource-intensive process and could potentially starve the machine of resources needed to rapidly serve queries. Improper manage- ment of compaction can cause a cascading failure of the entire cluster.\n",
    "* *Concurrency*—A read/write database can potentially receive many reads or writes for the same value at the same time. It therefore needs to coordinate these reads and writes to prevent returning stale or inconsistent values. Sharing mutable state across threads is a notoriously complex problem, and control strategies such as locking are notoriously bug-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges of incremental computation\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### Validity of the CAP theorem\n",
    "\n",
    "![Figure 12.4](images/12fig04_alt.jpg)\n",
    "\n",
    "**Replicas can diverge if updates are allowed under partitions.**\n",
    "\n",
    "### The complex interaction between the CAP theorem and incremental algorithms\n",
    "\n",
    "**conflict-free replicated data types (*CRDT*s)**\n",
    "\n",
    "#### A G-Counter is a grow-only counter (or [`accumulator`](http://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka)) where a replica only increments its assigned counter. \n",
    "\n",
    "![Figure 12.5](images/12fig05_alt.jpg)\n",
    "\n",
    "**The overall value of the accumulator is the sum of the replica counts.**\n",
    "\n",
    "#### Merging G-Counters\n",
    "\n",
    "![Figure 12.6](images/12fig06_alt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous versus synchronous updates\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### A simple speed layer architecture using synchronous updates\n",
    "\n",
    "![Figure 12.7](images/12fig07.jpg)\n",
    "\n",
    "### Asynchronous updates provide higher throughput and readily handle variable loads.\n",
    "\n",
    "![Figure 12.8](images/12fig08_alt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expiring realtime views\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "#### The state of the serving and speed layer views at the end of the first batch computation run:\n",
    "\n",
    "![Figure 12.9](images/12fig09.jpg)\n",
    "\n",
    "#### A portion of the realtime views can be expired after the second run completes:\n",
    "\n",
    "![Figure 12.10](images/12fig10.jpg)\n",
    "\n",
    "#### The serving and speed layer views immediately before the completion of the third batch computation run:\n",
    "\n",
    "![Figure 12.11](images/12fig11_alt.jpg)\n",
    "\n",
    "#### Alternating clearing between two different sets of realtime views guarantees one set always contains the appropriate data for the speed layer:\n",
    "\n",
    "![Figure 12.12](images/12fig12_alt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queueing and stream processing\n",
    "===============================================================\n",
    "\n",
    "**To implement asynchronous processing without queues, a client submits an event without monitoring whether its processing is successful.**\n",
    "\n",
    "![Figure 14.1](images/14fig01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queuing\n",
    "---------------------------------------------------------------------\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### Single-consumer queue servers\n",
    "\n",
    "![Figure 14.2](images/14fig02.jpg)\n",
    "\n",
    "**Multiple applications sharing a single queue consumer**\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### Multi-consumer queues\n",
    "\n",
    "![Figure 14.3](images/14fig03_alt.jpg)\n",
    "\n",
    "**With a multi-consumer queue, applications request specific items from the queue and are responsible for tracking the successful processing of each event.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream processing\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "![Figure 14.4](images/14fig04_alt.jpg)\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "**Comparison of stream-processing paradigms**\n",
    "\n",
    "![Figure 14.5](images/14fig05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queues and workers\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "#### A representative system using a queues-and-workers architecture. \n",
    "\n",
    "![Figure 14.6](images/14fig06_alt.jpg)\n",
    "\n",
    "**The queues in the diagram could potentially be distributed queues as well.**\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "#### Computing pageviews over time with a queues-and-workers architecture\n",
    "\n",
    "![Figure 14.7](images/14fig07_alt.jpg)\n",
    "\n",
    "**For our purposes we can use HBase in place of Cassandra**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Micro-batch stream processing\n",
    "===============================================================\n",
    "> This section covers:\n",
    ">  \n",
    "* Exactly-once processing semantics\n",
    "* Micro-batch processing and its trade-offs\n",
    "* Extending pipe diagrams for micro-batch stream processing\n",
    "\n",
    "Achieving exactly-once semantics\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### Strongly ordered processing\n",
    ">  \n",
    "* The stored ID is the same as the current tuple ID. In this case, you know that the count already reflects the current tuple, so you do nothing.\n",
    "* The stored ID is different from the current tuple ID. In this case, you know that the count doesn’t reflect the current tuple. So you increment the counter and update the stored ID. This works because tuples are processed in order, and the count and ID are updated atomically.\n",
    "\n",
    "### Micro-batch stream processing\n",
    "\n",
    "**Tuple stream divided into batches**\n",
    "\n",
    "![Figure 16.1](images/16fig01.jpg)\n",
    "\n",
    "### Micro-batch processing topologies\n",
    "\n",
    "#### Each batch includes tuples from all partitions of the incoming stream.\n",
    "\n",
    "![Figure 16.4](images/16fig04.jpg)\n",
    "\n",
    "**Word-count topology:**\n",
    "\n",
    "![Figure 16.5](images/16fig05.jpg)\n",
    "\n",
    "**Storing word counts with batch IDs:**\n",
    "\n",
    "![Figure 16.6](images/16fig06.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core concepts of micro-batch stream processing\n",
    "---------------------------------------------------------------------\n",
    ">  \n",
    "* *Batch-local computation*—There’s computation that occurs solely within the batch, not dependent on any state being kept. This includes things like reparti- tioning the word stream by the word field and computing the count of all the tuples in a batch.\n",
    "* *Stateful computation*—Then there’s computation that keeps state across all batches, such as updating a global count, updating word counts, or storing a top-three list of most frequently used words. This is where you have to be really careful about how you do state updates so that processing is idempotent under failures and retries. The trick of storing the batch ID with the state is particu- larly useful here to add idempotence to non-idempotent operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Illustration\n",
    "===============================================================\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realtime views\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "> Three separate queries you’re implementing for SuperWebAnalytics.com:\n",
    ">  \n",
    "* Number of pageviews over a range of hours\n",
    "* Unique number of visitors over a range of hours\n",
    "* Bounce rate for a domain  \n",
    "    \n",
    "---------------------------------------------------------------------\n",
    "\n",
    "> Consider the following sequence of events:\n",
    ">  \n",
    "1. IP address `11.11.11.111` visits `foo.com/about` at 1:30 pm.\n",
    "2. User `sally` visits `foo.com/about` at 1:40 pm.\n",
    "3. An equiv edge between `11.11.11.111` and `sally` is discovered at 2:00 pm.\n",
    "\n",
    "### Topology structure\n",
    "\n",
    "1. Consume a stream of pageview events that contains a user identifier, a URL, and a timestamp.\n",
    "2. Normalize URLs.\n",
    "3. Update a database containing a nested map from URL to hour to a HyperLogLog (*i.e.* `approxCountDistinct`) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "SuperWebAnalytics.com speed layer\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "### HBase data model\n",
    "\n",
    "#### The HBase data model consists of column families, keys, and columns.\n",
    "\n",
    "![Figure 13.1](images/13fig01_alt.jpg)\n",
    "\n",
    "#### Pageviews over time represented in HBase\n",
    "\n",
    "![Figure 13.2](images/13fig02.jpg)\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
