{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load.resolver(\"Typesafe repository\" at \"http://repo.typesafe.com/typesafe/releases/\")\n",
    "load.ivy(\"org.apache.spark\" %% \"spark-core\" % \"1.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcollection.mutable.HashMap\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "import collection.mutable.HashMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/09/18 13:38:07 INFO SparkContext: Running Spark version 1.4.1\n",
      "15/09/18 13:38:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/18 13:38:07 INFO SecurityManager: Changing view acls to: Balaji\n",
      "15/09/18 13:38:07 INFO SecurityManager: Changing modify acls to: Balaji\n",
      "15/09/18 13:38:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Balaji); users with modify permissions: Set(Balaji)\n",
      "15/09/18 13:38:08 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/09/18 13:38:08 INFO Remoting: Starting remoting\n",
      "15/09/18 13:38:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.56.1:50253]\n",
      "15/09/18 13:38:08 INFO Utils: Successfully started service 'sparkDriver' on port 50253.\n",
      "15/09/18 13:38:08 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/09/18 13:38:08 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/09/18 13:38:08 INFO DiskBlockManager: Created local directory at C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-d0875411-a996-43ea-82a2-20476b677e5b\\blockmgr-33cdb0dc-4430-42e7-8ad3-3b3ee8a7cbcf\n",
      "15/09/18 13:38:08 INFO MemoryStore: MemoryStore started with capacity 1958.6 MB\n",
      "15/09/18 13:38:08 INFO HttpFileServer: HTTP File server directory is C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-d0875411-a996-43ea-82a2-20476b677e5b\\httpd-5a057bf0-3d3e-4e8e-a653-ca1c95c0697f\n",
      "15/09/18 13:38:08 INFO HttpServer: Starting HTTP Server\n",
      "15/09/18 13:38:08 INFO Utils: Successfully started service 'HTTP file server' on port 50254.\n",
      "15/09/18 13:38:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/09/18 13:38:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/09/18 13:38:08 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "15/09/18 13:38:08 INFO SparkUI: Started SparkUI at http://192.168.56.1:4041\n",
      "15/09/18 13:38:08 INFO SparkContext: Added JAR C:\\Anaconda\\Galvanize\\DataEngineering\\week3\\3.4\\.34Lab\\target\\scala-2.10\\34-lab_2.10-1.0.jar at http://192.168.56.1:50254/jars/34-lab_2.10-1.0.jar with timestamp 1442608688753\n",
      "15/09/18 13:38:08 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@127.0.0.1:7077/user/Master...\n",
      "15/09/18 13:38:09 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150918133808-0001\n",
      "15/09/18 13:38:09 INFO AppClient$ClientActor: Executor added: app-20150918133808-0001/0 on worker-20150918133647-192.168.56.1-50011 (192.168.56.1:50011) with 1 cores\n",
      "15/09/18 13:38:09 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150918133808-0001/0 on hostPort 192.168.56.1:50011 with 1 cores, 512.0 MB RAM\n",
      "15/09/18 13:38:09 INFO AppClient$ClientActor: Executor added: app-20150918133808-0001/1 on worker-20150918133645-192.168.56.1-49993 (192.168.56.1:49993) with 1 cores\n",
      "15/09/18 13:38:09 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150918133808-0001/1 on hostPort 192.168.56.1:49993 with 1 cores, 512.0 MB RAM\n",
      "15/09/18 13:38:09 INFO AppClient$ClientActor: Executor updated: app-20150918133808-0001/0 is now LOADING\n",
      "15/09/18 13:38:09 INFO AppClient$ClientActor: Executor updated: app-20150918133808-0001/1 is now LOADING\n",
      "15/09/18 13:38:09 INFO AppClient$ClientActor: Executor updated: app-20150918133808-0001/0 is now RUNNING\n",
      "15/09/18 13:38:09 INFO AppClient$ClientActor: Executor updated: app-20150918133808-0001/1 is now RUNNING\n",
      "15/09/18 13:38:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50292.\n",
      "15/09/18 13:38:09 INFO NettyBlockTransferService: Server created on 50292\n",
      "15/09/18 13:38:09 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/09/18 13:38:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.56.1:50292 with 1958.6 MB RAM, BlockManagerId(driver, 192.168.56.1, 50292)\n",
      "15/09/18 13:38:09 INFO BlockManagerMaster: Registered BlockManager\n",
      "15/09/18 13:38:09 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3cbe0694"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// val sc = new SparkContext(\"spark://127.0.0.1:7077\",\n",
    "//                           \"3.4 Lab\", \n",
    "//                           \"C:\\\\spark-1.4.1-bin-hadoop2.4\", \n",
    "//                           List(\"C:\\\\Anaconda\\\\Galvanize\\\\DataEngineering\\\\week3\\\\3.4\\\\.34Lab\\\\target\\\\scala-2.10\\\\34-lab_2.10-1.0.jar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconf\u001b[0m: org.apache.spark.SparkConf = org.apache.spark.SparkConf@140d4c6b\n",
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@6c8356c"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAppName(\"Testing\").setMaster(\"local[4]\").set(\"spark.executor.memory\",\"1g\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlogFile\u001b[0m: java.lang.String = \u001b[32m\"s3n://AKIAJL7MGVMKVAGH66TQ:W+eHmJhCi7u8USuwGIO2FTSuTmXZ5cK6cwFlIv31@mortar-example-data/airline-data\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val logFile = \"s3n://AKIAJL7MGVMKVAGH66TQ:W+eHmJhCi7u8USuwGIO2FTSuTmXZ5cK6cwFlIv31@mortar-example-data/airline-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres17\u001b[0m: Long = \u001b[32m5113194\u001b[0mL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.textFile(logFile).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlogData\u001b[0m: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at Main.scala:279\n",
       "\u001b[36mdata\u001b[0m: org.apache.spark.rdd.RDD[java.lang.String] = ParallelCollectionRDD[4] at parallelize at Main.scala:282"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val logData  = sc.textFile(logFile)\n",
    "val data = sc.parallelize(sc.textFile(logFile).take(100).map(line => line.replace(\"'\",\"\").replace(\"\\\"\",\"\")).map(line => line.substring(0,line.length()-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "\torg.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:315)",
      "\torg.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)",
      "\torg.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)",
      "\torg.apache.spark.SparkContext.clean(SparkContext.scala:1893)",
      "\torg.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:311)",
      "\torg.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:310)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)",
      "\torg.apache.spark.rdd.RDD.withScope(RDD.scala:286)",
      "\torg.apache.spark.rdd.RDD.filter(RDD.scala:310)",
      "\tcmd6$$user$$anonfun$3.apply(Main.scala:134)",
      "\tcmd6$$user$$anonfun$3.apply(Main.scala:133)",
      "java.io.NotSerializableException: org.apache.spark.SparkConf",
      "Serialization stack:",
      "\t- object not serializable (class: org.apache.spark.SparkConf, value: org.apache.spark.SparkConf@309ed441)",
      "\t- field (class: cmd2$$user, name: conf, type: class org.apache.spark.SparkConf)",
      "\t- object (class cmd2$$user, cmd2$$user@75a88665)",
      "\t- field (class: cmd6, name: $ref$cmd2, type: class cmd2$$user)",
      "\t- object (class cmd6, cmd6@5e9e8f0b)",
      "\t- field (class: cmd6$$user, name: $outer, type: class cmd6)",
      "\t- object (class cmd6$$user, cmd6$$user@692f81c)",
      "\t- field (class: cmd6$$user$$anonfun$3, name: $outer, type: class cmd6$$user)",
      "\t- object (class cmd6$$user$$anonfun$3, <function0>)",
      "\t- field (class: cmd6$$user$$anonfun$3$$anonfun$apply$1, name: $outer, type: class cmd6$$user$$anonfun$3)",
      "\t- object (class cmd6$$user$$anonfun$3$$anonfun$apply$1, <function1>)",
      "\torg.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)",
      "\torg.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)",
      "\torg.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:81)",
      "\torg.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:312)",
      "\torg.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)",
      "\torg.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)",
      "\torg.apache.spark.SparkContext.clean(SparkContext.scala:1893)",
      "\torg.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:311)",
      "\torg.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:310)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)",
      "\torg.apache.spark.rdd.RDD.withScope(RDD.scala:286)",
      "\torg.apache.spark.rdd.RDD.filter(RDD.scala:310)",
      "\tcmd6$$user$$anonfun$3.apply(Main.scala:134)",
      "\tcmd6$$user$$anonfun$3.apply(Main.scala:133)"
     ]
    }
   ],
   "source": [
    "val rawRDD = sc.textFile(logFile)\n",
    "\n",
    "// Fetch the header\n",
    "val header =  rawRDD.first\n",
    "\n",
    "// Filter on the header than map to clean the line\n",
    "val sample = rawRDD.filter(!_.contains(header)).map { \n",
    " line => line.replaceAll(\"['\\\"]\",\"\").substring(0,line.length()-1)\n",
    "}.takeSample(false,100,12L) // takeSample returns a fixed-size sampled subset of this RDD in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR,MONTH,UNIQUE_CARRIER,ORIGIN_AIRPORT_ID,DEST_AIRPORT_ID,DEP_DELAY,DEP_DELAY_NEW,ARR_DELAY,ARR_DELAY_NEW,CANCELLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mheader\u001b[0m: java.lang.String = \u001b[32m\"YEAR,MONTH,UNIQUE_CARRIER,ORIGIN_AIRPORT_ID,DEST_AIRPORT_ID,DEP_DELAY,DEP_DELAY_NEW,ARR_DELAY,ARR_DELAY_NEW,CANCELLED\"\u001b[0m\n",
       "\u001b[36mlimited\u001b[0m: org.apache.spark.rdd.RDD[java.lang.String] = MapPartitionsRDD[5] at filter at Main.scala:288\n",
       "\u001b[36mres114_3\u001b[0m: Long = \u001b[32m99\u001b[0mL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val header = data.first\n",
    "println(header)\n",
    "val limited = data.filter(line => line != \"YEAR,MONTH,UNIQUE_CARRIER,ORIGIN_AIRPORT_ID,DEST_AIRPORT_ID,DEP_DELAY,DEP_DELAY_NEW,ARR_DELAY,ARR_DELAY_NEW,CANCELLED\")\n",
    "limited.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR\n",
      "MONTH\n",
      "UNIQUE_CARRIER\n",
      "ORIGIN_AIRPORT_ID\n",
      "DEST_AIRPORT_ID\n",
      "DEP_DELAY\n",
      "DEP_DELAY_NEW\n",
      "ARR_DELAY\n",
      "ARR_DELAY_NEW\n",
      "CANCELLED\n",
      "2012\n",
      "4\n",
      "AA\n",
      "12478\n",
      "12892\n",
      "-4.00\n",
      "0.00\n",
      "-21.00\n",
      "0.00\n",
      "0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "header.split(\",\").foreach(println)\n",
    "limited.take(89)(0).split(\",\").foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres116\u001b[0m: scala.Array[java.lang.String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-4.00,0.00,-21.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-7.00,0.00,-65.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-6.00,0.00,-63.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-6.00,0.00,5.00,5.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-2.00,0.00,-39.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-6.00,0.00,-34.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-8.00,0.00,-16.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-7.00,0.00,-19.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-9.00,0.00,-2.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-5.00,0.00,-17.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,-10.00,0.00,-10.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,1.00,1.00,-4.00,0.00,0.00\"\u001b[0m,\n",
       "  \u001b[32m\"2012,4,AA,12478,12892,0.00,0.00,-12.00,0.00,0.00\"\u001b[0m\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limited.take(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mmake_rows\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_rows(rowline: String): HashMap[String, Float] = {\n",
    "    val row = new HashMap[String, Float]\n",
    "    val headers = header.split(\",\")\n",
    "    val content = rowline.split(\",\")\n",
    "    row.put(headers(4),content(4).toFloat)\n",
    "    row.put(headers(3),content(3).toFloat)\n",
    "    row.put(headers(7),content(7).replace(\"\\\"\\\"\",\"0\").toFloat-content(5).toFloat)\n",
    "    if(content(9).toFloat == 1)\n",
    "    {\n",
    "        row.put(headers(5),content(5).toFloat+5)\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        row.put(headers(5),content(5).toFloat)\n",
    "    }\n",
    "    return row\n",
    "//     println(row.toSeq)\n",
    "//     val rowrdd: RDD[(String, Float)] = sc.parallelize(row.toSeq)\n",
    "//     val rowrdd = pairRDD.mapPartitions(iterator => {\n",
    "//         val hashmap = row\n",
    "//         iterator.foreach { case (key, value}  => hashmap.getOrElseUpdate(key,\n",
    "//                                             new Float) += value\n",
    "//         Iterator(hashmap)\n",
    "//                                  }, preserveParitioning = true}\n",
    "//     return rowrdd\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres129\u001b[0m: scala.collection.mutable.HashMap[String,Float] = Map(DEP_DELAY -> -4.0, DEST_AIRPORT_ID -> 12892.0, ARR_DELAY -> -17.0, ORIGIN_AIRPORT_ID -> 12478.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// val info1 = limited.map(make_rows)\n",
    "make_rows(limited.take(4)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mmake_rows2\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_rows2(rowline: String): (Float, Float) = {\n",
    "//     val headers = header.split(\",\")\n",
    "    val content = rowline.split(\",\")\n",
    "    val ID = content(3).toFloat\n",
    "    if (content(5).isEmpty && !content(7).isEmpty) {\n",
    "        return(ID, content(7).toFloat)\n",
    "    }\n",
    "    if (content(7).isEmpty && !content(5).isEmpty) {\n",
    "        return(ID, 0-content(5).toFloat)\n",
    "    }\n",
    "    else {\n",
    "        return (ID, 0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "\torg.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:315)",
      "\torg.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)",
      "\torg.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)",
      "\torg.apache.spark.SparkContext.clean(SparkContext.scala:1893)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)",
      "\torg.apache.spark.rdd.RDD.withScope(RDD.scala:286)",
      "\torg.apache.spark.rdd.RDD.map(RDD.scala:293)",
      "\tcmd104$$user$$anonfun$1.apply(Main.scala:273)",
      "\tcmd104$$user$$anonfun$1.apply(Main.scala:272)",
      "java.io.NotSerializableException: org.apache.spark.SparkConf",
      "Serialization stack:",
      "\t- object not serializable (class: org.apache.spark.SparkConf, value: org.apache.spark.SparkConf@309ed441)",
      "\t- field (class: cmd2$$user, name: conf, type: class org.apache.spark.SparkConf)",
      "\t- object (class cmd2$$user, cmd2$$user@75a88665)",
      "\t- field (class: cmd4, name: $ref$cmd2, type: class cmd2$$user)",
      "\t- object (class cmd4, cmd4@285f92e3)",
      "\t- field (class: cmd4$$user, name: $outer, type: class cmd4)",
      "\t- object (class cmd4$$user, cmd4$$user@531dab)",
      "\t- field (class: cmd5, name: $ref$cmd4, type: class cmd4$$user)",
      "\t- object (class cmd5, cmd5@5592596e)",
      "\t- field (class: cmd5$$user, name: $outer, type: class cmd5)",
      "\t- object (class cmd5$$user, cmd5$$user@61e1b152)",
      "\t- field (class: cmd102, name: $ref$cmd5, type: class cmd5$$user)",
      "\t- object (class cmd102, cmd102@50be18c5)",
      "\t- field (class: cmd102$$user, name: $outer, type: class cmd102)",
      "\t- object (class cmd102$$user, cmd102$$user@6003330c)",
      "\t- field (class: cmd104, name: $ref$cmd102, type: class cmd102$$user)",
      "\t- object (class cmd104, cmd104@2c62893b)",
      "\t- field (class: cmd104$$user, name: $outer, type: class cmd104)",
      "\t- object (class cmd104$$user, cmd104$$user@1a37fc95)",
      "\t- field (class: cmd104$$user$$anonfun$1, name: $outer, type: class cmd104$$user)",
      "\t- object (class cmd104$$user$$anonfun$1, <function0>)",
      "\t- field (class: cmd104$$user$$anonfun$1$$anonfun$apply$1, name: $outer, type: class cmd104$$user$$anonfun$1)",
      "\t- object (class cmd104$$user$$anonfun$1$$anonfun$apply$1, <function1>)",
      "\torg.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)",
      "\torg.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)",
      "\torg.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:81)",
      "\torg.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:312)",
      "\torg.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)",
      "\torg.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)",
      "\torg.apache.spark.SparkContext.clean(SparkContext.scala:1893)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)",
      "\torg.apache.spark.rdd.RDD.withScope(RDD.scala:286)",
      "\torg.apache.spark.rdd.RDD.map(RDD.scala:293)",
      "\tcmd104$$user$$anonfun$1.apply(Main.scala:273)",
      "\tcmd104$$user$$anonfun$1.apply(Main.scala:272)"
     ]
    }
   ],
   "source": [
    "val info2 = limited.map(make_rows2)\n",
    "// make_rows2(limited.take(4)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mmake_rows3\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_rows3(rowline: String): (Float, Float) = {\n",
    "//     val headers = header.split(\",\")\n",
    "    val content = rowline.split(\",\")\n",
    "    val ID = content(3).toFloat\n",
    "    if (content(5).isEmpty) {\n",
    "        return (ID, 0)\n",
    "    }\n",
    "    else {\n",
    "        return(ID, content(5).toFloat)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Task not serializable",
      "\torg.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:315)",
      "\torg.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)",
      "\torg.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)",
      "\torg.apache.spark.SparkContext.clean(SparkContext.scala:1893)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)",
      "\torg.apache.spark.rdd.RDD.withScope(RDD.scala:286)",
      "\torg.apache.spark.rdd.RDD.map(RDD.scala:293)",
      "\tcmd109$$user$$anonfun$1.apply(Main.scala:277)",
      "\tcmd109$$user$$anonfun$1.apply(Main.scala:276)",
      "java.io.NotSerializableException: org.apache.spark.SparkConf",
      "Serialization stack:",
      "\t- object not serializable (class: org.apache.spark.SparkConf, value: org.apache.spark.SparkConf@309ed441)",
      "\t- field (class: cmd2$$user, name: conf, type: class org.apache.spark.SparkConf)",
      "\t- object (class cmd2$$user, cmd2$$user@75a88665)",
      "\t- field (class: cmd4, name: $ref$cmd2, type: class cmd2$$user)",
      "\t- object (class cmd4, cmd4@285f92e3)",
      "\t- field (class: cmd4$$user, name: $outer, type: class cmd4)",
      "\t- object (class cmd4$$user, cmd4$$user@531dab)",
      "\t- field (class: cmd5, name: $ref$cmd4, type: class cmd4$$user)",
      "\t- object (class cmd5, cmd5@5592596e)",
      "\t- field (class: cmd5$$user, name: $outer, type: class cmd5)",
      "\t- object (class cmd5$$user, cmd5$$user@61e1b152)",
      "\t- field (class: cmd109, name: $ref$cmd5, type: class cmd5$$user)",
      "\t- object (class cmd109, cmd109@30184fc)",
      "\t- field (class: cmd109$$user, name: $outer, type: class cmd109)",
      "\t- object (class cmd109$$user, cmd109$$user@7c6fbef9)",
      "\t- field (class: cmd109$$user$$anonfun$1, name: $outer, type: class cmd109$$user)",
      "\t- object (class cmd109$$user$$anonfun$1, <function0>)",
      "\t- field (class: cmd109$$user$$anonfun$1$$anonfun$apply$1, name: $outer, type: class cmd109$$user$$anonfun$1)",
      "\t- object (class cmd109$$user$$anonfun$1$$anonfun$apply$1, <function1>)",
      "\torg.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)",
      "\torg.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)",
      "\torg.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:81)",
      "\torg.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:312)",
      "\torg.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:305)",
      "\torg.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)",
      "\torg.apache.spark.SparkContext.clean(SparkContext.scala:1893)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)",
      "\torg.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)",
      "\torg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)",
      "\torg.apache.spark.rdd.RDD.withScope(RDD.scala:286)",
      "\torg.apache.spark.rdd.RDD.map(RDD.scala:293)",
      "\tcmd109$$user$$anonfun$1.apply(Main.scala:277)",
      "\tcmd109$$user$$anonfun$1.apply(Main.scala:276)"
     ]
    }
   ],
   "source": [
    "val info3 = limited.map(make_rows3)\n",
    "// make_rows3(limited.take(4)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres145\u001b[0m: scala.Array[Any] = \u001b[33mArray\u001b[0m(\n",
       "  -34.00,\n",
       "  -45.00,\n",
       "  31.00,\n",
       "  -41.00,\n",
       "  0,\n",
       "  -12.00,\n",
       "  -23.00,\n",
       "  -30.00,\n",
       "  -52.00,\n",
       "  82.00,\n",
       "  -16.00,\n",
       "  -38.00,\n",
       "  6.00,\n",
       "  -63.00,\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limited.map(line => if (line.split(\",\")(7).isEmpty) { 0 } else\n",
    "            line.split(\",\")(7)).distinct.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info2.mapValues((_, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)).mapValues{ case (sum, count) => (1.0 * sum)/count}.collectAsMap()\n",
    "info3.mapValues((_, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2)).mapValues{ case (sum, count) => (1.0 * sum)/count}.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:332: not found: value info2",
      "info2.join(info3).map({case(key,(val1, val2)) => (key, (val1+val2)/2)}).take(5)",
      "^\u001b[0m"
     ]
    }
   ],
   "source": [
    "info2.join(info3).map({case(key,(val1, val2)) => (key, (val1+val2)/2)}).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info2.setName(\"airline_rdd1\").persist()\n",
    "info3.setName(\"airline_rdd2\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info2.sortBy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
