{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "This file was auto-generated from markdown using notedown.\n",
    "Instead of modifying the ipynb modify the markdown source. \n",
    "-->\n",
    "\n",
    "<h1 class=\"tocheading\">Apache Spark</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "<img src=\"images/spark-logo.png\">\n",
    "\n",
    "Apache Spark \n",
    "============\n",
    "\n",
    "Key Value Pairs\n",
    "===============\n",
    "\n",
    "PairRDD\n",
    "-------\n",
    "\n",
    "At this point we know how to aggregate values across an RDD. If we\n",
    "have an RDD containing sales transactions we can find the total\n",
    "revenue across all transactions.\n",
    "\n",
    "Q: Using the following sales data find the total revenue across all\n",
    "transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: x[0].startswith('#'))\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick off last field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to float and then sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: float(x[-1]))\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceByKey\n",
    "-----------\n",
    "\n",
    "Q: Calculate revenue per state?\n",
    "\n",
    "- Instead of creating a sequence of revenue numbers we can create\n",
    "  tuples of states and revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use `reduceByKey` to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Find the state with the highest total revenue.\n",
    "\n",
    "- You can either use the action `top` or the transformation `sortBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What does `reduceByKey` do?\n",
    "</summary>\n",
    "1. It is like a reducer.\n",
    "<br>\n",
    "2. If the RDD is made up of key-value pairs, it combines the values\n",
    "   across all tuples with the same key by using the function we pass\n",
    "   to it.\n",
    "<br>\n",
    "3. It only works on RDDs made up of key-value pairs or 2-tuples.\n",
    "</details>\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `reduceByKey` only works on RDDs made up of 2-tuples.\n",
    "\n",
    "- `reduceByKey` works as both a reducer and a combiner.\n",
    "\n",
    "- It requires that the operation is associative.\n",
    "\n",
    "Word Count\n",
    "----------\n",
    "\n",
    "Q: Implement word count in Spark.\n",
    "\n",
    "- Create some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: (word,1))\\\n",
    "    .reduceByKey(lambda count1,count2: count1+count2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making List Indexing Readable\n",
    "-----------------------------\n",
    "\n",
    "- While this code looks reasonable, the list indexes are cryptic and\n",
    "  hard to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can make this more readable using Python's argument unpacking\n",
    "  feature.\n",
    "\n",
    "Argument Unpacking\n",
    "------------------\n",
    "\n",
    "Q: Which version of `getCity` is more readable and why?\n",
    "\n",
    "- Consider this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ('Dmitri','Smith','SF')\n",
    "\n",
    "def getCity1(client):\n",
    "    return client[2]\n",
    "\n",
    "def getCity2((first,last,city)):\n",
    "    return city\n",
    "\n",
    "print getCity1(client)\n",
    "\n",
    "print getCity2(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between `getCity1` and `getCity2`?\n",
    "\n",
    "- Which is more readable?\n",
    "\n",
    "- What is the essence of argument unpacking?\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "<details><summary>\n",
    "Q: Can argument unpacking work for deeper nested structures?\n",
    "</summary>\n",
    "Yes. It can work for arbitrarily nested tuples and lists.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How would you write `getCity` given \n",
    "`client = ('Dmitri','Smith',('123 Eddy','SF','CA'))`\n",
    "</summary>\n",
    "`def getCity((first,last,(street,city,state))): return city`\n",
    "</details>\n",
    "\n",
    "Argument Unpacking\n",
    "------------------\n",
    "\n",
    "- Lets test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ('Dmitri','Smith',('123 Eddy','SF','CA'))\n",
    "\n",
    "def getCity((first,last,(street,city,state))):\n",
    "    return city\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever you find yourself indexing into a tuple consider using\n",
    "  argument unpacking to make it more readable.\n",
    "\n",
    "- Here is what `getCity` looks like with tuple indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badGetCity(client):\n",
    "    return client[2][1]\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument Unpacking In Spark\n",
    "---------------------------\n",
    "\n",
    "Q: Rewrite the last Spark job using argument unpacking.\n",
    "\n",
    "- Here is the original version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code with argument unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda (id,date,store,state,product,amount): (state,float(amount)))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda (state,amount):amount,ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case because we have a long list or tuple argument unpacking\n",
    "  is a judgement call.\n",
    "\n",
    "GroupByKey\n",
    "----------\n",
    "\n",
    "`reduceByKey` lets us aggregate values using sum, max, min, and other\n",
    "associative operations. But what about non-associative operations like\n",
    "average? How can we calculate them?\n",
    "\n",
    "- There are several ways to do this.\n",
    "\n",
    "- The first approach is to change the RDD tuples so that the operation\n",
    "  becomes associative. \n",
    "\n",
    "- Instead of `(state, amount)` use `(state, (amount, count))`.\n",
    "\n",
    "- The second approach is to use `groupByKey`, which is like\n",
    "  `reduceByKey` except it gathers together all the values in an\n",
    "  iterator. \n",
    "  \n",
    "- The iterator can then be reduced in a `map` step immediately after\n",
    "  the `groupByKey`.\n",
    "\n",
    "Q: Calculate the average sales per state.\n",
    "\n",
    "- Approach 1: Restructure the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],(float(x[-1]),1)))\\\n",
    "    .reduceByKey(lambda (amount1,count1),(amount2,count2): \\\n",
    "        (amount1+amount2, count1+count2))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the argument unpacking we are doing in `reduceByKey` to name\n",
    "  the elements of the tuples.\n",
    "\n",
    "- Approach 2: Use `groupByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(iter):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iter:\n",
    "        total += x; count += 1\n",
    "    return total/count\n",
    "\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .groupByKey() \\\n",
    "    .map(lambda (state,iter): mean(iter))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are using unpacking again.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What would be the disadvantage of not using unpacking?\n",
    "</summary>\n",
    "1. We will need to drill down into the elements.\n",
    "<br>\n",
    "2. The code will be harder to read.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What are the pros and cons of `reduceByKey` vs `groupByKey`?\n",
    "</summary>\n",
    "1. `groupByKey` stores the values for particular key as an iterable.\n",
    "<br>\n",
    "2. This will take up space in memory or on disk.\n",
    "<br>\n",
    "3. `reduceByKey` therefore is more scalable.\n",
    "<br>\n",
    "4. However, `groupByKey` does not require associative reducer\n",
    "   operation.\n",
    "<br>\n",
    "5. For this reason `groupByKey` can be easier to program with.\n",
    "</details>\n",
    "\n",
    "\n",
    "Joins\n",
    "-----\n",
    "\n",
    "Q: Given a table of employees and locations find the cities that the\n",
    "employees live in.\n",
    "\n",
    "\n",
    "- The easiest way to do this is with a `join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employees: emp_id, loc_id, name\n",
    "employee_data = [\n",
    "    (101, 14, 'Alice'),\n",
    "    (102, 15, 'Bob'),\n",
    "    (103, 14, 'Chad'),\n",
    "    (104, 15, 'Jen'),\n",
    "    (105, 13, 'Dee') ]\n",
    "\n",
    "# Locations: loc_id, location\n",
    "location_data = [\n",
    "    (14, 'SF'),\n",
    "    (15, 'Seattle'),\n",
    "    (16, 'Portland')]\n",
    "\n",
    "employees = sc.parallelize(employee_data)\n",
    "locations = sc.parallelize(location_data)\n",
    "\n",
    "# Re-key employee records with loc_id\n",
    "employees2 = employees.map(lambda (emp_id,loc_id,name):(loc_id,name));\n",
    "\n",
    "# Now join.\n",
    "employees2.join(locations).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can we keep employees that don't have a valid location ID in\n",
    "the final result?\n",
    "</summary>\n",
    "1. Use `leftOuterJoin` to keep employees without location IDs.\n",
    "<br>\n",
    "2. Use `rightOuterJoin` to keep locations without employees. \n",
    "<br>\n",
    "3. Use `fullOuterJoin` to keep both.\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Cogroup\n",
    "-------\n",
    "\n",
    "Q: What is `cogroup` over RDDs?\n",
    "\n",
    "- If `rdd1` and `rdd2` are pair RDDs.\n",
    "\n",
    "- Meaning their elements are key-value pairs.\n",
    "\n",
    "- Then `rdd1.cogroups(rdd2)` will produce another pair RDD.\n",
    "\n",
    "- The key will be the keys of `rdd1` and `rdd2`.\n",
    "\n",
    "- For each key the value will be a pair.\n",
    "\n",
    "- The first element of the pair is a sequence of values from `rdd1` for that key. \n",
    "\n",
    "- The second element is the sequence of values from `rdd2` for that key.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "First, lets initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = sc.parallelize(xrange(5)).map(lambda x:(x%2,x))\n",
    "r3 = sc.parallelize(xrange(5)).map(lambda x:(x%3,x))\n",
    "for (k,(seq1,seq2)) in r2.cogroup(r3).collect(): \n",
    "    print [k,list(seq1),list(seq2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching and Persistence\n",
    "=======================\n",
    "\n",
    "RDD Caching\n",
    "-----------\n",
    "\n",
    "- Consider this Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in xrange(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets time running `count()` on `rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The RDD does no work until an action is called. And then when an\n",
    "  action is called it figures out the answer and then throws away all\n",
    "  the data.\n",
    "\n",
    "- If you have an RDD that you are going to reuse in your computation\n",
    "  you can use `cache()` to make Spark cache the RDD.\n",
    "\n",
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to\n",
    "  be computed from scratch again.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- Calling `cache()` flips a flag on the RDD. \n",
    "\n",
    "- The data is not cached until an action is called.\n",
    "\n",
    "- You can uncache an RDD using `unpersist()`.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Will `unpersist` uncache the RDD immediately or does it wait for an\n",
    "action?\n",
    "</summary>\n",
    "It unpersists immediately.\n",
    "</details>\n",
    "\n",
    "Caching and Persistence\n",
    "-----------------------\n",
    "\n",
    "Q: Persist RDD to disk instead of caching it in memory.\n",
    "\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "- Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Will the RDD be stored on disk at this point?\n",
    "</summary>\n",
    "No. It will get stored after we call an action.\n",
    "</details>\n",
    "\n",
    "Persistence Levels\n",
    "------------------\n",
    "\n",
    "Level                      |Meaning\n",
    "-----                      |-------\n",
    "`MEMORY_ONLY`              |Same as `cache()`\n",
    "`MEMORY_AND_DISK`          |Cache in memory then overflow to disk\n",
    "`MEMORY_AND_DISK_SER`      |Like above; in cache keep objects serialized instead of live \n",
    "`DISK_ONLY`                |Cache to disk not to memory\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `MEMORY_AND_DISK_SER` is a good compromise between the levels. \n",
    "\n",
    "- Fast, but not too expensive.\n",
    "\n",
    "- Make sure you unpersist when you don't need the RDD any more.\n",
    "\n",
    "\n",
    "Spark Performance\n",
    "=================\n",
    "\n",
    "Narrow and Wide Transformations\n",
    "-------------------------------\n",
    "\n",
    "- Spark transformations are *narrow* if each RDD has one unique child\n",
    "  past the transformation.\n",
    "\n",
    "- Spark transformations are *wide* if each RDD can have multiple\n",
    "  children past the transformation.\n",
    "\n",
    "- Narrow transformations are map-like, while wide transformations are\n",
    "  reduce-like.\n",
    "\n",
    "- Narrow transformations are faster because they do move data between\n",
    "  executors, while wide transformations are slower.\n",
    " \n",
    "Repartitioning\n",
    "--------------\n",
    "\n",
    "- Over time partitions can get skewed. \n",
    "\n",
    "- Or you might have less data or more data than you started with.\n",
    "\n",
    "- You can rebalance your partitions using `repartition` or `coalesce`.\n",
    "\n",
    "- `coalesce` is narrow while `repartition` is wide.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Between `coalesce` and `repartition` which one is faster? Which one is\n",
    "more effective?\n",
    "</summary>\n",
    "1. `coalesce` is narrow so it is faster. \n",
    "<br>\n",
    "2. However, it only combines partitions and does not shuffle them.\n",
    "<br>\n",
    "3. `repartition` is wide but it partitions more effectively because it\n",
    "   reshuffles the records.\n",
    "</details>\n",
    "\n",
    "Misc\n",
    "====\n",
    "\n",
    "Amazon S3\n",
    "---------\n",
    "\n",
    "- *\"s3:\" URLs break when Secret Key contains a slash, even if encoded*\n",
    "    <https://issues.apache.org/jira/browse/HADOOP-3733>\n",
    "\n",
    "- *Spark 1.3.1 / Hadoop 2.6 prebuilt pacakge has broken S3 filesystem access*\n",
    "    <https://issues.apache.org/jira/browse/SPARK-7442>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
