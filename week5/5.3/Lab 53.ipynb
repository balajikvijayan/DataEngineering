{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: problems summary ::"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      ":::: ERRORS\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-spark_1.4_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:50 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-spark_1.4_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-shell-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-shell-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "Warning: the following JARs were previously added and are no more required:\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\com.sun.jersey.jersey-test-framework\\jersey-test-framework-grizzly2\\jars\\jersey-test-framework-grizzly2-1.9.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\commons-lang\\commons-lang\\jars\\commons-lang-2.5.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\commons-net\\commons-net\\jars\\commons-net-2.2.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.commons\\commons-math\\jars\\commons-math-2.1.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-annotations\\jars\\hadoop-annotations-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-auth\\jars\\hadoop-auth-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-client\\jars\\hadoop-client-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-common\\jars\\hadoop-common-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-hdfs\\jars\\hadoop-hdfs-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-mapreduce-client-app\\jars\\hadoop-mapreduce-client-app-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-mapreduce-client-common\\jars\\hadoop-mapreduce-client-common-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-mapreduce-client-core\\jars\\hadoop-mapreduce-client-core-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-mapreduce-client-jobclient\\jars\\hadoop-mapreduce-client-jobclient-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-mapreduce-client-shuffle\\jars\\hadoop-mapreduce-client-shuffle-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-yarn-api\\jars\\hadoop-yarn-api-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-yarn-client\\jars\\hadoop-yarn-client-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-yarn-common\\jars\\hadoop-yarn-common-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-yarn-server-common\\jars\\hadoop-yarn-server-common-2.2.0.jar\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.apache.hadoop\\hadoop-yarn-server-nodemanager\\jars\\hadoop-yarn-server-nodemanager-2.2.0.jar\n",
      "It is likely they were updated, which may lead to instabilities in the REPL.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.resolver(\"Typesafe repository\" at \"http://repo.typesafe.com/typesafe/releases/\")\n",
    "// load.resolver(\"MVN repository\" at \"http://mvnrepository.com/artifact/org.apache.spark/\")\n",
    "// load.ivy(\"org.apache.spark\" %% \"spark-core\" % \"1.4.1\")\n",
    "// load.ivy(\"org.apache.spark\" %% \"spark-sql\" % \"1.4.1\")\n",
    "load.ivy(\"org.apache.spark\" %% \"spark-hive\" % \"1.4.1\")\n",
    "load.ivy(\"com.github.alexarchambault\" % \"ammonite-spark_1.4_2.10.5\" % \"0.3.1-SNAPSHOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SQLContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.hive.HiveContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.hive.HiveContext\n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/09/29 23:43:49 INFO SparkContext: Running Spark version 1.4.1\n",
      "15/09/29 23:43:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/29 23:43:49 INFO SecurityManager: Changing view acls to: Balaji\n",
      "15/09/29 23:43:49 INFO SecurityManager: Changing modify acls to: Balaji\n",
      "15/09/29 23:43:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Balaji); users with modify permissions: Set(Balaji)\n",
      "15/09/29 23:43:49 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/09/29 23:43:49 INFO Remoting: Starting remoting\n",
      "15/09/29 23:43:50 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.56.1:63520]\n",
      "15/09/29 23:43:50 INFO Utils: Successfully started service 'sparkDriver' on port 63520.\n",
      "15/09/29 23:43:50 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/09/29 23:43:50 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/09/29 23:43:50 INFO DiskBlockManager: Created local directory at C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-b6fe4812-fa18-479d-a1b4-cc762f7d4dd0\\blockmgr-6dae9fad-faf2-4ddd-b779-312cf35e9e25\n",
      "15/09/29 23:43:50 INFO MemoryStore: MemoryStore started with capacity 1958.6 MB\n",
      "15/09/29 23:43:50 INFO HttpFileServer: HTTP File server directory is C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-b6fe4812-fa18-479d-a1b4-cc762f7d4dd0\\httpd-7e32f27c-ccf9-4533-9c19-7200216bb5eb\n",
      "15/09/29 23:43:50 INFO HttpServer: Starting HTTP Server\n",
      "15/09/29 23:43:50 INFO Utils: Successfully started service 'HTTP file server' on port 63521.\n",
      "15/09/29 23:43:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/09/29 23:43:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/09/29 23:43:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "15/09/29 23:43:50 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "15/09/29 23:43:50 INFO SparkUI: Started SparkUI at http://192.168.56.1:4042\n",
      "15/09/29 23:43:50 INFO Executor: Starting executor ID driver on host localhost\n",
      "15/09/29 23:43:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63558.\n",
      "15/09/29 23:43:50 INFO NettyBlockTransferService: Server created on 63558\n",
      "15/09/29 23:43:50 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/09/29 23:43:50 INFO BlockManagerMasterEndpoint: Registering block manager localhost:63558 with 1958.6 MB RAM, BlockManagerId(driver, localhost, 63558)\n",
      "15/09/29 23:43:50 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@50d0f634"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@transient val sc = new SparkContext(new SparkConf().setAppName(\"Testing\").setMaster(\n",
    "    \"local[4]\").set(\"spark.executor.memory\",\"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mhiveContext\u001b[0m: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@414b14b6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n",
    "// val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mhiveContext._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hiveContext._\n",
    "// import sqlContext._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataFile\u001b[0m: java.lang.String = \u001b[32m\"\"\"\n",
       "C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\yelp_academic_dataset_business.json\n",
       "\"\"\"\u001b[0m\n",
       "\u001b[36myelpDF\u001b[0m: org.apache.spark.sql.DataFrame = [attributes: struct<Accepts Credit Cards:string,Accepts Insurance:boolean,Ages Allowed:string,Alcohol:string,Ambience:struct<casual:boolean,classy:boolean,divey:boolean,hipster:boolean,intimate:boolean,romantic:boolean,touristy:boolean,trendy:boolean,upscale:boolean>,Attire:string,BYOB:boolean,BYOB/Corkage:string,By Appointment Only:boolean,Caters:boolean,Coat Check:boolean,Corkage:boolean,Delivery:boolean,Dietary Restrictions:struct<dairy-free:boolean,gluten-free:boolean,halal:boolean,kosher:boolean,soy-free:boolean,vegan:boolean,vegetarian:boolean>,Dogs Allowed:boolean,Drive-Thru:boolean,Good For:struct<breakfast:boolean,brunch:boolean,dessert:boolean,dinner:boolean,latenight:boolean,lunch:boolean>,Good For Dancing:boolean,Good For Groups:boolean,Good For Kids:boolean,Good for Kids:boolean,Hair Types Specialized In:struct<africanamerican:boolean,asian:boolean,coloring:boolean,curly:boolean,extensions:boolean,kids:boolean,perms:boolean,straightperms:boolean>,Happy Hour:boolean,Has TV:boolean,Music:struct<background_music:boolean,dj:boolean,jukebox:boolean,karaoke:boolean,live:boolean,playlist:boolean,video:boolean>,Noise Level:string,Open 24 Hours:boolean,Order at Counter:boolean,Outdoor Seating:boolean,Parking:struct<garage:boolean,lot:boolean,street:boolean,valet:boolean,validated:boolean>,Payment Types:struct<amex:boolean,cash_only:boolean,discover:boolean,mastercard:boolean,visa:boolean>,Price Range:bigint,Smoking:string,Take-out:boolean,Takes Reservations:boolean,Waiter\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dataFile = \"C:\\\\Anaconda\\\\Galvanize\\\\DataEngineering\\\\week5\\\\5.3\\\\yelp_academic_dataset_business.json\"\n",
    "val yelpDF = hiveContext.read.json(dataFile)\n",
    "// val yelpRDD = sqlContext.read.json(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+----------+-----------+--------------------+---------------+-----+------------+-----+-----+--------+\n",
      "|          attributes|         business_id|          categories|       city|        full_address|               hours|  latitude|  longitude|                name|  neighborhoods| open|review_count|stars|state|    type|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+----------+-----------+--------------------+---------------+-----+------------+-----+-----+--------+\n",
      "|[null,null,null,n...|vcNAWiLM4dR7D2nww...|List(Doctors, Hea...|    Phoenix|4840 E Indian Sch...|[[17:00,08:00],[1...| 33.499313|-111.983758|   Eric Goldberg, MD|         List()| true|           9|  3.5|   AZ|business|\n",
      "|[true,null,null,n...|UsFtqoBl7naz8AVUB...|     List(Nightlife)| Dravosburg|202 McClure St\n",
      "Dr...|[null,null,null,n...| 40.350519|  -79.88693|        Clancy's Pub|         List()| true|           4|  3.5|   PA|business|\n",
      "|[null,null,null,n...|cE27W9VPgO88Qxe4o...|List(Active Life,...|Bethel Park|1530 Hamilton Rd\n",
      "...|[null,null,null,n...|40.3568962|  -80.01591|Cool Springs Golf...|         List()|false|           5|  2.5|   PA|business|\n",
      "|[null,null,null,n...|HZdLhv6COCleJMo7n...|List(Shopping, Ho...| Pittsburgh|301 S Hills Vlg\n",
      "P...|[[21:00,10:00],[2...|  40.35762|  -80.05998|    Verizon Wireless|         List()| true|           3|  3.5|   PA|business|\n",
      "|[true,null,null,f...|mVHrayjG3uZ_RLHkL...|List(Bars, Americ...|   Braddock|414 Hawkins Ave\n",
      "B...|[[20:00,10:00],nu...| 40.408735|-79.8663507|       Emil's Lounge|         List()| true|          11|  4.5|   PA|business|\n",
      "|[true,null,null,f...|KayYbHCt-RkbGcPdG...|List(Bars, Americ...|   Carnegie|141 Hawthorne St\n",
      "...|[null,null,null,n...| 40.415517| -80.067534|Alexion's Bar & G...|List(Greentree)| true|          15|  4.0|   PA|business|\n",
      "|[null,null,null,n...|b12U9TFESStdy7CsT...|List(Auto Repair,...|   Carnegie|718 Hope Hollow R...|[null,null,null,n...| 40.394588|-80.0844539|Flynn's E W Tire ...|         List()| true|           5|  1.5|   PA|business|\n",
      "|[null,null,null,n...|Sktj1eHQFuVa-M4bg...|List(Active Life,...|   Carnegie|920 Forsythe Rd\n",
      "C...|[null,null,null,n...|40.4054039|-80.0762665|Forsythe Miniatur...| List(Carnegie)| true|           4|  4.0|   PA|business|\n",
      "|[null,null,null,n...|3ZVKmuK2l7uXPE6lX...|List(Home Service...|   Carnegie|8 Logan St\n",
      "Carneg...|[null,null,null,n...| 40.406324| -80.090357|Quaker State Cons...| List(Carnegie)| true|           3|  2.5|   PA|business|\n",
      "|[true,null,null,n...|wJr6kSA5dchdgOdwH...|List(Burgers, Bre...|   Carnegie|2100 Washington P...|[[02:00,08:00],[0...|40.3877323|-80.0928745|Kings Family Rest...|         List()| true|           8|  3.5|   PA|business|\n",
      "|[true,null,null,n...|yXuao0pFz1AxB21vJ...| List(Food, Grocery)|   Carnegie|2100 Washington P...|[null,null,null,n...|40.3877323|-80.0928745|         Shop N'save|         List()| true|           3|  3.5|   PA|business|\n",
      "|[true,null,null,f...|fNGIbpazjTRdXgwRY...|List(Bars, Americ...|   Carnegie|1201 Washington A...|[null,null,null,n...|40.3964688|-80.0849416|      Rocky's Lounge|         List()| true|           5|  4.0|   PA|business|\n",
      "|[false,null,null,...|b9WZJp5L1RZr4F1nx...|List(Breakfast & ...|   Carnegie|1073 Washington A...|[[14:30,06:00],[1...|40.3967441|-80.0847998|           Gab & Eat|         List()| true|          38|  4.5|   PA|business|\n",
      "|[true,null,null,n...|zaXDakTd3RXyOa7sM...|List(Cafes, Resta...|   Carnegie|202 3rd Ave\n",
      "Carne...|[null,null,null,n...| 40.404638| -80.089985|Barb's Country Ju...| List(Carnegie)| true|           5|  4.0|   PA|business|\n",
      "|[true,null,null,n...|zgy27FSnvwdINfk5c...|List(Hotels & Tra...|   Carnegie|520 North Bell Av...|[[00:00,00:00],[0...| 40.417419| -80.088557|Extended Stay Ame...| List(Carnegie)| true|           5|  4.0|   PA|business|\n",
      "|[true,null,null,f...|WETE_LykpcnrC1sFc...|List(Pubs, Irish,...|   Carnegie|215 E Main St\n",
      "Car...|[null,null,null,n...|  40.40867| -80.085279|  Paddy's Pour House| List(Carnegie)|false|           6|  3.5|   PA|business|\n",
      "|[null,null,null,n...|6o3RK6rTcN3nw-j-r...|              List()|   Carnegie|231 E Main St\n",
      "Car...|[null,null,null,n...|40.4085677|-80.0849924|Porto Fino Pizzar...| List(Carnegie)| true|           4|  2.5|   PA|business|\n",
      "|[false,null,null,...|woOVJ0Z2f6XNCDntH...|List(Local Servic...|   Carnegie|117 E Mall Plz\n",
      "Ca...|[[19:00,08:30],[1...|40.4078889|  -80.08606|    Alteration World| List(Carnegie)| true|           5|  5.0|   PA|business|\n",
      "|[true,null,null,n...|rv7CY8G_XibTx82Yh...|   List(Restaurants)|   Carnegie|Raceway Plz\n",
      "Carne...|[null,null,null,n...|40.3868915|-80.0937037|  Long John Silver's|         List()| true|           3|  3.5|   PA|business|\n",
      "|[null,null,null,n...|e_U_FnpdKVgNb4mUN...|List(Health & Med...|   Carnegie|2323 Greentree Rd...|[null,null,null,n...|  40.39076| -80.078657|  Weinberg Lisa, DMD|         List()| true|           6|  2.0|   PA|business|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+--------------------+----------+-----------+--------------------+---------------+-----+------------+-----+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yelpDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- Accepts Credit Cards: string (nullable = true)\n",
      " |    |-- Accepts Insurance: boolean (nullable = true)\n",
      " |    |-- Ages Allowed: string (nullable = true)\n",
      " |    |-- Alcohol: string (nullable = true)\n",
      " |    |-- Ambience: struct (nullable = true)\n",
      " |    |    |-- casual: boolean (nullable = true)\n",
      " |    |    |-- classy: boolean (nullable = true)\n",
      " |    |    |-- divey: boolean (nullable = true)\n",
      " |    |    |-- hipster: boolean (nullable = true)\n",
      " |    |    |-- intimate: boolean (nullable = true)\n",
      " |    |    |-- romantic: boolean (nullable = true)\n",
      " |    |    |-- touristy: boolean (nullable = true)\n",
      " |    |    |-- trendy: boolean (nullable = true)\n",
      " |    |    |-- upscale: boolean (nullable = true)\n",
      " |    |-- Attire: string (nullable = true)\n",
      " |    |-- BYOB: boolean (nullable = true)\n",
      " |    |-- BYOB/Corkage: string (nullable = true)\n",
      " |    |-- By Appointment Only: boolean (nullable = true)\n",
      " |    |-- Caters: boolean (nullable = true)\n",
      " |    |-- Coat Check: boolean (nullable = true)\n",
      " |    |-- Corkage: boolean (nullable = true)\n",
      " |    |-- Delivery: boolean (nullable = true)\n",
      " |    |-- Dietary Restrictions: struct (nullable = true)\n",
      " |    |    |-- dairy-free: boolean (nullable = true)\n",
      " |    |    |-- gluten-free: boolean (nullable = true)\n",
      " |    |    |-- halal: boolean (nullable = true)\n",
      " |    |    |-- kosher: boolean (nullable = true)\n",
      " |    |    |-- soy-free: boolean (nullable = true)\n",
      " |    |    |-- vegan: boolean (nullable = true)\n",
      " |    |    |-- vegetarian: boolean (nullable = true)\n",
      " |    |-- Dogs Allowed: boolean (nullable = true)\n",
      " |    |-- Drive-Thru: boolean (nullable = true)\n",
      " |    |-- Good For: struct (nullable = true)\n",
      " |    |    |-- breakfast: boolean (nullable = true)\n",
      " |    |    |-- brunch: boolean (nullable = true)\n",
      " |    |    |-- dessert: boolean (nullable = true)\n",
      " |    |    |-- dinner: boolean (nullable = true)\n",
      " |    |    |-- latenight: boolean (nullable = true)\n",
      " |    |    |-- lunch: boolean (nullable = true)\n",
      " |    |-- Good For Dancing: boolean (nullable = true)\n",
      " |    |-- Good For Groups: boolean (nullable = true)\n",
      " |    |-- Good For Kids: boolean (nullable = true)\n",
      " |    |-- Good for Kids: boolean (nullable = true)\n",
      " |    |-- Hair Types Specialized In: struct (nullable = true)\n",
      " |    |    |-- africanamerican: boolean (nullable = true)\n",
      " |    |    |-- asian: boolean (nullable = true)\n",
      " |    |    |-- coloring: boolean (nullable = true)\n",
      " |    |    |-- curly: boolean (nullable = true)\n",
      " |    |    |-- extensions: boolean (nullable = true)\n",
      " |    |    |-- kids: boolean (nullable = true)\n",
      " |    |    |-- perms: boolean (nullable = true)\n",
      " |    |    |-- straightperms: boolean (nullable = true)\n",
      " |    |-- Happy Hour: boolean (nullable = true)\n",
      " |    |-- Has TV: boolean (nullable = true)\n",
      " |    |-- Music: struct (nullable = true)\n",
      " |    |    |-- background_music: boolean (nullable = true)\n",
      " |    |    |-- dj: boolean (nullable = true)\n",
      " |    |    |-- jukebox: boolean (nullable = true)\n",
      " |    |    |-- karaoke: boolean (nullable = true)\n",
      " |    |    |-- live: boolean (nullable = true)\n",
      " |    |    |-- playlist: boolean (nullable = true)\n",
      " |    |    |-- video: boolean (nullable = true)\n",
      " |    |-- Noise Level: string (nullable = true)\n",
      " |    |-- Open 24 Hours: boolean (nullable = true)\n",
      " |    |-- Order at Counter: boolean (nullable = true)\n",
      " |    |-- Outdoor Seating: boolean (nullable = true)\n",
      " |    |-- Parking: struct (nullable = true)\n",
      " |    |    |-- garage: boolean (nullable = true)\n",
      " |    |    |-- lot: boolean (nullable = true)\n",
      " |    |    |-- street: boolean (nullable = true)\n",
      " |    |    |-- valet: boolean (nullable = true)\n",
      " |    |    |-- validated: boolean (nullable = true)\n",
      " |    |-- Payment Types: struct (nullable = true)\n",
      " |    |    |-- amex: boolean (nullable = true)\n",
      " |    |    |-- cash_only: boolean (nullable = true)\n",
      " |    |    |-- discover: boolean (nullable = true)\n",
      " |    |    |-- mastercard: boolean (nullable = true)\n",
      " |    |    |-- visa: boolean (nullable = true)\n",
      " |    |-- Price Range: long (nullable = true)\n",
      " |    |-- Smoking: string (nullable = true)\n",
      " |    |-- Take-out: boolean (nullable = true)\n",
      " |    |-- Takes Reservations: boolean (nullable = true)\n",
      " |    |-- Waiter Service: boolean (nullable = true)\n",
      " |    |-- Wheelchair Accessible: boolean (nullable = true)\n",
      " |    |-- Wi-Fi: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- full_address: string (nullable = true)\n",
      " |-- hours: struct (nullable = true)\n",
      " |    |-- Friday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |    |-- Monday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |    |-- Saturday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |    |-- Sunday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |    |-- Thursday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |    |-- Tuesday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |    |-- Wednesday: struct (nullable = true)\n",
      " |    |    |-- close: string (nullable = true)\n",
      " |    |    |-- open: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- neighborhoods: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- open: boolean (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(yelpDF.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yelpDF.registerTempTable(\"yelp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|       Auslers Grill|\n",
      "|Mulligan's Restau...|\n",
      "|             Sunfare|\n",
      "|              Subway|\n",
      "|           Lil Cal's|\n",
      "|                Ed's|\n",
      "|Frenchys Caribbea...|\n",
      "|           WY Market|\n",
      "|       Pollo Sabroso|\n",
      "|Queen Creek Olive...|\n",
      "|Gluten Free Creat...|\n",
      "|Panini Bread and ...|\n",
      "|        One Eighty Q|\n",
      "|Saffron JAK Origi...|\n",
      "|Los Primos Carnic...|\n",
      "| Bertie's Of Arcadia|\n",
      "|     Little Miss BBQ|\n",
      "|Las Jicaras Mexic...|\n",
      "|  Santos Lucha Libre|\n",
      "|   Taqueria El Chino|\n",
      "+--------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mresult\u001b[0m: org.apache.spark.sql.DataFrame = [name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// val result = hiveContext.sql(\"select name from yelp where stars = 5 and \n",
    "// city = 'Phoenix' and attributes.`Accepts Credit Cards` = 'true' \n",
    "// and categories like '%Restaurants%'\")\n",
    "result.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36muser_rdd\u001b[0m: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at Main.scala:173\n",
       "\u001b[36mtransaction_rdd\u001b[0m: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at textFile at Main.scala:176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val user_rdd = sc.textFile(\"s3n://AKIAJL7MGVMKVAGH66TQ:W+eHmJhCi7u8USuwGIO2FTSuTmXZ5cK6cwFlIv31@sparkdatasets/users.txt\")\n",
    "val transaction_rdd = sc.textFile(\"s3n://AKIAJL7MGVMKVAGH66TQ:W+eHmJhCi7u8USuwGIO2FTSuTmXZ5cK6cwFlIv31@sparkdatasets/transactions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[0m: scala.Array[String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"1106214172;Prometheus Barwis;prometheus.barwis@me.com;(533) 072-2779\"\u001b[0m,\n",
       "  \u001b[32m\"527133132;Ashraf Bainbridge;ashraf.bainbridge@gmail.com;\"\u001b[0m,\n",
       "  \u001b[32m\"1290614884;Alain Hennesey;alain.hennesey@facebook.com,alain.hennesey@me.com;(942) 208-8460,(801) 938-2376\"\u001b[0m\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8\u001b[0m: scala.Array[String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"815581247;$144.82;2015-09-05\"\u001b[0m,\n",
       "  \u001b[32m\"1534673027;$140.93;2014-03-11\"\u001b[0m,\n",
       "  \u001b[32m\"842468364;$104.26;2014-05-06\"\u001b[0m\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transaction_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mClean\u001b[0m\n",
       "\u001b[36mclean\u001b[0m: $user.Clean = cmd9$$user$Clean@737c6e8f\n",
       "\u001b[36mvalidUsers\u001b[0m: org.apache.spark.rdd.RDD[scala.Array[String]] = MapPartitionsRDD[4] at flatMap at Main.scala:207\n",
       "\u001b[36mvalidTransactions\u001b[0m: org.apache.spark.rdd.RDD[scala.Array[String]] = MapPartitionsRDD[5] at flatMap at Main.scala:210"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Clean extends Serializable {\n",
    "    def cleanUser(row: String): Option[Array[String]] = {\n",
    "            val content = row.trim.split(\";\")\n",
    "            if (content.length < 4) {\n",
    "                None\n",
    "            }\n",
    "            else {\n",
    "                Some(content) \n",
    "            }\n",
    "        }\n",
    "    def cleanTransaction(row: String): Option[Array[String]] = {\n",
    "            val content = row.trim.split(\";\")\n",
    "            if (content.length < 3) {\n",
    "                None\n",
    "            }\n",
    "            else {\n",
    "                Some(content) \n",
    "            }\n",
    "        }\n",
    "}\n",
    "\n",
    "val clean = new Clean\n",
    "val validUsers = user_rdd.flatMap(clean.cleanUser(_))\n",
    "val validTransactions = transaction_rdd.flatMap(clean.cleanTransaction(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10_0\u001b[0m: Long = \u001b[32m16678\u001b[0mL\n",
       "\u001b[36mres10_1\u001b[0m: Long = \u001b[32m25000\u001b[0mL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validUsers.count\n",
    "user_rdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11_0\u001b[0m: Long = \u001b[32m500000\u001b[0mL\n",
       "\u001b[36mres11_1\u001b[0m: Long = \u001b[32m500000\u001b[0mL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transaction_rdd.count\n",
    "validTransactions.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjsonUsers\u001b[0m: org.apache.spark.rdd.RDD[java.lang.String] = MapPartitionsRDD[6] at map at Main.scala:199"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val jsonUsers = validUsers.map(line => \"{user_id: \"+line(0)+\n",
    "                     \", name: \"+line(1)+\n",
    "                     \", email: \"+line(2)+\n",
    "                     \", phone: \"+line(3)+\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres16\u001b[0m: scala.Array[java.lang.String] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[32m\"{user_id: 815581247, amount_paid: 144.82, date: 2015-09-05}\"\u001b[0m,\n",
       "  \u001b[32m\"{user_id: 1534673027, amount_paid: 140.93, date: 2014-03-11}\"\u001b[0m\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validTransactions.map(line => \"{user_id: \"+line(0)+\n",
    "                     \", amount_paid: \"+line(1).replace(\"$\",\"\")+\n",
    "                     \", date: \"+line(2)+\"}\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.lang.reflect.InvocationTargetException",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1412)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.lang.reflect.InvocationTargetException",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r",
      "java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.Driver20.connect(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\r",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\r",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r",
      "\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r",
      "\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\r",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r",
      "\tat java.security.AccessController.doPrivileged(Native Method)\r",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)\r",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\r",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)\r",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)\r",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)\r",
      "\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)\r",
      "\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)\r",
      "\tat org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)\r",
      "\tat org.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)\r",
      "\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)\r",
      "\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)\r",
      "\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\r",
      "\tat org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)\r",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)\r",
      "\tat cmd13$$user$$anonfun$1.apply(Main.scala:203)\r",
      "\tat cmd13$$user$$anonfun$1.apply(Main.scala:202)\r",
      "\tat cmd13$$user.<init>(Main.scala:204)\r",
      "\tat cmd13.<init>(Main.scala:208)\r",
      "\tat cmd13$.<init>(Main.scala:172)\r",
      "\tat cmd13$.<clinit>(Main.scala)\r",
      "\tat cmd13$Main$.$main(Main.scala:167)\r",
      "\tat cmd13$Main.$main(Main.scala)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25$$anonfun$11.apply(Interpreter.scala:259)\r",
      "\tat ammonite.interpreter.Interpreter.evaluatorRunPrinter(Interpreter.scala:279)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25.apply(Interpreter.scala:259)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25.apply(Interpreter.scala:248)\r",
      "\tat ammonite.interpreter.Catching.map(Util.scala:75)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24.apply(Interpreter.scala:248)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24.apply(Interpreter.scala:246)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:246)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:244)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:244)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:243)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter.process(Interpreter.scala:243)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:202)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:201)\r",
      "\tat ammonite.interpreter.Capturing$$anonfun$apply$7.apply(Capture.scala:111)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$ammonite$interpreter$Capture$$withErr$1.apply(Capture.scala:40)\r",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\r",
      "\tat scala.Console$.withErr(Console.scala:148)\r",
      "\tat ammonite.interpreter.Capture$.ammonite$interpreter$Capture$$withErr(Capture.scala:36)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$3.apply(Capture.scala:50)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$withOut$1.apply(Capture.scala:31)\r",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\r",
      "\tat scala.Console$.withOut(Console.scala:107)\r",
      "\tat ammonite.interpreter.Capture$.withOut(Capture.scala:27)\r",
      "\tat ammonite.interpreter.Capture$.withOutAndErr(Capture.scala:50)\r",
      "\tat ammonite.interpreter.Capture$.apply(Capture.scala:91)\r",
      "\tat ammonite.interpreter.Capturing.apply(Capture.scala:111)\r",
      "\tat ammonite.interpreter.Capturing.flatMap(Capture.scala:116)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:201)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:199)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:199)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:195)\r",
      "\tat ammonite.interpreter.Catching.flatMap(Util.scala:73)\r",
      "\tat ammonite.interpreter.Interpreter.apply(Interpreter.scala:195)\r",
      "\tat jupyter.scala.ScalaInterpreter$$anon$1.interpret(ScalaInterpreter.scala:189)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:100)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:82)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)\r",
      "\tat scalaz.concurrent.Task$.Try(Task.scala:379)\r",
      "\tat scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)\r",
      "\tat scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)\r",
      "\tat scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)\r",
      "\tat scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)\r",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r",
      "\tat java.lang.Thread.run(Thread.java:745)\r",
      "Caused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r",
      "\t... 149 more\r",
      "Caused by: java.sql.SQLException: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r",
      "\t... 146 more\r",
      "Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.\r",
      "\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r",
      "\tat java.security.AccessController.doPrivileged(Native Method)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r",
      "\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r",
      "\t... 146 more\r",
      "------\r",
      "",
      "NestedThrowables:",
      "java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r",
      "java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.Driver20.connect(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\r",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\r",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r",
      "\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r",
      "\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\r",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r",
      "\tat java.security.AccessController.doPrivileged(Native Method)\r",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)\r",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\r",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)\r",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)\r",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)\r",
      "\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)\r",
      "\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)\r",
      "\tat org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)\r",
      "\tat org.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)\r",
      "\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)\r",
      "\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)\r",
      "\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\r",
      "\tat org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)\r",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)\r",
      "\tat cmd13$$user$$anonfun$1.apply(Main.scala:203)\r",
      "\tat cmd13$$user$$anonfun$1.apply(Main.scala:202)\r",
      "\tat cmd13$$user.<init>(Main.scala:204)\r",
      "\tat cmd13.<init>(Main.scala:208)\r",
      "\tat cmd13$.<init>(Main.scala:172)\r",
      "\tat cmd13$.<clinit>(Main.scala)\r",
      "\tat cmd13$Main$.$main(Main.scala:167)\r",
      "\tat cmd13$Main.$main(Main.scala)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25$$anonfun$11.apply(Interpreter.scala:259)\r",
      "\tat ammonite.interpreter.Interpreter.evaluatorRunPrinter(Interpreter.scala:279)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25.apply(Interpreter.scala:259)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25.apply(Interpreter.scala:248)\r",
      "\tat ammonite.interpreter.Catching.map(Util.scala:75)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24.apply(Interpreter.scala:248)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24.apply(Interpreter.scala:246)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:246)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:244)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:244)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:243)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter.process(Interpreter.scala:243)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:202)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:201)\r",
      "\tat ammonite.interpreter.Capturing$$anonfun$apply$7.apply(Capture.scala:111)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$ammonite$interpreter$Capture$$withErr$1.apply(Capture.scala:40)\r",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\r",
      "\tat scala.Console$.withErr(Console.scala:148)\r",
      "\tat ammonite.interpreter.Capture$.ammonite$interpreter$Capture$$withErr(Capture.scala:36)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$3.apply(Capture.scala:50)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$withOut$1.apply(Capture.scala:31)\r",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\r",
      "\tat scala.Console$.withOut(Console.scala:107)\r",
      "\tat ammonite.interpreter.Capture$.withOut(Capture.scala:27)\r",
      "\tat ammonite.interpreter.Capture$.withOutAndErr(Capture.scala:50)\r",
      "\tat ammonite.interpreter.Capture$.apply(Capture.scala:91)\r",
      "\tat ammonite.interpreter.Capturing.apply(Capture.scala:111)\r",
      "\tat ammonite.interpreter.Capturing.flatMap(Capture.scala:116)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:201)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:199)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:199)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:195)\r",
      "\tat ammonite.interpreter.Catching.flatMap(Util.scala:73)\r",
      "\tat ammonite.interpreter.Interpreter.apply(Interpreter.scala:195)\r",
      "\tat jupyter.scala.ScalaInterpreter$$anon$1.interpret(ScalaInterpreter.scala:189)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:100)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:82)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)\r",
      "\tat scalaz.concurrent.Task$.Try(Task.scala:379)\r",
      "\tat scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)\r",
      "\tat scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)\r",
      "\tat scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)\r",
      "\tat scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)\r",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r",
      "\tat java.lang.Thread.run(Thread.java:745)\r",
      "Caused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r",
      "\t... 149 more\r",
      "Caused by: java.sql.SQLException: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r",
      "\t... 146 more\r",
      "Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.\r",
      "\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r",
      "\tat java.security.AccessController.doPrivileged(Native Method)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r",
      "\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r",
      "\t... 146 more\r",
      "------\r",
      "",
      "\torg.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tjava.lang.reflect.Method.invoke(Method.java:497)",
      "\tjavax.jdo.JDOHelper$16.run(JDOHelper.java:1965)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\tjavax.jdo.JDOHelper.invoke(JDOHelper.java:1960)",
      "\tjavax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)",
      "\torg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
      "\torg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\r",
      "java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.Driver20.connect(Unknown Source)\r",
      "\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\r",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\r",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\r",
      "\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\r",
      "\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\r",
      "\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\r",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\r",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\r",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\r",
      "\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\r",
      "\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\r",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\r",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\r",
      "\tat java.security.AccessController.doPrivileged(Native Method)\r",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\r",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\r",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\r",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)\r",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)\r",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\r",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\r",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)\r",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\r",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\r",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)\r",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)\r",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)\r",
      "\tat org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\r",
      "\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)\r",
      "\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)\r",
      "\tat org.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)\r",
      "\tat org.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)\r",
      "\tat org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)\r",
      "\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)\r",
      "\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)\r",
      "\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\r",
      "\tat org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)\r",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)\r",
      "\tat cmd13$$user$$anonfun$1.apply(Main.scala:203)\r",
      "\tat cmd13$$user$$anonfun$1.apply(Main.scala:202)\r",
      "\tat cmd13$$user.<init>(Main.scala:204)\r",
      "\tat cmd13.<init>(Main.scala:208)\r",
      "\tat cmd13$.<init>(Main.scala:172)\r",
      "\tat cmd13$.<clinit>(Main.scala)\r",
      "\tat cmd13$Main$.$main(Main.scala:167)\r",
      "\tat cmd13$Main.$main(Main.scala)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25$$anonfun$11.apply(Interpreter.scala:259)\r",
      "\tat ammonite.interpreter.Interpreter.evaluatorRunPrinter(Interpreter.scala:279)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25.apply(Interpreter.scala:259)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24$$anonfun$apply$25.apply(Interpreter.scala:248)\r",
      "\tat ammonite.interpreter.Catching.map(Util.scala:75)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24.apply(Interpreter.scala:248)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21$$anonfun$apply$24.apply(Interpreter.scala:246)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:246)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:244)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:244)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:243)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter.process(Interpreter.scala:243)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:202)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:201)\r",
      "\tat ammonite.interpreter.Capturing$$anonfun$apply$7.apply(Capture.scala:111)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$ammonite$interpreter$Capture$$withErr$1.apply(Capture.scala:40)\r",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\r",
      "\tat scala.Console$.withErr(Console.scala:148)\r",
      "\tat ammonite.interpreter.Capture$.ammonite$interpreter$Capture$$withErr(Capture.scala:36)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$3.apply(Capture.scala:50)\r",
      "\tat ammonite.interpreter.Capture$$anonfun$withOut$1.apply(Capture.scala:31)\r",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\r",
      "\tat scala.Console$.withOut(Console.scala:107)\r",
      "\tat ammonite.interpreter.Capture$.withOut(Capture.scala:27)\r",
      "\tat ammonite.interpreter.Capture$.withOutAndErr(Capture.scala:50)\r",
      "\tat ammonite.interpreter.Capture$.apply(Capture.scala:91)\r",
      "\tat ammonite.interpreter.Capturing.apply(Capture.scala:111)\r",
      "\tat ammonite.interpreter.Capturing.flatMap(Capture.scala:116)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:201)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:199)\r",
      "\tat ammonite.interpreter.Res$Success.flatMap(Util.scala:27)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:199)\r",
      "\tat ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:195)\r",
      "\tat ammonite.interpreter.Catching.flatMap(Util.scala:73)\r",
      "\tat ammonite.interpreter.Interpreter.apply(Interpreter.scala:195)\r",
      "\tat jupyter.scala.ScalaInterpreter$$anon$1.interpret(ScalaInterpreter.scala:189)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:100)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:82)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)\r",
      "\tat jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)\r",
      "\tat scalaz.concurrent.Task$.Try(Task.scala:379)\r",
      "\tat scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)\r",
      "\tat scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)\r",
      "\tat scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)\r",
      "\tat scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)\r",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r",
      "\tat java.lang.Thread.run(Thread.java:745)\r",
      "Caused by: java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r",
      "\t... 149 more\r",
      "Caused by: java.sql.SQLException: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\r",
      "\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\r",
      "\t... 146 more\r",
      "Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.\r",
      "\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\r",
      "\tat java.security.AccessController.doPrivileged(Native Method)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\r",
      "\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\r",
      "\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\r",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\r",
      "\t... 146 more\r",
      "------\r",
      "",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\tcom.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)",
      "\tcom.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)",
      "\tcom.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\torg.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)",
      "\torg.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)",
      "\torg.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\torg.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)",
      "\torg.datanucleus.NucleusContext.initialise(NucleusContext.java:356)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tjava.lang.reflect.Method.invoke(Method.java:497)",
      "\tjavax.jdo.JDOHelper$16.run(JDOHelper.java:1965)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\tjavax.jdo.JDOHelper.invoke(JDOHelper.java:1960)",
      "\tjavax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)",
      "\torg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
      "\torg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)",
      "\torg.apache.derby.jdbc.InternalDriver.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver20.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:664)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:208)",
      "\tcom.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tcom.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tcom.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\torg.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)",
      "\torg.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)",
      "\torg.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\torg.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)",
      "\torg.datanucleus.NucleusContext.initialise(NucleusContext.java:356)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tjava.lang.reflect.Method.invoke(Method.java:497)",
      "\tjavax.jdo.JDOHelper$16.run(JDOHelper.java:1965)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\tjavax.jdo.JDOHelper.invoke(JDOHelper.java:1960)",
      "\tjavax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)",
      "\torg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
      "\torg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@217c38df, see the next exception for details.",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)",
      "\torg.apache.derby.jdbc.InternalDriver.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver20.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:664)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:208)",
      "\tcom.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tcom.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tcom.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\torg.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)",
      "\torg.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)",
      "\torg.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\torg.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)",
      "\torg.datanucleus.NucleusContext.initialise(NucleusContext.java:356)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tjava.lang.reflect.Method.invoke(Method.java:497)",
      "\tjavax.jdo.JDOHelper$16.run(JDOHelper.java:1965)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\tjavax.jdo.JDOHelper.invoke(JDOHelper.java:1960)",
      "\tjavax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)",
      "\torg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
      "\torg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "java.sql.SQLException: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)",
      "\torg.apache.derby.jdbc.InternalDriver.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver20.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:664)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:208)",
      "\tcom.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tcom.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tcom.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\torg.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)",
      "\torg.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)",
      "\torg.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\torg.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)",
      "\torg.datanucleus.NucleusContext.initialise(NucleusContext.java:356)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tjava.lang.reflect.Method.invoke(Method.java:497)",
      "\tjavax.jdo.JDOHelper$16.run(JDOHelper.java:1965)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\tjavax.jdo.JDOHelper.invoke(JDOHelper.java:1960)",
      "\tjavax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)",
      "\torg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
      "\torg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)",
      "ERROR XSDB6: Another instance of Derby may have already booted the database C:\\Anaconda\\Galvanize\\DataEngineering\\week5\\5.3\\metastore_db.",
      "\torg.apache.derby.iapi.error.StandardException.newException(Unknown Source)",
      "\torg.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)",
      "\torg.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\torg.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)",
      "\torg.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)",
      "\torg.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)",
      "\torg.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)",
      "\torg.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)",
      "\torg.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)",
      "\torg.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)",
      "\torg.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)",
      "\torg.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)",
      "\torg.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)",
      "\torg.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)",
      "\torg.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)",
      "\torg.apache.derby.jdbc.InternalDriver.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.Driver20.connect(Unknown Source)",
      "\torg.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:664)",
      "\tjava.sql.DriverManager.getConnection(DriverManager.java:208)",
      "\tcom.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)",
      "\tcom.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)",
      "\tcom.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)",
      "\torg.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)",
      "\torg.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)",
      "\torg.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)",
      "\torg.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)",
      "\torg.datanucleus.NucleusContext.initialise(NucleusContext.java:356)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)",
      "\torg.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "\tsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "\tjava.lang.reflect.Method.invoke(Method.java:497)",
      "\tjavax.jdo.JDOHelper$16.run(JDOHelper.java:1965)",
      "\tjava.security.AccessController.doPrivileged(Native Method)",
      "\tjavax.jdo.JDOHelper.invoke(JDOHelper.java:1960)",
      "\tjavax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)",
      "\tjavax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:310)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:339)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:248)",
      "\torg.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:223)",
      "\torg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
      "\torg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)",
      "\torg.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:497)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:475)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:523)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:397)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:356)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
      "\torg.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4944)",
      "\torg.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:171)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1410)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
      "\torg.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2453)",
      "\torg.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2465)",
      "\torg.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:340)",
      "\torg.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:116)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
      "\tsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
      "\tsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "\tjava.lang.reflect.Constructor.newInstance(Constructor.java:422)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)",
      "\torg.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:168)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)",
      "\torg.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$2.<init>(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)",
      "\torg.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)",
      "\torg.apache.spark.sql.hive.HiveContext$$anon$1.<init>(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)",
      "\torg.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)",
      "\torg.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)",
      "\torg.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)",
      "\torg.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)",
      "\torg.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)",
      "\torg.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:240)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:203)",
      "\tcmd13$$user$$anonfun$1.apply(Main.scala:202)"
     ]
    }
   ],
   "source": [
    "val userDF = hiveContext.read.json(jsonUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
