{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark-logo.png\">\n",
    "\n",
    "<h1 class=\"tocheading\">Spark SQL</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Spark SQL\n",
    "=========\n",
    "\n",
    "Spark SQL\n",
    "---------\n",
    "\n",
    "What is Spark SQL?\n",
    "\n",
    "- Spark SQL takes basic RDDs and puts a schema on them.\n",
    "\n",
    "What are schemas?\n",
    "\n",
    "- Schema = Table Names + Column Names + Column Types\n",
    "\n",
    "What are the pros of schemas?\n",
    "\n",
    "- Schemas enable using column names instead of column positions\n",
    "\n",
    "- Schemas enable queries using SQL and DataFrame syntax\n",
    "\n",
    "- Schemas make your data more structured.\n",
    "\n",
    "- Schemas can then play nice with other systems that use only accept strctured data (I'm looking at you RDMS!)\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What are the cons of schemas?\n",
    "</summary>\n",
    "1. Schemas make your data more structured.\n",
    "<br>\n",
    "2. They make things more fragile.\n",
    "<br>\n",
    "3. Y2K was a schema-problem.\n",
    "</details>\n",
    "\n",
    "\n",
    "Start Spark SQL\n",
    "---------------\n",
    "\n",
    "How can I start using Spark SQL?\n",
    "\n",
    "- Create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "# sc = pyspark.SparkContext() # Not neccssary because of launching notebook with pyspark `$ IPYTHON_OPTS=\"notebook\" pyspark` \n",
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = pyspark.HiveContext(sc)\n",
    "print sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of a HiveContext you can initialize `sqlContext` using\n",
    "  `pyspark.SqlContext(sc)`\n",
    "\n",
    "- However, this is less preferred.\n",
    "\n",
    "What is the difference between SparkContext and HiveContext?\n",
    "\n",
    "- HiveContext gives you access to the metadata stored in Hive.\n",
    "\n",
    "- This enables Spark SQL to interact with tables created in Hive.\n",
    "\n",
    "- Hive tables can be backed by HDFS files, S3, HBase, and other data\n",
    "  sources.\n",
    "\n",
    "DataFrame, Schema, SchemaRDD\n",
    "----------------------------\n",
    "\n",
    "What is a DataFrame?\n",
    "\n",
    "- DataFrames are the primary abstraction in Spark SQL.\n",
    "\n",
    "- Think of a DataFrames as RDDs with schema. \n",
    "\n",
    "What is a schema?\n",
    "\n",
    "- Schemas are metadata about your data.\n",
    "\n",
    "- Schemas define table names, column names, and column types over your\n",
    "  data.\n",
    "\n",
    "- Schemas enable using SQL and DataFrame syntax to query your RDDs,\n",
    "  instead of using column positions.\n",
    "\n",
    "What is a SchemaRDD?\n",
    "\n",
    "- Spark 1.3 introduced the concept of a DataFrame as the primary SQL\n",
    "  abstraction.\n",
    "\n",
    "- Before Spark 1.3 DataFrames were called SchemaRDD.\n",
    "\n",
    "- Some of the DataFrame syntax will require using Spark 1.3 or later.\n",
    "\n",
    "- Watch out for syntax changes.\n",
    "\n",
    "- We will use the term DataFrame to refer to both SchemaRDDs and\n",
    "  DataFrames.\n",
    "\n",
    "Spark SQL Using CSV\n",
    "-------------------\n",
    "\n",
    "How can I pull in my CSV data and use Spark SQL on it?\n",
    "\n",
    "- Make sure the CSV exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile sales.csv\n",
    "#ID,Date,Store,State,Product,Amount\n",
    "101,11/13/2014,100,WA,331,300.00\n",
    "104,11/18/2014,700,OR,329,450.00\n",
    "102,11/15/2014,203,CA,321,200.00\n",
    "106,11/19/2014,202,CA,331,330.00\n",
    "103,11/17/2014,101,WA,373,750.00\n",
    "105,11/19/2014,202,CA,321,200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file and convert columns to right types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('sales.csv')\\\n",
    "    .filter(lambda line: not line.startswith('#'))\\\n",
    "    .map(lambda line: line.split(','))\\\n",
    "    .map(lambda \\\n",
    "      (id,date,store,state,product,amount):\\\n",
    "      (int(id),date,int(store),state,int(product),float(amount)))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType( [\n",
    "    StructField('id',IntegerType(),True),\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('store',IntegerType(),True),\n",
    "    StructField('state',StringType(),True),\n",
    "    StructField('product',IntegerType(),True),\n",
    "    StructField('amount',FloatType(),True) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the DataFrame object. Note: This will only work with Spark\n",
    "  1.3 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(rdd,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If your version of Spark is earlier than 1.3 use the following\n",
    "  syntax instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.applySchema(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The older syntax will work in Spark 1.3 and later as well, but it\n",
    "  will give you deprecation warnings.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What change do we have to make to the code above if we are\n",
    "processing a TSV file instead of a CSV file?\n",
    "</summary>\n",
    "<br>\n",
    "Replace `line.split(',')` with `line.split()`\n",
    "</details>\n",
    "\n",
    "Using SQL With DataFrames\n",
    "-------------------------\n",
    "\n",
    "How can I run SQL queries on DataFrames?\n",
    "\n",
    "- Register the table with SqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run queries on the registered tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sqlContext.sql(\n",
    "    'SELECT state,amount from sales where amount > 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View the results using `show()` or `collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.show()\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "If I run `result.collect()` twice how many times will the data be read\n",
    "from disk?\n",
    "</summary>\n",
    "1. RDDs are lazy.<br>\n",
    "2. Therefore the data will be read twice.<br>\n",
    "3. Unless you cache the RDD, All transformations in the RDD will\n",
    "execute on each action.<br>\n",
    "</details>\n",
    "\n",
    "Caching Tables\n",
    "--------------\n",
    "\n",
    "How can I cache the RDD for a table to avoid roundtrips to disk on\n",
    "each action?\n",
    "\n",
    "- Use `cacheTable()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.cacheTable('sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is particularly useful if you are using Spark SQL to explore\n",
    "  data.\n",
    "\n",
    "Saving Results\n",
    "--------------\n",
    "\n",
    "How can I save the results back out to the file system?\n",
    "\n",
    "- Make sure the files do not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf high-sales.json high-sales.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can either write them out using the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.toJSON().saveAsTextFile('high-sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or you can save them as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.write.parquet('high-sales.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -l sales.csv high-sales.json high-sales.parquet \n",
    "!for i in high-sales.json/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Using JSON Data\n",
    "-------------------------\n",
    "\n",
    "What is JSON-formatted data?\n",
    "\n",
    "- In Spark the JSON format means that each line is a JSON document.\n",
    "\n",
    "- JSON-formatted data can be saved as text using `saveAsTextFile()` and\n",
    "  read using `textFile()`.\n",
    "\n",
    "- JSON works well with Spark SQL because the data has an embedded\n",
    "  schema.\n",
    "\n",
    "What other formats are supported by Spark SQL?\n",
    "\n",
    "- Spark SQL also supports Parquet, which is a compact binary format\n",
    "  for big data.\n",
    "\n",
    "- If your data is in CSV then you have to add the schema\n",
    "  programmatically after you load the data.\n",
    "\n",
    "Parsing JSON Data\n",
    "-----------------\n",
    "\n",
    "How can I read JSON input and put it into a DataFrame?\n",
    "\n",
    "- First make sure the file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile sales.json\n",
    "{\"id\":101, \"date\":\"11/13/2014\", \"store\":100, \"state\":\"WA\", \"product\":331, \"amount\":300.00}\n",
    "{\"id\":104, \"date\":\"11/18/2014\", \"store\":700, \"state\":\"OR\", \"product\":329, \"amount\":450.00}\n",
    "{\"id\":102, \"date\":\"11/15/2014\", \"store\":203, \"state\":\"CA\", \"product\":321, \"amount\":200.00}\n",
    "{\"id\":106, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":331, \"amount\":330.00}\n",
    "{\"id\":103, \"date\":\"11/17/2014\", \"store\":101, \"state\":\"WA\", \"product\":373, \"amount\":750.00}\n",
    "{\"id\":105, \"date\":\"11/19/2014\", \"store\":202, \"state\":\"CA\", \"product\":321, \"amount\":200.00}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = sqlContext.read.json('sales.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- JSON is self-describing and does not require defining a schema.\n",
    "\n",
    "How can inspect my DataFrame?\n",
    "\n",
    "- Use `show()` to look at the first 20 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to look at a 50% sample of the DataFrame (without\n",
    "  replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.sample(False,0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how to inspect the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sales.schema\n",
    "print '--'\n",
    "print sales.schema.fields\n",
    "print '--'\n",
    "print sales.describe()\n",
    "print '--'\n",
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Methods\n",
    "-----------------\n",
    "\n",
    "How can I slice the DataFrame by column and by row?\n",
    "\n",
    "- DataFrames provide a *Pandas*-like API for manipulating data.\n",
    "\n",
    "- To select specific columns use `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.select('state','amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also modify the columns while selecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.select('state',sales.amount+100).show()\n",
    "sales.select('state',sales['amount']+100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can evaluate boolean expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.select('state',sales.amount<300).show()\n",
    "sales.select('state',sales.amount == 300).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can group values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.select('state','amount').groupBy('state').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can filter rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.filter(sales.state == 'CA').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use SQL to write more elaborate queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.registerTempTable('sales')\n",
    "sqlContext.sql('select * from sales where amount > 300').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert DataFrames to regular RDDs?\n",
    "\n",
    "- DataFrames are also RDDs.\n",
    "\n",
    "- You can use `map()` to iterate over the rows of the DataFrame.\n",
    "\n",
    "- You can access the values in a row using field names or column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.map(lambda row: row.amount).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also use `collect()` or `take()` to pull DataFrame rows into\n",
    "  the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I convert Spark DataFrames to Pandas data frames?\n",
    "\n",
    "- Use `toPandas()` to convert Spark DataFrames to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = sales.toPandas()\n",
    "print type(x)\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON vs CSV vs Parquet \n",
    "----------------------\n",
    "\n",
    "What are the pros and cons of JSON vs CSV vs Parquet?\n",
    "\n",
    "Feature            |JSON               |CSV            |Parquet\n",
    "-------            |----               |---            |-------\n",
    "Human-Readable     |Yes                |Yes            |No\n",
    "Compact            |No                 |Moderately     |Highly\n",
    "Columnar           |No                 |No             |Yes\n",
    "Self-Describing    |Yes                |No             |Yes\n",
    "Requires Schema    |No                 |Yes            |No\n",
    "Splittable         |Yes                |Yes            |Yes\n",
    "Popular            |No                 |Yes            |Not yet\n",
    "\n",
    "What are columnar data formats?\n",
    "\n",
    "- Columnar data formats store data column-wise.\n",
    "\n",
    "- This allows them to do RLE or run-length encoding.\n",
    "\n",
    "- Instead of storing `San Francisco` 100 times, they will just store\n",
    "  it once and the count of how many times it occurs.\n",
    "\n",
    "- When the data is repetitive and redundant as unstructured big data\n",
    "  tends to be, columnar data formats use up a fraction of the disk\n",
    "  space of non-columnar formats.\n",
    "\n",
    "What are splittable data formats?\n",
    "\n",
    "- On big data systems data is stored in blocks.\n",
    "\n",
    "- For example, on HDFS data is stored in 128 MB blocks.\n",
    "\n",
    "- Splittable data formats enable records in a block to be processed\n",
    "  without looking at the entire file.\n",
    "\n",
    "What are some examples of a non-splittable data format?\n",
    "\n",
    "- Gzip\n",
    "\n",
    "User Defined Functions\n",
    "----------------------\n",
    "\n",
    "How can I create my own User-Defined Functions?\n",
    "\n",
    "- Import the types (e.g. StringType, IntegerType, FloatType) that we\n",
    "  are returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a UDF to calculate sales tax of 10% on the amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_tax(amount):\n",
    "    return amount * 1.10\n",
    "\n",
    "sqlContext.registerFunction(\"add_tax\", add_tax, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT *, add_tax(amount) AS with_tax FROM sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optional last argument of `registerFunction` is function return\n",
    "  type; default is `StringType`.\n",
    "\n",
    "- UDFs can single or multiple arguments. \n",
    "\n",
    "SQL Types\n",
    "---------\n",
    "\n",
    "How can I find out all the types that are available for SQL schemas\n",
    "and UDF?\n",
    "\n",
    "- In the IPython REPL type `import pyspark.sql.types`. \n",
    "\n",
    "- Then type `pyspark.sql.types.[TAB]`\n",
    "\n",
    "- Autocomplete will show you all the available types.\n",
    "\n",
    "Types          |Meaning\n",
    "-----          |-------\n",
    "StringType     |String\n",
    "IntegerType    |Int\n",
    "FloatType      |Float"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
