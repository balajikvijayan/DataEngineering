{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install fastavro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash \n",
    "sudo pip install fastavro\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from cStringIO import StringIO\n",
    "import fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, and use avro schema to map to JSON\n",
    "We must firts acquire data. In this case, we will load a series of .avro files from AWS S3. Note:\n",
    "- in our binaryFiles() call, we use a \"?\" character as a regular expression to indicate either zero or one character.\n",
    "- for access tot the S3 bucket, you need to set your AWS credentials as environment variables\n",
    "\n",
    "In the few lines below, we:\n",
    "- read the files from disk into a JavaPairRDD,\n",
    "- map each binary data value in the RDD to a string using StringIO,\n",
    "- read each string and combines (flatMap) them into a json RDD.\n",
    "\n",
    "**An important note on distributed processing**: Imagine we're dealing with files that are ~500TB put together. We can't process that locally, which is where Spark's distributed framework comes in. By loading the data into an RDD, we ensure we're using Spark's core strength to process these large data sets at scale.\n",
    "\n",
    "**An important note on lazy evaluation in Spark**: As noted during the Spark lecture, it applies _lazy evaluation_, which is to say that for example transformations like map() and flatMap() are only evaluated when their results are explicitly requested through a function like <code>.take()</code> or <code>.collect()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "conn = S3Connection('Access Key Id','Secret Access Key')\n",
    "bucket = conn.get_bucket('dsci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = sc.parallelize(bucket.get_all_keys(prefix='6007/data/SuperWebAnalytics/new_data/data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avro_data = keys.map(lambda key: StringIO(key.get_contents_as_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data = avro_data.flatMap(fastavro.reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Before working with a data set, it is useful to explore it a bit and see what we are working with. In this case, our data is based on an avro schema for a graph schema we have worked with before. The data consists of records, each describing either a property of a node, or an edge.\n",
    "\n",
    "Let's use .take() to grab the first few records: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'dataunit': {u'page_view': {u'nonce': 788500601, u'person': {u'cookie': u'UVWXY'}, u'page': {u'url': u'http://mysite.com/'}}}, u'pedigree': {u'true_as_of_secs': 1438379334}}, {u'dataunit': {u'equiv': {u'id2': {u'cookie': u'KLMNO'}, u'id1': {u'user_id': 888}}}, u'pedigree': {u'true_as_of_secs': 1438379334}}]\n"
     ]
    }
   ],
   "source": [
    "print json_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the data\n",
    "Now that we have the data, we need to divide it into pieces according to the partitioning scheme outlined in the Lab specs. Our data is stored as a json array of objects. We'll take each record and map it to a 2-tuple contaning the datatype and the actual datum. By dynamically generating the partition name, our code will be able to handle any new node properties or edge types that might be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_data(datum):\n",
    "    print datum\n",
    "    datatype = datum['dataunit'].keys()[0]\n",
    "    if datatype.endswith('property'):\n",
    "        return '/'.join((datatype, datum['dataunit'][datatype]['property'].keys()[0])), datum\n",
    "    else:\n",
    "        return datatype, datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitioned_json = json_data.map(partition_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'page_view', {u'dataunit': {u'page_view': {u'nonce': 788500601, u'person': {u'cookie': u'UVWXY'}, u'page': {u'url': u'http://mysite.com/'}}}, u'pedigree': {u'true_as_of_secs': 1438379334}}), (u'equiv', {u'dataunit': {u'equiv': {u'id2': {u'cookie': u'KLMNO'}, u'id1': {u'user_id': 888}}}, u'pedigree': {u'true_as_of_secs': 1438379334}}), (u'page_view', {u'dataunit': {u'page_view': {u'nonce': 3444084808, u'person': {u'cookie': u'UVWXY'}, u'page': {u'url': u'http://mysite.com/'}}}, u'pedigree': {u'true_as_of_secs': 1438379334}})]\n"
     ]
    }
   ],
   "source": [
    "print partitioned_json.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_json.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partition_names = partitioned_json.map(lambda t: t[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitioned_json.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partition_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: need to gracefully handle when dir/file already exists\n",
    "\n",
    "for p in partition_names:\n",
    "    path = \"../SuperWebAnalytics/master/{}\".format(p)\n",
    "    if os.path.exists(path):\n",
    "        print \"{} exists\".format(path)\n",
    "    else:\n",
    "        partitioned_json.filter(lambda t: t[0] == p).values().saveAsPickleFile(path)\n",
    "#         #  line below does avro:\n",
    "#         partitioned_json.filter(lambda t: t[0] == p).values().mapPartitions(avro_writer).saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tree *_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
