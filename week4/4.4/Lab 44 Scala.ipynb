{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n",
      "\t\tmodule not found: org.scala-lang.modules#scala-pickling_2.10_2.10;0.10.1\n",
      "\n",
      "\t==== local: tried\n",
      "\n",
      "\t  C:\\Users\\Balaji\\.ivy2\\local\\org.scala-lang.modules\\scala-pickling_2.10_2.10\\0.10.1\\ivys\\ivy.xml\n",
      "\n",
      "\t==== public: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== sonatype-snapshots: tried\n",
      "\n",
      "\t  https://oss.sonatype.org/content/repositories/snapshots/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n",
      "\t\tmodule not found: org.scala-lang.modules#scala-pickling_2.10_2.10;0.10.1\n",
      "\n",
      "\t==== local: tried\n",
      "\n",
      "\t  C:\\Users\\Balaji\\.ivy2\\local\\org.scala-lang.modules\\scala-pickling_2.10_2.10\\0.10.1\\ivys\\ivy.xml\n",
      "\n",
      "\t==== public: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== sonatype-snapshots: tried\n",
      "\n",
      "\t  https://oss.sonatype.org/content/repositories/snapshots/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n",
      "\t\tmodule not found: org.scala-lang.modules#scala-pickling_2.10_2.10;0.10.1\n",
      "\n",
      "\t==== local: tried\n",
      "\n",
      "\t  C:\\Users\\Balaji\\.ivy2\\local\\org.scala-lang.modules\\scala-pickling_2.10_2.10\\0.10.1\\ivys\\ivy.xml\n",
      "\n",
      "\t==== public: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== sonatype-snapshots: tried\n",
      "\n",
      "\t  https://oss.sonatype.org/content/repositories/snapshots/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::: WARNINGS\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\n",
      "\t\tmodule not found: org.scala-lang.modules#scala-pickling_2.10_2.10;0.10.1\n",
      "\n",
      "\t==== local: tried\n",
      "\n",
      "\t  C:\\Users\\Balaji\\.ivy2\\local\\org.scala-lang.modules\\scala-pickling_2.10_2.10\\0.10.1\\ivys\\ivy.xml\n",
      "\n",
      "\t==== public: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== sonatype-snapshots: tried\n",
      "\n",
      "\t  https://oss.sonatype.org/content/repositories/snapshots/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  http://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "\t==== Typesafe repository: tried\n",
      "\n",
      "\t  ht"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":::: ERRORS\r\n",
      "\tunknown resolver null\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp://repo.typesafe.com/typesafe/releases/org/scala-lang/modules/scala-pickling_2.10_2.10/0.10.1/scala-pickling_2.10_2.10-0.10.1.pom\n",
      "\n",
      "Warning: the following JARs were previously added and are no more required:\n",
      "  C:\\Users\\Balaji\\.ivy2\\cache\\org.scala-lang\\scala-compiler\\jars\\scala-compiler-2.10.4.jar\n",
      "It is likely they were updated, which may lead to instabilities in the REPL.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.resolver(\"Typesafe repository\" at \"http://repo.typesafe.com/typesafe/releases/\")\n",
    "load.ivy(\"org.apache.spark\" %% \"spark-core\" % \"1.4.1\")\n",
    "load.ivy(\"org.apache.spark\" %% \"spark-sql\" % \"1.4.1\")\n",
    "load.ivy(\"org.apache.spark\" %% \"spark-hive\" % \"1.4.1\")\n",
    "load.ivy(\"org.scala-lang.modules\" %% \"scala-pickling\" % \"0.10.1\")\n",
    "//load.ivy(\"com.github.alexarchambault\" % \"ammonite-spark_1.4_2.10.5\" % \"0.3.1-SNAPSHOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.input.PortableDataStream\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SQLContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.hive.HiveContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.pickling._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.pickling.Defaults._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.pickling.binary._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.pickling.json._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.pickling.binary.BinaryPickle\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.pickling.binary.BinaryPickle._\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.input.PortableDataStream\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.hive.HiveContext\n",
    "import scala.io.Source\n",
    "import scala.pickling._\n",
    "import scala.pickling.Defaults._\n",
    "import scala.pickling.binary._\n",
    "import scala.pickling.json._\n",
    "import scala.pickling.binary.BinaryPickle\n",
    "import scala.pickling.binary.BinaryPickle._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/09/30 14:24:16 INFO SparkContext: Running Spark version 1.4.1\n",
      "15/09/30 14:24:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/30 14:24:16 INFO SecurityManager: Changing view acls to: Balaji\n",
      "15/09/30 14:24:16 INFO SecurityManager: Changing modify acls to: Balaji\n",
      "15/09/30 14:24:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Balaji); users with modify permissions: Set(Balaji)\n",
      "15/09/30 14:24:16 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/09/30 14:24:16 INFO Remoting: Starting remoting\n",
      "15/09/30 14:24:17 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.56.1:55184]\n",
      "15/09/30 14:24:17 INFO Utils: Successfully started service 'sparkDriver' on port 55184.\n",
      "15/09/30 14:24:17 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/09/30 14:24:17 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/09/30 14:24:17 INFO DiskBlockManager: Created local directory at C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-bb099e36-c457-4a65-9352-82fcfedd0338\\blockmgr-58be3523-8d77-4f46-9be3-72eeab13de2f\n",
      "15/09/30 14:24:17 INFO MemoryStore: MemoryStore started with capacity 1958.6 MB\n",
      "15/09/30 14:24:17 INFO HttpFileServer: HTTP File server directory is C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-bb099e36-c457-4a65-9352-82fcfedd0338\\httpd-7812b13c-e453-4245-bb5b-6f537ed22870\n",
      "15/09/30 14:24:17 INFO HttpServer: Starting HTTP Server\n",
      "15/09/30 14:24:17 INFO Utils: Successfully started service 'HTTP file server' on port 55185.\n",
      "15/09/30 14:24:17 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/09/30 14:24:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "15/09/30 14:24:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "15/09/30 14:24:17 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "15/09/30 14:24:17 INFO SparkUI: Started SparkUI at http://192.168.56.1:4042\n",
      "15/09/30 14:24:17 INFO Executor: Starting executor ID driver on host localhost\n",
      "15/09/30 14:24:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55222.\n",
      "15/09/30 14:24:17 INFO NettyBlockTransferService: Server created on 55222\n",
      "15/09/30 14:24:17 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/09/30 14:24:17 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55222 with 1958.6 MB RAM, BlockManagerId(driver, localhost, 55222)\n",
      "15/09/30 14:24:17 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7fcd4adf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@transient val sc = new SparkContext(new SparkConf().setAppName(\"Testing\").setMaster(\n",
    "    \"local[4]\").set(\"spark.executor.memory\",\"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36munpickleFrom\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unpickleFrom(filename: String): Array[String] = {\n",
    "        val valuesRawFromFile = Source.fromFile(\n",
    "             filename).map(_.toByte).toArray\n",
    "        val valuesUnpickleValue = BinaryPickle(valuesRawFromFile)\n",
    "        val valuesUnpickle = valuesUnpickleValue.unpickle[Array[String]]\n",
    "        valuesUnpickle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcurdir\u001b[0m: java.lang.String = \u001b[32m\"\"\"\n",
       "C:\\Anaconda\\Galvanize\\DataEngineering\\week4\\4.4\\\n",
       "\"\"\"\u001b[0m\n",
       "\u001b[36mpath\u001b[0m: java.lang.String = \u001b[32m\"\"\"\n",
       "SuperWebAnalytics\\master\\page_view\\\n",
       "\"\"\"\u001b[0m\n",
       "\u001b[36mres19_2\u001b[0m: java.lang.String = \u001b[32m\"\"\"\n",
       "C:\\Anaconda\\Galvanize\\DataEngineering\\week4\\4.4\\SuperWebAnalytics\\master\\page_view\\\n",
       "\"\"\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val curdir = new java.io.File(\".\").getAbsolutePath().substring(\n",
    "    0, new java.io.File(\".\").getAbsolutePath().length()-1)\n",
    "val path = \"SuperWebAnalytics\\\\master\\\\page_view\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "java.io.FileNotFoundException: C:\\Anaconda\\Galvanize\\DataEngineering\\week4\\4.4\\SuperWebAnalytics\\master\\page_view (Access is denied)",
      "\tjava.io.FileInputStream.open0(Native Method)",
      "\tjava.io.FileInputStream.open(FileInputStream.java:195)",
      "\tjava.io.FileInputStream.<init>(FileInputStream.java:138)",
      "\tscala.io.Source$.fromFile(Source.scala:90)",
      "\tscala.io.Source$.fromFile(Source.scala:75)",
      "\tscala.io.Source$.fromFile(Source.scala:53)",
      "\tcmd24$$user.unpickleFrom(Main.scala:362)",
      "\tcmd26$$user$$anonfun$1.apply(Main.scala:366)",
      "\tcmd26$$user$$anonfun$1.apply(Main.scala:365)"
     ]
    }
   ],
   "source": [
    "val pickled_page_views = unpickleFrom(curdir+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mraw_page_views\u001b[0m: org.apache.spark.rdd.RDD[(String, org.apache.spark.input.PortableDataStream)] = ./SuperWebAnalytics/master/page_view/* BinaryFileRDD[0] at binaryFiles at Main.scala:361"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val raw_page_views = sc.binaryFiles(\"./SuperWebAnalytics/master/page_view/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres28\u001b[0m: scala.Array[(String, org.apache.spark.input.PortableDataStream)] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m\"file:/C:/Anaconda/Galvanize/DataEngineering/week4/4.4/SuperWebAnalytics/master/page_view/part-00000\"\u001b[0m,\n",
       "    org.apache.spark.input.PortableDataStream@15017791\n",
       "  ),\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m\"file:/C:/Anaconda/Galvanize/DataEngineering/week4/4.4/SuperWebAnalytics/master/page_view/part-00001\"\u001b[0m,\n",
       "    org.apache.spark.input.PortableDataStream@530c8909\n",
       "  ),\n",
       "  \u001b[33m\u001b[0m(\n",
       "    \u001b[32m\"file:/C:/Anaconda/Galvanize/DataEngineering/week4/4.4/SuperWebAnalytics/master/page_view/part-00002\"\u001b[0m,\n",
       "    org.apache.spark.input.PortableDataStream@40f93740\n",
       "\u001b[33m...\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_page_views.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mMapper\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Mapper extends java.io.Serializable {\n",
    "    def mapstream(data: (String,\n",
    "        org.apache.spark.input.PortableDataStream)): String = {\n",
    "        val (a, b) = data\n",
    "        return scala.io.Source.fromInputStream(b.open()\n",
    "                                              ).getLines().mkString(\"\\n\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmapper\u001b[0m: cmd64.INSTANCE.$ref$cmd63.Mapper = cmd63$$user$Mapper@77eaf215\n",
       "\u001b[36mavro_page_views\u001b[0m: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at map at Main.scala:405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val mapper = new Mapper()\n",
    "val avro_page_views = raw_page_views.map(mapper.mapstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 9.0 failed 1 times, most recent failure: Lost task 3.0 in stage 9.0 (TID 28, localhost): java.lang.ClassNotFoundException: cmd64$$user$$anonfun$2$$anonfun$apply$1\r",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r",
      "\tat java.lang.Class.forName0(Native Method)\r",
      "\tat java.lang.Class.forName(Class.java:348)\r",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)\r",
      "\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)\r",
      "\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)\r",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)\r",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\r",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)\r",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)\r",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\r",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\r",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)\r",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)\r",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\r",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\r",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)\r",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)\r",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)\r",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)\r",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)\r",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)\r",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:58)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r",
      "\tat java.lang.Thread.run(Thread.java:745)\r",
      "",
      "Driver stacktrace:",
      "\torg.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)",
      "\torg.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)",
      "\torg.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)",
      "\tscala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "\tscala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)",
      "\torg.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)",
      "\torg.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)",
      "\torg.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)",
      "\tscala.Option.foreach(Option.scala:236)",
      "\torg.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)",
      "\torg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)",
      "\torg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)",
      "\torg.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)"
     ]
    }
   ],
   "source": [
    "avro_page_views.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "SyntaxError: found \"1].split(\\\"\\\\n\\\")).filt\", expected (Type | \"]\") in",
      "val page_views = raw_page_views.flatMap(line => line[1].split(\"\\n\")).filter(len)",
      "                                                     ^"
     ]
    }
   ],
   "source": [
    "val page_views = raw_page_views.flatMap(line => line[1].split(\"\\n\")).filter(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
