{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Recommendation with MLlib\n",
    "===============================\n",
    "<!--adapted from https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html -->\n",
    "In this lab, we will use MLlib to make personalized movie recommendations tailored _for you_. We will work with 10 million ratings from 72,000 users on 10,000 movies, collected by [MovieLens](http://movielens.umn.edu/). This dataset is can be found in S3 under `data/movielens/large`. For quick testing of your code, you may want to use a smaller dataset under `data/movielens/medium`, which contains 1 million ratings from 6000 users on 4000 movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data set\n",
    "------------------------------\n",
    "We will use two files from this MovieLens dataset: \"`ratings.dat`\" and \"`movies.dat`\". All ratings are contained in the file \"`ratings.dat`\" and are in the following format:\n",
    "\n",
    "    UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "Movie information is in the file \"`movies.dat`\" and is in the following format:\n",
    "\n",
    "    MovieID::Title::Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collaborative filtering\n",
    "------------------------------\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix, in our case, the user-movie rating matrix. MLlib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. In particular, we implement the alternating least squares (ALS) algorithm to learn these latent factors.\n",
    "<img src=\"https://databricks-training.s3.amazonaws.com/img/matrix_factorization.png\" title=\"Matrix Factorization\" alt=\"Matrix Factorization\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create training examples\n",
    "------------------------------\n",
    "To make recommendation _for you_, we are going to learn your taste by asking you to rate a few movies. We have selected a small set of movies that have received the most ratings from users in the MovieLens dataset. You can rate those movies by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os import remove, removedirs\n",
    "from os.path import dirname, join, isfile, dirname\n",
    "from time import time\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topMovies = \"\"\"1,Toy Story (1995)\n",
    "780,Independence Day (a.k.a. ID4) (1996)\n",
    "590,Dances with Wolves (1990)\n",
    "1210,Star Wars: Episode VI - Return of the Jedi (1983)\n",
    "648,Mission: Impossible (1996)\n",
    "344,Ace Ventura: Pet Detective (1994)\n",
    "165,Die Hard: With a Vengeance (1995)\n",
    "153,Batman Forever (1995)\n",
    "597,Pretty Woman (1990)\n",
    "1580,Men in Black (1997)\n",
    "231,Dumb & Dumber (1994)\"\"\"\n",
    "\n",
    "ratingsFile = 'personalRatings.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if isfile(ratingsFile):\n",
    "    r = raw_input(\"Looks like you've already rated the movies. Overwrite ratings (y/N)? \")\n",
    "    if r and r[0].lower() == \"y\":\n",
    "        remove(ratingsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please rate the following movie (1-5 (best), or 0 if not seen): \n",
      "Toy Story (1995): 5\n",
      "Independence Day (a.k.a. ID4) (1996): 4\n",
      "Dances with Wolves (1990): 0\n",
      "Star Wars: Episode VI - Return of the Jedi (1983): 5\n",
      "Mission: Impossible (1996): 3\n",
      "Ace Ventura: Pet Detective (1994): 0\n",
      "Die Hard: With a Vengeance (1995): 3\n",
      "Batman Forever (1995): 2\n",
      "Pretty Woman (1990): 0\n",
      "Men in Black (1997): 2\n",
      "Dumb & Dumber (1994): 1\n"
     ]
    }
   ],
   "source": [
    "if not isfile(ratingsFile):\n",
    "    prompt = \"Please rate the following movie (1-5 (best), or 0 if not seen): \"\n",
    "    print prompt\n",
    "\n",
    "    now = int(time())\n",
    "    n = 0\n",
    "\n",
    "    f = open(ratingsFile, 'w')\n",
    "    for line in topMovies.split(\"\\n\"):\n",
    "        ls = line.strip().split(\",\")\n",
    "        valid = False\n",
    "        while not valid:\n",
    "            rStr = raw_input(ls[1] + \": \")\n",
    "            r = int(rStr) if rStr.isdigit() else -1\n",
    "            if r < 0 or r > 5:\n",
    "                print prompt\n",
    "            else:\n",
    "                valid = True\n",
    "                if r > 0:\n",
    "                    f.write(\"0::%s::%d::%d\\n\" % (ls[0], r, now))\n",
    "                    n += 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you’re done rating the movies, we save your ratings in `personalRatings.txt` in the MovieLens format, where a special user id `0` is assigned to you.\n",
    "\n",
    "`bin/rateMovies` allows you to re-rate the movies if you’d like to see how your ratings affect your recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setup\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any Spark computation, we first create a SparkConf object and use it to create a SparkContext object. Since we will be using spark-submit to execute the programs in this tutorial (more on spark-submit in the next section), we only need to configure the executor memory allocation and give the program a name, e.g. “MovieLensALS”, to identify it in Spark’s web UI. In local mode, the web UI can be access at `localhost:4040` during the execution of a program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "      .setAppName(\"MovieLensALS\") \\\n",
    "      .set(\"spark.executor.memory\", \"2g\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the code uses the SparkContext to read in ratings. Recall that the rating file is a text file with \"`::`\" as the delimiter. The code parses each line to create a RDD for ratings that contains `(Int, Rating)` pairs. We only keep the last digit of the timestamp as a random key. The `Rating` class is a wrapper around the tuple `(user: Int, product: Int, rating: Double)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseRating(line):\n",
    "    \"\"\"\n",
    "    Parses a rating record in MovieLens format userId::movieId::rating::timestamp .\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 6, localhost): java.io.IOException: Cannot run program \"C:\\spark-1.4.1-bin-hadoop2.4\\python\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:117)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:64)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:130)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:73)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 12 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-321f6ffa9df6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovieLensHomeDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ratings.dat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparseRating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-1.4.1-bin-hadoop2.4\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.4.1-bin-hadoop2.4\\python\\pyspark\\context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions,\n\u001b[1;32m--> 897\u001b[1;33m                                           allowLocal)\n\u001b[0m\u001b[0;32m    898\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 6, localhost): java.io.IOException: Cannot run program \"C:\\spark-1.4.1-bin-hadoop2.4\\python\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:117)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:64)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:130)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:73)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 12 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n"
     ]
    }
   ],
   "source": [
    "movieLensHomeDir = 'data/'\n",
    "\n",
    "# ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))\n",
    "ratings = sc.textFile(join(movieLensHomeDir, \"ratings.dat\")).map(parseRating).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the code read in movie ids and titles, collect them into a movie id to title map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 5, localhost): java.io.IOException: Cannot run program \"C:\\spark-1.4.1-bin-hadoop2.4\\python\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:117)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:64)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:130)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:73)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 12 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-51057f25cb0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrunAsAdmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmovies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovieLensHomeDir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"movies.dat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparseMovie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-1.4.1-bin-hadoop2.4\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \"\"\"\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\py4j\\protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2.0 (TID 5, localhost): java.io.IOException: Cannot run program \"C:\\spark-1.4.1-bin-hadoop2.4\\python\": CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:117)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:64)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:130)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:73)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.IOException: CreateProcess error=5, Access is denied\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 12 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n"
     ]
    }
   ],
   "source": [
    "def parseMovie(line):\n",
    "  fields = line.split(\"::\")\n",
    "  return int(fields[0]), fields[1]\n",
    "\n",
    "movies = dict(sc.textFile(join(movieLensHomeDir, \"movies.dat\")).map(parseMovie).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s get a summary of the ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Splitting training data\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadRatings(ratingsFile):\n",
    "    \"\"\"\n",
    "    Load ratings from file.\n",
    "    \"\"\"\n",
    "    if not isfile(ratingsFile):\n",
    "        print \"File %s does not exist.\" % ratingsFile\n",
    "        sys.exit(1)\n",
    "    f = open(ratingsFile, 'r')\n",
    "    ratings = filter(lambda r: r[2] > 0, [parseRating(line)[1] for line in f])\n",
    "    f.close()\n",
    "    if not ratings:\n",
    "        print \"No ratings provided.\"\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 5.0), (0, 780, 4.0), (0, 1210, 5.0), (0, 648, 3.0), (0, 165, 3.0), (0, 153, 2.0), (0, 1580, 2.0), (0, 231, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# load personal ratings\n",
    "myRatings = loadRatings(ratingsFile)\n",
    "print myRatings\n",
    "# myRatingsRDD = sc.parallelize(myRatings, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MLlib’s `ALS` to train a `MatrixFactorizationModel`, which takes a `RDD[(user, product, rating)]`. ALS has training parameters such as rank for matrix factors and regularization constants. To determine a good combination of the training parameters, we split the data into three non-overlapping subsets, named training, test, and validation, based on the last digit of the timestamp, and cache them. We will train multiple models based on the training set, select the best model on the validation set based on RMSE (Root Mean Squared Error), and finally evaluate the best model on the test set. We also add your ratings to the training set to make recommendations for you. We hold the training, validation, and test sets in memory by calling cache because we need to visit them multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 602,251; validation: 198,919; test: 199,049\n"
     ]
    }
   ],
   "source": [
    "numPartitions = 4\n",
    "training = ratings.filter(lambda x: x[0] < 6) \\\n",
    "  .values() \\\n",
    "  .union(myRatingsRDD) \\\n",
    "  .repartition(numPartitions) \\\n",
    "  .cache()\n",
    "\n",
    "validation = ratings.filter(lambda x: x[0] >= 6 and x[0] < 8) \\\n",
    "  .values() \\\n",
    "  .repartition(numPartitions) \\\n",
    "  .cache()\n",
    "\n",
    "test = ratings.filter(lambda x: x[0] >= 8).values().cache()\n",
    "\n",
    "numTraining = training.count()\n",
    "numValidation = validation.count()\n",
    "numTest = test.count()\n",
    "\n",
    "print \"Training: {:,}; validation: {:,}; test: {:,}\".format(numTraining, numValidation, numTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Training using ALS\n",
    "------------------------------\n",
    "In this section, we will use `ALS.train` to train a bunch of models, and select and evaluate the best. Among the training paramters of ALS, the most important ones are rank, lambda (regularization constant), and number of iterations. The `train` method of ALS we are going to use is defined as the following:\n",
    "```python\n",
    "class ALS(object):\n",
    "\n",
    "    def train(cls, ratings, rank, iterations=5, lambda_=0.01, blocks=-1):\n",
    "        # ...\n",
    "        return MatrixFactorizationModel(sc, mod)\n",
    "```\n",
    "Ideally, we want to try a large number of combinations of them in order to find the best one. Due to time constraint, we will test only 8 combinations resulting from the cross product of 2 different ranks (8 and 12), 2 different lambdas (1.0 and 10.0), and two different numbers of iterations (10 and 20). We use the provided method `computeRmse` to compute the RMSE on the validation set for each model. The model with the smallest RMSE on the validation set becomes the one selected and its RMSE on the test set is used as the final metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark might take a minute or two to train the models. You should see the following on the screen:\n",
    "\n",
    "    The best model was trained using rank 8 and lambda 10.0, and its RMSE on test is 0.8808492431998702."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Recommending movies for you\n",
    "------------------------------\n",
    "As the last part of our tutorial, let’s take a look at what movies our model recommends for you. This is done by generating `(0, movieId)` pairs for all movies you haven’t rated and calling the model’s [`predict`](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/recommendation.html#MatrixFactorizationModel.predictAll) method to get predictions. `0` is the special user id assigned to you.\n",
    "```python\n",
    "class MatrixFactorizationModel(object):\n",
    "    def predictAll(self, usersProducts):\n",
    "        # ...\n",
    "        return RDD(self._java_model.predict(usersProductsJRDD._jrdd),\n",
    "                   self._context, RatingDeserializer())\n",
    "```\n",
    "After we get all predictions, let us list the top 50 recommendations and see whether they look good to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be similar to\n",
    "\n",
    "    Movies recommended for you:\n",
    "     1: Silence of the Lambs, The (1991)\n",
    "     2: Saving Private Ryan (1998)\n",
    "     3: Godfather, The (1972)\n",
    "     4: Star Wars: Episode IV - A New Hope (1977)\n",
    "     5: Braveheart (1995)\n",
    "     6: Schindler's List (1993)\n",
    "     7: Shawshank Redemption, The (1994)\n",
    "     8: Star Wars: Episode V - The Empire Strikes Back (1980)\n",
    "     9: Pulp Fiction (1994)\n",
    "    10: Alien (1979)\n",
    "    ...\n",
    "\n",
    "YMMV, and don’t expect to see movies from this decade, becaused the data set is old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Exercises\n",
    "------------------------------\n",
    "### 8.1 Comparing to a naïve baseline\n",
    "Does ALS output a non-trivial model? We can compare the evaluation result with a naive baseline model that only outputs the average rating (or you may try one that outputs the average rating per movie). Computing the baseline’s RMSE is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be similar to\n",
    "\n",
    "    The best model improves the baseline by 20.96%.\n",
    "\n",
    "It seems obvious that the trained model would outperform the naive baseline. However, a bad combination of training parameters would lead to a model worse than this naive baseline. Choosing the right set of parameters is quite important for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Augmenting matrix factors\n",
    "In this tutorial, we add your ratings to the training set. A better way to get the recommendations for you is training a matrix factorization model first and then augmenting the model using your ratings. If this sounds interesting to you, you can take a look at the implementation of [MatrixFactorizationModel](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/recommendation.html#MatrixFactorizationModel) and see how to update the model for new users and new movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
