{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">MLlib</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib History\n",
    "\n",
    "MLlib is a Spark subproject providing machine learning primitives\n",
    "\n",
    "Initial contribution from AMPLab, UC Berkeley\n",
    "\n",
    "Shipped with Spark since Sept 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Text Classification\n",
    "1. Start with RDD of strings\n",
    "2. Run _feature extraction_ to convert text into numerical features.\n",
    "3. Call classification algorithm on RDD of vectors\n",
    "4. Evaluate model on test dataset using evaluation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_N.B._ You do not _need_ to use MLlib to do machine learning on Spark. Depending on the nature of the problem, it may make more sense to use a single-node machine learning library (_i.e._ `scikit-learn`) on multiple smaller datasets for such embarassingly parallel problems as grid search, for example (though MLlib also has grid-search built in, see `ParamGridBuilder` and `CrossValidator` below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "* `Vector` - can be dense or sparse; see `mllib.linalg.Vectors`\n",
    "* `LabeledPoint` - labeled data point for supervised learning algorithms; see `mllib.regression`\n",
    "* `Rating` - rating of product by a user; see `mllib.recommendation`\n",
    "* *Various* `Model` *classes* - typically has a `predict()` method (similar to `scikit-learn` API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "<details><summary>\n",
    "Q: Which data type would you use for random forests?\n",
    "</summary>\n",
    "A: `LabeledPoint`. Random forests is a supervised learning algorithm. It requires labels in order to classify.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib: Available algorithms\n",
    "\n",
    "**feature extraction:** HashingTF, IDF, StandardScaler, Normalizer, Word2Vec\n",
    "\n",
    "**statistics:** mean, stdev, sum, corr, chiSqTest\n",
    "\n",
    "**classification:** logistic regression, linear SVM,\" naïve Bayes, least squares, classification tree  \n",
    "\n",
    "**regression:** generalized linear models (GLMs), regression tree\n",
    "\n",
    "**collaborative filtering:** alternating least squares (ALS), non-negative matrix factorization (NMF)\n",
    "\n",
    "**clustering:** k-means||\n",
    "\n",
    "**decomposition:** SVD, PCA\n",
    "\n",
    "**optimization:** stochastic gradient descent, L-BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "### Statistics\n",
    "\n",
    "**`Statistics.colStats(`*rdd*`)`:** returns min, max, mean, and variance  \n",
    "\n",
    "#### Pop Quiz:\n",
    "<p><details><summary>Q: What common statistics are missing from this list? Why?</summary>\n",
    "A: Median and other quantiles are missing.  \n",
    "Why? Because they are non-associative.\n",
    "</details></p>\n",
    "\n",
    "**`Statistics.corr(`*rdd*`, `*method*`)`:** Computes correlation matrix between columns in RDD of vectors, using either the Pearson or Spearman correlation  \n",
    "\n",
    "**`Statistics.corr(`*rdd1*`, `*rdd2*`, `*method*`)`:** Computes the correlation between two RDDs (*method* must be one of `pearson` and `spearman`).\n",
    "\n",
    "#### Pop Quiz:\n",
    "<p><details><summary>Q: What's the difference between Pearson and Spearman correlation?</summary>\n",
    "A: Pearson's $r$ is parametric. Spearman's $\\rho$ is non-parametric.  \n",
    "Q: What's the difference between parametric and non-parmetric?\n",
    "</details></p>\n",
    "\n",
    "In addition, RDDs support `sample()` and `sampleByKey()` to build simple and stratified samples of data.\n",
    "\n",
    "_N.B._ This can be very useful for bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and Regression\n",
    "-----------------------------\n",
    "\n",
    "_N.B._ For binary classification, MLlib expects the labels $0$ and $1$. In some texts, $–1$ and $1$ are used instead, but this will lead to incorrect results. For multiclass classification, MLlib expects labels from $0$ to $C–1$, where $C$ is the number of classes.\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Almost all machine learning objectives are optimized using this update\n",
    "$$w\\leftarrow w-\\alpha\\cdot\\sum_{i=1}^ng(w;x_i,y_i)$$\n",
    "$w$ is a vector of dimension $d$  \n",
    "we’re trying to find the best $w$ via optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "1. Data size\n",
    "2. Number of models\n",
    "3. Model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Goal: find best line separating two sets of points\n",
    "\n",
    "$$w\\leftarrow w-\\alpha\\cdot\\sum_{i=1}^ng(w;x_i,y_i)$$\n",
    "\n",
    "```python\n",
    "points = sc.textFile(...).mapPartitions(readPointBatch).cache()\n",
    "\n",
    "w = 2 * np.random.ranf(size=D) - 1\n",
    "\n",
    "def gradient(matrix, w):\n",
    "    Y = matrix[:, 0]    # point labels (first column of input file): -1 or 1\n",
    "    X = matrix[:, 1:]   # point coordinates\n",
    "    # For each point (x, y), compute gradient function, then sum these up\n",
    "    return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
    "\n",
    "for i in range(iterations):\n",
    "    w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separable Updates\n",
    "\n",
    "Can be generalized for\n",
    "\n",
    "* Unconstrained optimization \n",
    "\n",
    "* Smooth or non-smooth\n",
    "\n",
    "* LBFGS, Conjugate Gradient, Accelerated Gradient methods, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Results\n",
    "![](http://a3ab771892fd198a96736e50.javacodegeeks.netdna-cdn.com/wp-content/uploads/2015/05/hadoop_spark_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lots of little models\n",
    "\n",
    "Is embarrassingly parallel\n",
    "\n",
    "Most of the work should be handled by data flow paradigm\n",
    "\n",
    "ML pipelines does this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning\n",
    "```python\n",
    "# Build a parameter grid.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "\n",
    "# Set up cross-validation.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3) \n",
    "                          \n",
    "# Fit a model with cross-validation.\n",
    "cvModel = crossval.fit(training)\n",
    "```\n",
    "see http://spark.apache.org/docs/latest/ml-guide.html  \n",
    "and https://github.com/apache/spark/blob/master/examples/src/main/python/ml/cross_validator.py#L69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization\n",
    "\n",
    "At least two large classes of optimization problems humans can solve:  \n",
    "\n",
    "* Convex Programs\n",
    "\n",
    "* Spectral Problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Completion with ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Goal: predict users’ movie ratings based on past ratings of other movies\n",
    "\n",
    "$$\n",
    "R = \\left( \\begin{array}{ccccccc}\n",
    "1 & ? & ? & 4 & 5 & ? & 3 \\\\\n",
    "? & ? & 3 & 5 & ? & ? & 3 \\\\\n",
    "5 & ? & 5 & ? & ? & ? & 1 \\\\\n",
    "4 & ? & ? & ? & ? & 2 & ?\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t mistake this with SVD.\n",
    "\n",
    "Both are matrix factorizations, however SVD cannot handle missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix Factorization](https://databricks-training.s3.amazonaws.com/img/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares  \n",
    "![ALS](https://databricks.com/wp-content/uploads/2014/07/als-illustration.png)  \n",
    "1. Start with random $U_1$, $V_1$\n",
    "2. Solve for $U_2$ to minimize $||R – U_2V_1^T||$ \n",
    "3. Solve for $V_2$ to minimize $||R – U_2V_2^T||$ \n",
    "4. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS on Spark\n",
    "\n",
    "Cache 2 copies of $R$ in memory, one partitioned by rows and one by columns \n",
    "\n",
    "Keep $U~\\&~V$ partitioned in corresponding way \n",
    "\n",
    "Operate on blocks to lower communication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lab\n",
    "---\n",
    "[Movie Recommendation with MLlib](lab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Advanced:\n",
    "\n",
    "# Distributing Matrix Computations\n",
    "\n",
    "## Distributing Matrices\n",
    "\n",
    "How to distribute a matrix across machines?  \n",
    "\n",
    "* By Entries (CoordinateMatrix)\n",
    "\n",
    "* By Rows (RowMatrix)\n",
    "\n",
    "* By Blocks (BlockMatrix)\n",
    "\n",
    "All of Linear Algebra to be rebuilt using these partitioning schemes\n",
    "\n",
    "Even the simplest operations require thinking about communication e.g. multiplication\n",
    "\n",
    "\n",
    "How many different matrix multiplies needed?\n",
    "\n",
    "* At least one per pair of {Coordinate, Row, Block, LocalDense, LocalSparse} = 10\n",
    "\n",
    "* More because multiplies not commutative\n",
    "\n",
    "# Singular Value Decomposition on Spark\n",
    "\n",
    "## Singular Value Decomposition\n",
    "![SVD](http://langvillea.people.cofc.edu/DISSECTION-LAB/Emmie'sLSI-SVDModule/svddiagram.gif)\n",
    "\n",
    "## Singular Value Decomposition\n",
    "\n",
    "Two cases:\n",
    "\n",
    "* Tall and Skinny\n",
    "\n",
    "* Short and Fat (not really) \n",
    "\n",
    "* Roughly Square\n",
    "\n",
    "SVD method on RowMatrix takes care of which one to call.\n",
    "\n",
    "*(If I want the SVD of a fat matrix, I can do it by taking the transpose, finding the SVD of this skinny matrix, and then swapping the \"U\"s and \"V\"s.)*\n",
    "\n",
    "## Tall and Skinny SVD\n",
    "\n",
    "* Given $m \\times n$ matrix $A$, with $m \\gg n$\n",
    "* We compute $A^TA$\n",
    "* $A^TA$ is $n \\times n$, considerably smaller than $A$\n",
    "* $A^TA$ is dense\n",
    "* Holds dot products between all pairs of columns of $A$.\n",
    "$$A=U\\Sigma V^T \\quad A^TA=V\\Sigma^2V^T$$\n",
    "\n",
    "## Tall and Skinny SVD  \n",
    "\n",
    "* $A^TA=V\\Sigma^2V^T$ Gets us V and the singular values  \n",
    "\n",
    "* $A=U\\Sigma V^T$ Gets us U by one matrix multiplication\n",
    "\n",
    "## Square SVD\n",
    "\n",
    "ARPACK: Very mature Fortran77 package for computing eigenvalue decompositions\"\n",
    "\n",
    "JNI interface available via netlib-java\n",
    "\n",
    "Distributed using Spark – how?\n",
    "\n",
    "## Square SVD via ARPACK\n",
    "\n",
    "Only interfaces with distributed matrix via matrix-vector multiplies\n",
    "\n",
    "$$ K_n=[b\\;Ab\\;A^2b\\;\\cdots\\;A^{n-1}b] $$\n",
    "\n",
    "The result of matrix-vector multiply is small. \n",
    "\n",
    "The multiplication can be distributed.\n",
    "\n",
    "## Square SVD\n",
    "\n",
    "|￼ Matrix size    | Number of<br>nonzeros | Time per<br>iteration (s) | Total<br>time (s) |\n",
    "|:-------------------:| -------------:|:---:|:-:\n",
    "| 23,000,000 x 38,000 |    51,000,000 | 0.2 | 10\n",
    "| 63,000,000 x 49,000 |   440,000,000 | 1   | 50\n",
    "| 94,000,000 x  4,000 | 1,600,000,000 | 0.5 | 50\n",
    "\n",
    "With 68 executors and 8GB memory in each, looking for the top 5 singular vectors\n",
    "\n",
    "# Communication-Efficient $A^TA$\n",
    "### All pairs similarity on Spark (DIMSUM)\n",
    "\n",
    "## All pairs Similarity\n",
    "\n",
    "All pairs of cosine scores between n vectors\n",
    "\n",
    "* Don’t want to brute force (n choose 2) m\n",
    "\n",
    "* Essentially computes $A^TA$\n",
    "\n",
    "Compute via [DIMSUM](http://arxiv.org/abs/1304.1467)\n",
    "\n",
    "* Dimension Independent Similarity Computation using MapReduce\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Sample columns that have many non-zeros with lower probability.\n",
    "\n",
    "On the flip side, columns that have fewer non-zeros are sampled with higher probability.\n",
    "\n",
    "Results provably correct and independent of larger dimension, m.\n",
    "\n",
    "## Spark implementation\n",
    "```scala\n",
    "// Load and parse the data file.\n",
    "val rows = sc.textFile(filename).map { line => \n",
    "    var values = line.split(' ').map(_.toDouble)\n",
    "    Vectors.dense(values)\n",
    "}\n",
    "val mat = new RowMatrix(rows)\n",
    "\n",
    "// Compute similar columns perfectly, with brute force.\n",
    "val simsPerfect = mat.columnSimilarities()\n",
    "\n",
    "// Compute similar columns with estimation using DIMSUM\n",
    "val simsEstimate = mat.columnSimilarities(threshold)\n",
    "```\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
