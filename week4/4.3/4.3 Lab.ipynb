{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: problems summary ::"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::: WARNINGS\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      ":::: ERRORS\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n",
      "\tunknown resolver null\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT from sonatype-snapshots, using Fri Jun 05 01:13:44 PDT 2015\n",
      "\r\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-scala-api_2.10.5;0.2.0-SNAPSHOT\n",
      "\r\n",
      "\tUnable to reparse com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT from sonatype-snapshots, using Thu Sep 10 03:28:42 PDT 2015\n",
      "\r\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault#ammonite-api_2.10.5;0.3.1-SNAPSHOT\n",
      "\r\n",
      "\tUnable to reparse com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT from sonatype-snapshots, using Sun May 31 17:53:32 PDT 2015\n",
      "\r\n",
      "\tChoosing sonatype-snapshots for com.github.alexarchambault.jupyter#jupyter-api_2.10;0.2.0-SNAPSHOT\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load.resolver(\"Typesafe repository\" at \"http://repo.typesafe.com/typesafe/releases/\")\n",
    "// load.ivy(\"org.apache.spark\" %% \"spark-core\" % \"1.4.1\")\n",
    "load.ivy(\"org.apache.spark\" %% \"spark-mllib\" % \"1.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkContext._\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.SparkConf\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.rdd.RDD\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.ALS\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.MatrixFactorizationModel\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.recommendation.Rating\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.mllib.evaluation.RegressionMetrics\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.Pipeline\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.io.Source\n",
    "import org.apache.spark.mllib.recommendation.ALS\n",
    "import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "15/09/24 14:29:32 INFO SparkContext: Running Spark version 1.4.1\n",
      "15/09/24 14:29:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/09/24 14:29:32 INFO SecurityManager: Changing view acls to: Balaji\n",
      "15/09/24 14:29:32 INFO SecurityManager: Changing modify acls to: Balaji\n",
      "15/09/24 14:29:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Balaji); users with modify permissions: Set(Balaji)\n",
      "15/09/24 14:29:32 INFO Slf4jLogger: Slf4jLogger started\n",
      "15/09/24 14:29:33 INFO Remoting: Starting remoting\n",
      "15/09/24 14:29:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.56.1:50479]\n",
      "15/09/24 14:29:33 INFO Utils: Successfully started service 'sparkDriver' on port 50479.\n",
      "15/09/24 14:29:33 INFO SparkEnv: Registering MapOutputTracker\n",
      "15/09/24 14:29:33 INFO SparkEnv: Registering BlockManagerMaster\n",
      "15/09/24 14:29:33 INFO DiskBlockManager: Created local directory at C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-37d3a1bf-f19f-4333-8ff5-56b8a644783e\\blockmgr-686576fe-1c97-4c30-ae3e-f5f683a783df\n",
      "15/09/24 14:29:33 INFO MemoryStore: MemoryStore started with capacity 1958.6 MB\n",
      "15/09/24 14:29:33 INFO HttpFileServer: HTTP File server directory is C:\\Users\\Balaji\\AppData\\Local\\Temp\\spark-37d3a1bf-f19f-4333-8ff5-56b8a644783e\\httpd-ac659ae1-5052-4d34-a045-453304840d82\n",
      "15/09/24 14:29:33 INFO HttpServer: Starting HTTP Server\n",
      "15/09/24 14:29:33 INFO Utils: Successfully started service 'HTTP file server' on port 50480.\n",
      "15/09/24 14:29:33 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "15/09/24 14:29:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "15/09/24 14:29:33 INFO SparkUI: Started SparkUI at http://192.168.56.1:4040\n",
      "15/09/24 14:29:33 INFO Executor: Starting executor ID driver on host localhost\n",
      "15/09/24 14:29:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50517.\n",
      "15/09/24 14:29:33 INFO NettyBlockTransferService: Server created on 50517\n",
      "15/09/24 14:29:33 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "15/09/24 14:29:33 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50517 with 1958.6 MB RAM, BlockManagerId(driver, localhost, 50517)\n",
      "15/09/24 14:29:33 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[0m: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4162503e"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@transient val sc = new SparkContext(new SparkConf().setAppName(\"Testing\").setMaster(\n",
    "    \"local[4]\").set(\"spark.executor.memory\",\"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass \u001b[36mTest\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Test extends java.io.Serializable {\n",
    "    val ratingFile = \"C:\\\\Anaconda\\\\Galvanize\\\\DataEngineering\\\\week4\\\\4.3\\\\data\\\\ratings.dat\" \n",
    "    val ratingRDD = sc.textFile(ratingFile)\n",
    "    val movieFile = \"C:\\\\Anaconda\\\\Galvanize\\\\DataEngineering\\\\week4\\\\4.3\\\\data\\\\movies.dat\" \n",
    "    val movieRDD = sc.textFile(movieFile)\n",
    "\n",
    "    val myratingFile = \"C:\\\\Anaconda\\\\Galvanize\\\\DataEngineering\\\\week4\\\\4.3\\\\personalRatings.txt\" \n",
    "    val myratingRDD = sc.textFile(myratingFile)\n",
    "\n",
    "    def parseMovie(row: String): (Int, String) = {\n",
    "        val content = row.trim.split(\"::\")\n",
    "        return (content(0).toInt, content(1))\n",
    "    }\n",
    "    def getmovie: Seq[(Int, String)] = {\n",
    "        val moviedata = movieRDD.map(parseMovie).collect.toSeq\n",
    "        return moviedata\n",
    "    }\n",
    "    \n",
    "    def parseRating(row: String): (Long, (Int, Int, Double)) = {\n",
    "        val content = row.trim.split(\"::\")\n",
    "        return (content(3).toLong % 10, (content(0).toInt, content(1).toInt, content(2).toDouble))\n",
    "    }\n",
    "    def getratingRDD: RDD[Rating] = {\n",
    "        val ratingdataRDD = ratingRDD.map(parseRating)\n",
    "        val ratingsRDD = ratingdataRDD.map({case (top, (user, item, rate)) =>\n",
    "            Rating(user.toInt, item.toInt, rate.toDouble) })\n",
    "        return ratingsRDD\n",
    "    }\n",
    "    \n",
    "    def parsemyRating(row: String): (Int, Int, Double) = {\n",
    "        val content = row.trim.split(\"::\")\n",
    "        return (content(0).toInt, content(1).toInt, content(2).toFloat)\n",
    "    }\n",
    "    def getmyratingRDD: RDD[Rating] = {\n",
    "        val myratingdataRDD = myratingRDD.map(parsemyRating)\n",
    "        val myratingsRDD = myratingdataRDD.map({case (user, item, rate) =>\n",
    "            Rating(user.toInt, item.toInt, rate.toDouble) })\n",
    "        return myratingsRDD\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtester\u001b[0m: cmd75.INSTANCE.$ref$cmd73.Test = cmd73$$user$Test@35b0996e\n",
       "\u001b[36mmovies\u001b[0m: scala.collection.immutable.Map[Int,String] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m2163\u001b[0m -> \u001b[32m\"Attack of the Killer Tomatoes! (1980)\"\u001b[0m,\n",
       "  \u001b[32m645\u001b[0m -> \u001b[32m\"Nelly & Monsieur Arnaud (1995)\"\u001b[0m,\n",
       "  \u001b[32m892\u001b[0m -> \u001b[32m\"Twelfth Night (1996)\"\u001b[0m,\n",
       "  \u001b[32m69\u001b[0m -> \u001b[32m\"Friday (1995)\"\u001b[0m,\n",
       "  \u001b[32m2199\u001b[0m -> \u001b[32m\"Phoenix (1998)\"\u001b[0m,\n",
       "  \u001b[32m3021\u001b[0m -> \u001b[32m\"Funhouse, The (1981)\"\u001b[0m,\n",
       "  \u001b[32m1322\u001b[0m -> \u001b[32m\"Amityville 1992: It's About Time (1992)\"\u001b[0m,\n",
       "  \u001b[32m1665\u001b[0m -> \u001b[32m\"Bean (1997)\"\u001b[0m,\n",
       "  \u001b[32m1036\u001b[0m -> \u001b[32m\"Die Hard (1988)\"\u001b[0m,\n",
       "  \u001b[32m2822\u001b[0m -> \u001b[32m\"Medicine Man (1992)\"\u001b[0m,\n",
       "  \u001b[32m2630\u001b[0m -> \u001b[32m\"Besieged (L' Assedio) (1998)\"\u001b[0m,\n",
       "  \u001b[32m3873\u001b[0m -> \u001b[32m\"Cat Ballou (1965)\"\u001b[0m,\n",
       "  \u001b[32m1586\u001b[0m -> \u001b[32m\"G.I. Jane (1997)\"\u001b[0m,\n",
       "  \u001b[32m1501\u001b[0m -> \u001b[32m\"Keys to Tulsa (1997)\"\u001b[0m,\n",
       "\u001b[33m...\u001b[0m\n",
       "\u001b[36mratings\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[5797] at map at Main.scala:323\n",
       "\u001b[36mmyratings\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[5799] at map at Main.scala:334"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val tester = new Test()\n",
    "val movies = tester.getmovie.toMap\n",
    "val ratings = tester.getratingRDD\n",
    "val myratings = tester.getmyratingRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres25_0\u001b[0m: scala.collection.immutable.Map[Int,String] = \u001b[33mMap\u001b[0m(\n",
       "  \u001b[32m2163\u001b[0m -> \u001b[32m\"Attack of the Killer Tomatoes! (1980)\"\u001b[0m,\n",
       "  \u001b[32m645\u001b[0m -> \u001b[32m\"Nelly & Monsieur Arnaud (1995)\"\u001b[0m,\n",
       "  \u001b[32m892\u001b[0m -> \u001b[32m\"Twelfth Night (1996)\"\u001b[0m\n",
       ")\n",
       "\u001b[36mres25_1\u001b[0m: scala.Array[org.apache.spark.mllib.recommendation.Rating] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m1193\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m661\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m914\u001b[0m, \u001b[32m3.0\u001b[0m)\n",
       ")\n",
       "\u001b[36mres25_2\u001b[0m: scala.Array[org.apache.spark.mllib.recommendation.Rating] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m780\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m1210\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m648\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m165\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m153\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m1580\u001b[0m, \u001b[32m2.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m231\u001b[0m, \u001b[32m1.0\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies.take(3)\n",
    "ratings.take(3)\n",
    "myratings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnumPartitions\u001b[0m: Int = \u001b[32m4\u001b[0m\n",
       "\u001b[36mtraining\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[26] at repartition at Main.scala:134\n",
       "\u001b[36mvalidation\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[31] at repartition at Main.scala:138\n",
       "\u001b[36mtest\u001b[0m: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[32] at filter at Main.scala:141"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val numPartitions = 4\n",
    "val training = ratings.filter(x => x.product < 6).union(\n",
    "    myratings).repartition(numPartitions).cache()\n",
    "val validation = ratings.filter(x => x.product >= 6 && x.product < 8\n",
    "                               ).repartition(numPartitions).cache()\n",
    "val test = ratings.filter(x => x.product >= 8).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10_0\u001b[0m: scala.Array[org.apache.spark.mllib.recommendation.Rating] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m6\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m10\u001b[0m, \u001b[32m1\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m18\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2.0\u001b[0m)\n",
       ")\n",
       "\u001b[36mres10_1\u001b[0m: scala.Array[org.apache.spark.mllib.recommendation.Rating] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m7\u001b[0m, \u001b[32m6\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m26\u001b[0m, \u001b[32m7\u001b[0m, \u001b[32m4.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m36\u001b[0m, \u001b[32m7\u001b[0m, \u001b[32m4.0\u001b[0m)\n",
       ")\n",
       "\u001b[36mres10_2\u001b[0m: scala.Array[org.apache.spark.mllib.recommendation.Rating] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m1193\u001b[0m, \u001b[32m5.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m661\u001b[0m, \u001b[32m3.0\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m914\u001b[0m, \u001b[32m3.0\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training.take(3)\n",
    "validation.take(3)\n",
    "test.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnumRatings\u001b[0m: Long = \u001b[32m1000209\u001b[0mL\n",
       "\u001b[36mnumUsers\u001b[0m: Long = \u001b[32m6040\u001b[0mL\n",
       "\u001b[36mnumMovies\u001b[0m: Long = \u001b[32m3706\u001b[0mL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val numRatings = ratings.count\n",
    "val numUsers = ratings.map(data => data.user).distinct.count\n",
    "val numMovies = ratings.map(data => data.product).distinct.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 3730, validation: 1398, test: 995089\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnumTraining\u001b[0m: Long = \u001b[32m3730\u001b[0mL\n",
       "\u001b[36mnumValidation\u001b[0m: Long = \u001b[32m1398\u001b[0mL\n",
       "\u001b[36mnumTest\u001b[0m: Long = \u001b[32m995089\u001b[0mL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val numTraining = training.count()\n",
    "val numValidation = validation.count()\n",
    "val numTest = test.count()\n",
    "println(\"Training: \" + numTraining + \", validation: \" + numValidation +\n",
    "        \", test: \" + numTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrank\u001b[0m: Int = \u001b[32m8\u001b[0m\n",
       "\u001b[36mlambda\u001b[0m: Int = \u001b[32m1\u001b[0m\n",
       "\u001b[36mnumIter\u001b[0m: Int = \u001b[32m20\u001b[0m\n",
       "\u001b[36mmodel\u001b[0m: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@75a95e16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val rank = 8\n",
    "val lambda = 1\n",
    "val numIter = 20\n",
    "val model = ALS.train(ratings, rank, numIter, lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 1.82455834235856\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36musersMovies\u001b[0m: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[2710] at map at Main.scala:203\n",
       "\u001b[36mpredictions\u001b[0m: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[2719] at map at Main.scala:208\n",
       "\u001b[36mratesAndPreds\u001b[0m: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[2723] at join at Main.scala:215\n",
       "\u001b[36mMSE\u001b[0m: Double = \u001b[32m1.82455834235856\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val usersMovies = ratings.map {case Rating(user, movie, rate) =>\n",
    "  (user, movie)\n",
    "}\n",
    "\n",
    "val predictions = \n",
    "  model.predict(usersMovies).map {case Rating(user, movie, rate) => \n",
    "    ((user, movie), rate)\n",
    "  }\n",
    "\n",
    "val ratesAndPreds = ratings.map { case Rating(user, movie, rate) => \n",
    "  ((user, movie), rate)\n",
    "}.join(predictions)\n",
    "\n",
    "val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) => \n",
    "    val err = (r1 - r2)\n",
    "    err * err\n",
    "}.mean()\n",
    "\n",
    "println(\"Mean Squared Error = \" + MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mcomputeRmse\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating],\n",
    "                n: Long, implicitPrefs: Boolean) : Double = {\n",
    "\n",
    "    def mapPredictedRating(r: Double): Double = {\n",
    "      if (implicitPrefs) math.max(math.min(r, 1.0), 0.0) else r\n",
    "    }\n",
    "\n",
    "    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))\n",
    "    val predictionsAndRatings = predictions.map{ x =>\n",
    "        ((x.user, x.product), mapPredictedRating(x.rating))\n",
    "        }.join(data.map(x => ((x.user, x.product), x.rating))).values\n",
    "//     return new RegressionMetrics(predictionsAndRatings).rootMeanSquaredError\n",
    "    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).mean())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (validation) = 0.0 for the model trained with rank = 8, lambda = 1.0, and numIter = 10.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 8, lambda = 1.0, and numIter = 20.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 8, lambda = 10.0, and numIter = 10.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 8, lambda = 10.0, and numIter = 20.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 12, lambda = 1.0, and numIter = 10.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 12, lambda = 1.0, and numIter = 20.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 12, lambda = 10.0, and numIter = 10.\n",
      "RMSE (validation) = 0.0 for the model trained with rank = 12, lambda = 10.0, and numIter = 20.\n",
      "The best model was trained with rank = 8 and lambda = 1.0, and numIter = 10, and its RMSE on the test set is 2.828292446101891.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mranks\u001b[0m: List[Int] = \u001b[33mList\u001b[0m(\u001b[32m8\u001b[0m, \u001b[32m12\u001b[0m)\n",
       "\u001b[36mlambdas\u001b[0m: List[Double] = \u001b[33mList\u001b[0m(\u001b[32m1.0\u001b[0m, \u001b[32m10.0\u001b[0m)\n",
       "\u001b[36mnumIters\u001b[0m: List[Int] = \u001b[33mList\u001b[0m(\u001b[32m10\u001b[0m, \u001b[32m20\u001b[0m)\n",
       "\u001b[36mbestModel\u001b[0m: scala.Option[org.apache.spark.mllib.recommendation.MatrixFactorizationModel] = Some(org.apache.spark.mllib.recommendation.MatrixFactorizationModel@52386873)\n",
       "\u001b[36mbestValidationRmse\u001b[0m: Double = \u001b[32m0.0\u001b[0m\n",
       "\u001b[36mbestRank\u001b[0m: Int = \u001b[32m8\u001b[0m\n",
       "\u001b[36mbestLambda\u001b[0m: Double = \u001b[32m1.0\u001b[0m\n",
       "\u001b[36mbestNumIter\u001b[0m: Int = \u001b[32m10\u001b[0m\n",
       "\u001b[36mtestRmse\u001b[0m: Double = \u001b[32m2.828292446101891\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val ranks = List(8, 12)\n",
    "val lambdas = List(1.0, 10.0)\n",
    "val numIters = List(10, 20)\n",
    "var bestModel: Option[MatrixFactorizationModel] = None\n",
    "var bestValidationRmse = Double.MaxValue\n",
    "var bestRank = 0\n",
    "var bestLambda = -1.0\n",
    "var bestNumIter = -1\n",
    "for (rank <- ranks; lambda <- lambdas; numIter <- numIters) {\n",
    "    val model = ALS.train(training, rank, numIter, lambda)\n",
    "//     val validationRmse = computeRmse(model, validation_ratings, true)\n",
    "    val validationRmse = computeRmse(model, validation, numValidation, true)\n",
    "    println(\"RMSE (validation) = \" + validationRmse + \" for the model trained with rank = \"\n",
    "    + rank + \", lambda = \" + lambda + \", and numIter = \" + numIter + \".\")\n",
    "    if (validationRmse < bestValidationRmse) {\n",
    "        bestModel = Some(model)\n",
    "        bestValidationRmse = validationRmse\n",
    "        bestRank = rank\n",
    "        bestLambda = lambda\n",
    "        bestNumIter = numIter\n",
    "    }\n",
    "}\n",
    "\n",
    "// val testRmse = computeRmse(bestModel.get, test_ratings, true)\n",
    "val testRmse = computeRmse(bestModel.get, test, numTest, true)\n",
    "\n",
    "println(\"The best model was trained with rank = \" + bestRank + \" and lambda = \" + bestLambda\n",
    "+ \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is \" + testRmse + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmyRatedMovieIds\u001b[0m: scala.collection.immutable.Set[Int] = \u001b[33mSet\u001b[0m(\n",
       "  \u001b[32m153\u001b[0m,\n",
       "  \u001b[32m1\u001b[0m,\n",
       "  \u001b[32m1580\u001b[0m,\n",
       "  \u001b[32m165\u001b[0m,\n",
       "  \u001b[32m1210\u001b[0m,\n",
       "  \u001b[32m780\u001b[0m,\n",
       "  \u001b[32m648\u001b[0m,\n",
       "  \u001b[32m231\u001b[0m\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val myRatedMovieIds = myratings.map(_.product).collect.toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcandidates\u001b[0m: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8453] at parallelize at Main.scala:351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val candidates = sc.parallelize(movies.keys.filter(!myRatedMovieIds.contains(_)).toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrecommendations\u001b[0m: scala.Array[org.apache.spark.mllib.recommendation.Rating] = \u001b[33mArray\u001b[0m(\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2.327900739307965\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m2.247874129182113\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m5\u001b[0m, \u001b[32m2.215709720258485\u001b[0m),\n",
       "  \u001b[33mRating\u001b[0m(\u001b[32m0\u001b[0m, \u001b[32m4\u001b[0m, \u001b[32m2.058695447742449\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val recommendations = bestModel.get.predict(candidates.map((0, _))).collect().sortBy(-_.rating).take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies recommended for you:\n",
      " 1: Jumanji (1995)\n",
      " 2: Grumpier Old Men (1995)\n",
      " 3: Father of the Bride Part II (1995)\n",
      " 4: Waiting to Exhale (1995)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mi\u001b[0m: Int = \u001b[32m5\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var i = 1\n",
    "println(\"Movies recommended for you:\")\n",
    "recommendations.foreach { r =>\n",
    "  println(\"%2d\".format(i) + \": \" + movies(r.product))\n",
    "  i += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model improves the baseline by -152.07%.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmeanRating\u001b[0m: Double = \u001b[32m3.682917316692668\u001b[0m\n",
       "\u001b[36mbaselineRmse\u001b[0m: Double = \u001b[32m1.1220411865510698\u001b[0m\n",
       "\u001b[36mimprovement\u001b[0m: Double = \u001b[32m-152.06672268381664\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val meanRating = training.union(validation).map(_.rating).mean\n",
    "val baselineRmse = math.sqrt(test.map(x => (\n",
    "        meanRating - x.rating) * (meanRating - x.rating)).mean)\n",
    "val improvement = (baselineRmse - testRmse) / baselineRmse * 100\n",
    "println(\"The best model improves the baseline by \" + \"%1.2f\".format(\n",
    "    improvement) + \"%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres124\u001b[0m: Double = \u001b[32m1.1220411865510698\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baselineRmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres125\u001b[0m: Double = \u001b[32m2.828292446101891\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testRmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres126\u001b[0m: Double = \u001b[32m3.682917316692668\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meanRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
