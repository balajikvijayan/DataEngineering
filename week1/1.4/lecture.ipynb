{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from seaborn import plt\n",
    "\n",
    "# Some nice default configuration for plots\n",
    "plt.rcParams['figure.figsize'] = 10, 7.5\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"tocheading\">Cluster Computing</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Model Selection and Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Quiz\n",
    "\n",
    "<details><summary>What does it mean for a task to be \"embarrassingly parallel\"?</summary>\n",
    "Little or no effort is required to separate the problem into a number of parallel tasks.</details>\n",
    "\n",
    "<details><summary>List some algorithms that are and are not embarrassingly parallel</summary>\n",
    "<table><tr><th>Embarassingly Parallel<th>Not Embarassingly Parallel</th></tr>\n",
    "<tr><td>Random Forests</td><td>AdaBoost</td></tr>\n",
    "<tr><td>Grid Search</td><td>MaxEnt</td></tr>\n",
    "<tr><td>...</td><td>...</td></tr>\n",
    "</table></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "When doing model evaluations and parameters tuning, many models must be trained independently on the same data. This is an embarrassingly parallel problem but having a copy of the dataset in memory for each process is waste of RAM:\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/grid_search_parameters.png\" style=\"display:inline; width: 49%\" />\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/grid_search_cv_splits.png\" style=\"display:inline; width: 49%\" />\n",
    "\n",
    "When doing 3 folds cross validation on a 9 parameters grid, a naive implementation could read the data from the disk and load it in memory 27 times. If this happens concurrently (e.g. on a compute node with 32 cores) the RAM might blow up hence breaking the potential linear speed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "<details><summary>How many copies would be needed to do 5-fold cross-validation if we added `1000` as a possibility for `param_1`?</summary>\n",
    "60</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython.parallel, a Primer\n",
    "\n",
    "This section gives a primer on some tools best utilizing computational resources when doing predictive modeling in the Python / NumPy ecosystem namely:\n",
    "\n",
    "- optimal usage of available CPUs and cluster nodes with **`IPython.parallel`**\n",
    "\n",
    "- optimal memory re-use using shared memory between Python processes using **`numpy.memmap`** and **`joblib`**\n",
    "\n",
    "### What is so great about `IPython.parallel`:\n",
    "\n",
    "- Single node multi-CPUs\n",
    "- Multiple node multi-CPUs\n",
    "- Interactive In-memory computing\n",
    "- Jupyter integration with `%px` and `%%px` magics\n",
    "- Possibility to interactively connect to individual computing processes to launch interactive debugger (`#priceless`)\n",
    "\n",
    "### Let's get started:\n",
    "\n",
    "Let start an IPython cluster using the `ipcluster` common (usually run from your operating system console). To make sure that we are not running several clusters on the same host, let's try to shut down any running IPython cluster first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-08-20 15:26:06.202 [IPClusterStop] CRITICAL | Could not read pid file, cluster is probably not running.\r\n"
     ]
    }
   ],
   "source": [
    "!ipcluster stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ipcluster start -n=2 --daemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the \"Cluster\" tab of the notebook and **start a local cluster with 2 engines**. Then come back here. We should now be able to use our cluster from our notebook session (or any other Python process running on localhost):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "len(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The %px and %%px magics\n",
    "\n",
    "All the engines of the client can be accessed imperatively using the `%px` and `%%px` IPython cell magics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "import os\n",
    "import socket\n",
    "\n",
    "print(\"This is running in process with pid {0} on host '{1}'.\".format(\n",
    "      os.getpid(), socket.gethostname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of the `__main__` namespace can also be read and written via the `%px` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "a *= 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to restrict the `%px` and `%%px` magic instructions to specific engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=-1\n",
    "a *= 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DirectView objects\n",
    "\n",
    "Cell magics are very nice to work interactively from the notebook but it's also possible to replicate their behavior programmatically with more flexibility with a `DirectView` instance. A `DirectView` can be created by slicing the client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_engines = client[:]\n",
    "all_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The namespace of the `__main__` module of each running python engine can be accessed in read and write mode as a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_engines['a'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_engines['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct views can also execute the same code in parallel on each engine of the view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "my_sum_apply_results = all_engines.apply(my_sum, 11, 31)\n",
    "my_sum_apply_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput of the `apply` method is an asynchronous handle returned immediately without waiting for the end of the computation. To block until the results are ready use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_sum_apply_results.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more useful example to fetch the network hostname of each engine in the cluster. Let's study it in more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hostname():\n",
    "    \"\"\"Return the name of the host where the function is being called\"\"\"\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "hostname_apply_result = all_engines.apply(hostname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the above, the `hostname` function is first defined locally (the client python process). The `DirectView.apply` method introspects it, serializes its name and bytecode and ships it to each engine of the cluster where it is reconstructed as local function on each engine. This function is then called on each engine of the view with the optionally provided arguments.\n",
    "\n",
    "In return, the client gets a python object that serves as an handle to asynchronously fetch the list of the results of the calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hostname_apply_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hostname_apply_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to key the results explicitly with the engine ids with the `AsyncResult.get_dict` method. This is a very simple idiom to fetch metadata on the runtime environment of each engine of the direct view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hostnames = hostname_apply_result.get_dict()\n",
    "hostnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be handy to invert this mapping to find one engine id per host in the cluster so as to execute host specific operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_engine_by_host = dict((hostname, engine_id) for engine_id, hostname\n",
    "                      in hostnames.items())\n",
    "one_engine_by_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_engine_per_host_view = client[one_engine_by_host.values()]\n",
    "one_engine_per_host_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trick:** you can even use those engines ids to execute shell commands in parallel on each host of the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_engine_by_host.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%px --targets=-1  # replace with one_engine_by_host.values()\n",
    "\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on Importing Modules on Remote Engines\n",
    "\n",
    "In the previous example we put the `import socket` statement inside the body of the `hostname` function to make sure to make sure that is is available when the rest of the function is executed in the python processes of the remote engines.\n",
    "\n",
    "Alternatively it is possible to import the required modules ahead of time on all the engines of a directview using a context manager / with syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with all_engines.sync_imports():\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this method does **not** support alternative import syntaxes:\n",
    "    \n",
    "    >>> import numpy as np\n",
    "    >>> from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the method of importing in the body of the \"applied\" functions is more flexible. Additionally, this does not pollute the `__main__` namespace of the engines as it only impact the local namespace of the function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Exercise**:\n",
    "\n",
    "- Write a function that returns the memory usage of each engine process in the cluster.\n",
    "- Allocate a largish numpy array of zeros of known size (e.g. 100MB) on each engine of the cluster.\n",
    "\n",
    "Hints:\n",
    "\n",
    "Use the `psutil` module to collect the runtime info on a specific process or host. For instance to fetch the memory usage of the currently running process in MB:\n",
    "\n",
    "    >>> import os\n",
    "    >>> import psutil\n",
    "    >>> psutil.Process(os.getpid()).get_memory_info().rss / 1e6\n",
    "\n",
    "To allocate a numpy array with 1000 zeros stored as 64bit floats you can use:\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> z = np.zeros(1000, dtype=np.float64)\n",
    "\n",
    "The size in bytes of such a numpy array can then be fetched with ``z.nbytes``:\n",
    "    \n",
    "    >>> z.nbytes / 1e6\n",
    "    0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Balanced View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LoadBalancedView` is an alternative to the `DirectView` to run one function call at a time on a free engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv = client.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def slow_square(x):\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = lv.apply(slow_square, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.get()  # blocking call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to spread some tasks among the engines of the LB view by passing a callable and an iterable of task arguments to the `LoadBalancedView.map` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = lv.map(slow_square, [0, 1, 2, 3])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results.abort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iteration on AsyncMapResult is blocking\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load balanced view will be used in the following to schedule work on the cluster while being able to monitor progress and occasionally add new computing nodes to the cluster while computing to speed up the processing when using EC2 and StarCluster (see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "<details><summary>What is the difference between a direct view and a load balanced view?</summary>\n",
    "In a direct view, engines are addressed explicitly.<br />\n",
    "In a load balanced view, the scheduler is trusted with assigning work to appropriate engines.</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Read-only Data between Processes on the Same Host with Memmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy package makes it possible to memory map large contiguous chunks of binary files as shared memory for all the Python processes running on a given host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `numpy.memmap` instance with the `w+` mode creates a file on the filesystem and zeros its content. Let's do it from the first engine process or our current IPython cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "# Cleanup any existing file from past session (necessary for windows)\n",
    "import os\n",
    "if os.path.exists('small.mmap'):\n",
    "    os.unlink('small.mmap')\n",
    "\n",
    "mm_w = np.memmap('small.mmap', shape=10, dtype=np.float32, mode='w+')\n",
    "print(mm_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This binary file can then be mapped as a new numpy array by all the engines having access to the same filesystem. The `mode='r+'` opens this shared memory area in read write mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "mm_r = np.memmap('small.mmap', dtype=np.float32, mode='r+')\n",
    "print(mm_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "mm_w[0] = 42\n",
    "print(mm_w)\n",
    "print(mm_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px print(mm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory mapped arrays created with `mode='r+'` can be modified and the modifications are shared with all the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=1\n",
    "\n",
    "mm_r[1] = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(mm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful those, there is no builtin read nor write lock available on this such datastructures so it's better to avoid concurrent read & write operations on the same array segments unless there engine operations are made to cooperate with some synchronization or scheduling orchestrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memmap arrays generally behave very much like regular in-memory numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "print(\"sum={0:.3}, mean={1:.3}, std={2:.3}\".format(\n",
    "    mm_r.sum(), np.mean(mm_r), np.std(mm_r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before allocating more data in memory on the cluster let us define a couple of utility functions from the previous exercise (and more) to monitor what is used by which engine and what is still free on the cluster as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_host_free_memory(client):\n",
    "    \"\"\"Free memory on each host of the cluster in MB.\"\"\"\n",
    "    all_engines = client[:]\n",
    "    def hostname():\n",
    "        import socket\n",
    "        return socket.gethostname()\n",
    "    \n",
    "    hostnames = all_engines.apply(hostname).get_dict()\n",
    "    one_engine_per_host = dict((hostname, engine_id)\n",
    "                               for engine_id, hostname\n",
    "                               in hostnames.items())\n",
    "\n",
    "    def host_free_memory():\n",
    "        import psutil\n",
    "        return psutil.virtual_memory().free / 1e6\n",
    "    \n",
    "    \n",
    "    host_mem = client[one_engine_per_host.values()].apply(\n",
    "        host_free_memory).get_dict()\n",
    "    \n",
    "    return dict((hostnames[eid], m) for eid, m in host_mem.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's allocate a 80MB memmap array in the first engine and load it in readwrite mode in all the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "# Cleanup any existing file from past session (necessary for windows)\n",
    "import os\n",
    "if os.path.exists('big.mmap'):\n",
    "    os.unlink('big.mmap')\n",
    "\n",
    "np.memmap('big.mmap', shape=10 * int(1e6), dtype=np.float64, mode='w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "ls -lh big.mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant memory was used in this operation as we just asked the OS to allocate the buffer on the hard drive and just maitain a virtual memory area as a cheap reference to this buffer.\n",
    "\n",
    "Let's open new references to the same buffer from all the engines at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px %time big_mmap = np.memmap('big.mmap', dtype=np.float64, mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px big_mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No physical memory was allocated in the operation as it just took a couple of ms to do so. This is also confirmed by the engines process stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trigger an actual load of the data from the drive into the in-memory disk cache of the OS, this can take some time depending on the speed of the hard drive (on the order of 100MB/s to 300MB/s hence 3s to 8s for this dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "%time np.sum(big_mmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first engine has now access to the data in memory and the free memory on the host has decreased by the same amount.\n",
    "\n",
    "We can now access this data from all the engines at once much faster as the disk will no longer be used: the shared memory buffer will instead accessed directly by all the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px %time np.sum(big_mmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that the engines have loaded a whole copy of the data but this actually not the case as the total amount of free memory was not impacted by the parallel access to the shared buffer. Furthermore, once the data has been preloaded from the hard drive using one process, all the of the other processes on the same host can access it almost instantly saving a lot of IO wait.\n",
    "\n",
    "This strategy makes it very interesting to load the readonly datasets of machine learning problems, especially when the same data is reused over and over by concurrent processes as can be the case when doing learning curves analysis or grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmaping Nested Numpy-based Data Structures with Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib is a utility library included in the sklearn package. Among other things it provides tools to serialize objects that comprise large numpy arrays and reload them as memmap backed datastructures.\n",
    "\n",
    "To demonstrate it, let's create an arbitrary python datastructure involving numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyDataStructure(object):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        self.float_zeros = np.zeros(shape, dtype=np.float32)\n",
    "        self.integer_ones = np.ones(shape, dtype=np.int64)\n",
    "        \n",
    "data_structure = MyDataStructure((3, 4))\n",
    "data_structure.float_zeros, data_structure.integer_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now persist this datastructure to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(data_structure, 'data_structure.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -l data_structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A memmapped copy of this datastructure can then be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memmaped_data_structure = joblib.load('data_structure.pkl', mmap_mode='r+')\n",
    "memmaped_data_structure.float_zeros, memmaped_data_structure.integer_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmaping CV Splits for Multiprocess Dataset Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the previous tools to build a utility function that extracts Cross Validation splits ahead of time to persist them on the hard drive in a format suitable for memmaping by IPython engine processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import os\n",
    "\n",
    "def persist_cv_splits(X, y, n_cv_iter=5, name='data',\n",
    "    suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], n_iter=n_cv_iter,\n",
    "        test_size=test_size, random_state=random_state)\n",
    "    cv_split_filenames = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the digits dataset, we can run this from the :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_split_filenames = persist_cv_splits(digits.data, digits.target,\n",
    "    name='digits', random_state=42)\n",
    "digits_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -lh digits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the persisted CV splits can then be loaded back again using memmaping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = joblib.load(\n",
    "    'digits_cv_002.pkl',  mmap_mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Model Selection and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leverage IPython.parallel and the Memory Mapping features of joblib to write a custom grid search utility that runs on cluster in a memory efficient manner.\n",
    "\n",
    "Assume that we want to reproduce the grid search from the previous session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "pprint(svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` internally uses the following `ParameterGrid` utility iterator class to build the possible combinations of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "list(ParameterGrid(svc_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to load the data from a CV split file and compute the validation score for a given parameter set and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    validation_score = model.score(X_validation, y_validation)\n",
    "    return validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on the digits dataset that we splitted previously as memmapable files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "\n",
    "all_parameters, all_tasks = grid_search(\n",
    "   lb_view, model, digits_split_filenames, svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grid_search` function is using the asynchronous API of the `LoadBalancedView`, we can hence monitor the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can introspect the completed task to find the best parameters set so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "pprint(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Trick: Truncated Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often wasteful to search all the possible combinations of parameters as done previously, especially if the number of parameters is large (e.g. more than 3).\n",
    "\n",
    "To speed up the discovery of good parameters combinations, it is often faster to randomized the search order and allocate a budget of evaluations, e.g. 10 or 100 combinations.\n",
    "\n",
    "See [this JMLR paper by James Bergstra](http://jmlr.csail.mit.edu/papers/v13/bergstra12a.html) for an empirical analysis of the problem. The interested reader should also have a look at [hyperopt](https://github.com/jaberg/hyperopt) that further refines this parameter search method using meta-optimizers.\n",
    "\n",
    "Randomized Parameter Search has just been implemented in the master branch of scikit-learn be part of the 0.14 release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complete Parallel Model Selection and Assessment Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Some nice default configuration for plots\n",
    "plt.rcParams['figure.figsize'] = 10, 7.5\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.gray();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, imp\n",
    "from collections import OrderedDict\n",
    "sys.path.append('..')\n",
    "import model_selection, mmap_utils\n",
    "imp.reload(model_selection), imp.reload(mmap_utils)\n",
    "\n",
    "lb_view.abort()\n",
    "\n",
    "svc_params = OrderedDict([\n",
    "    ('gamma', np.logspace(-4, 0, 5)),\n",
    "    ('C', np.logspace(-1, 2, 4)),\n",
    "])\n",
    "\n",
    "search = model_selection.RandomizedGridSeach(lb_view)\n",
    "search.launch_for_splits(model, svc_params, digits_split_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(search.report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(search.report())\n",
    "search.boxplot_parameters(display_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarCluster  \n",
    "\n",
    "<img src=http://star.mit.edu/cluster/docs/latest/_images/scoverview.png />\n",
    "\n",
    "StarCluster comes out of the STAR program at MIT. STAR stands for \"Software Tools for Academics and Researchers\". It is used to quickly provision a cluster of EC2 instances. Like EMR, it automatically configures them to be used as a cluster (rather than as independent machines) with one controller and many workers. Unlike EMR, it does not have a GUI. Also, Hadoop is just one of many plugins for StarCluster. The most important plugin, however, is IPython Cluster.\n",
    "\n",
    "<!--\n",
    "\n",
    "## Distributing the Computation on EC2 Spot Instances with StarCluster\n",
    "\n",
    "### Installation\n",
    "\n",
    "To provision a cheap transient compute cluster on Amazon EC2, the first step is to register on EC2 with a credit card and put your EC2 credentials as environment variables. For instance under Linux / OSX:\n",
    "\n",
    "    [laptop]% export AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXXX\n",
    "    [laptop]% export AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "You can put those exports in your `~/.bashrc` to automatically get those credentials loaded in new shell sessions.\n",
    "\n",
    "Then proceed to the installation of StarCluster it-self:\n",
    "\n",
    "    [laptop]% pip install StarCluster\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Let's run the help command a first time and create a template configuration file:\n",
    "\n",
    "    [laptop]% starcluster help\n",
    "    StarCluster - (http://star.mit.edu/cluster)\n",
    "    Software Tools for Academics and Researchers (STAR)\n",
    "    Please submit bug reports to starcluster@mit.edu\n",
    "    \n",
    "    cli.py:87 - ERROR - config file /home/user/.starcluster/config does not exist\n",
    "    \n",
    "    Options:\n",
    "    --------\n",
    "    [1] Show the StarCluster config template\n",
    "    [2] Write config template to /home/user/.starcluster/config\n",
    "    [q] Quit\n",
    "    \n",
    "    Please enter your selection:\n",
    "    2\n",
    "\n",
    "and create a password-less ssh key that will be dedicated to this transient cluster:\n",
    "    \n",
    "    [laptop]% starcluster createkey mykey -o ~/.ssh/mykey.rsa\n",
    "\n",
    "    \n",
    "You can now edit the file `/home/user/.starcluster/config` and remplace its content with the following sample configuration:\n",
    "    \n",
    "    [global]\n",
    "    DEFAULT_TEMPLATE=iptemplate\n",
    "    REFRESH_INTERVAL=5\n",
    "    \n",
    "    [key mykey]\n",
    "    KEY_LOCATION=~/.ssh/mykey.rsa\n",
    "    \n",
    "    [plugin ipcluster]\n",
    "    SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster\n",
    "    ENABLE_NOTEBOOK = True\n",
    "    NOTEBOOK_PASSWD = aaaa\n",
    "    \n",
    "    [plugin ipclusterstop]\n",
    "    SETUP_CLASS = starcluster.plugins.ipcluster.IPClusterStop\n",
    "    \n",
    "    [plugin ipclusterrestart]\n",
    "    SETUP_CLASS = starcluster.plugins.ipcluster.IPClusterRestartEngines\n",
    "    \n",
    "    [plugin pypackages]\n",
    "    setup_class = starcluster.plugins.pypkginstaller.PyPkgInstaller\n",
    "    packages = scikit-learn, psutil\n",
    "    \n",
    "    # Base configuration for IPython.parallel cluster\n",
    "    [cluster iptemplate]\n",
    "    KEYNAME = mykey\n",
    "    CLUSTER_SIZE = 1\n",
    "    CLUSTER_USER = ipuser\n",
    "    CLUSTER_SHELL = bash\n",
    "    REGION = us-east-1\n",
    "    NODE_IMAGE_ID = ami-5b3fb632     # REGION and NODE_IMAGE_ID go in pair\n",
    "    NODE_INSTANCE_TYPE = c1.xlarge   # 8 CPUs\n",
    "    DISABLE_QUEUE = True             # We don't need SGE, faster cluster startup\n",
    "    PLUGINS = pypackages, ipcluster\n",
    "\n",
    "### Launching a Cluster\n",
    "\n",
    "Start a new cluster using the `myclustertemplate` section of the `~/.startcluster/config` file:\n",
    "\n",
    "    [laptop]% starcluster start -c iptemplate -s 3 -b 0.5 mycluster\n",
    "    \n",
    "- The `-s` option makes it possible to select the number of EC2 instance to start.\n",
    "\n",
    "- The `-b` option makes it possible to provision non-master instances on the Spot Instance market\n",
    "\n",
    "- To also provision the master node on the Spot Instance market you can further add the `--force-spot-master` flag to the previous commandline.\n",
    "\n",
    "- Provisioning Spot Instances is typically up to 5x cheaper than regular instances for largish instance types such as `c1.xlarge` but you run the risk of having your instances shut down if the price goes up. Also provisioning new instances on the Spot market can be slower: often a couple of minutes instead of 30s for On Demand instances.\n",
    "\n",
    "- You can access the price history of spot instances of a specific region with:\n",
    "\n",
    "        [laptop]% starcluster -r us-west-1 spothistory c1.xlarge\n",
    "        StarCluster - (http://star.mit.edu/cluster) (v. 0.9999)\n",
    "        Software Tools for Academics and Researchers (STAR)\n",
    "        Please submit bug reports to starcluster@mit.edu\n",
    "\n",
    "        >>> Current price: $0.11\n",
    "        >>> Max price: $0.75\n",
    "        >>> Average price: $0.13\n",
    "\n",
    "Connect to the master node via ssh:\n",
    "\n",
    "    [laptop]% starcluster sshmaster -A -u ipuser\n",
    "\n",
    "- The `-A` flag makes it possible to use your local ssh agent to manage your keys: makes it possible to `git clone` / `git push` github repositories from the master node as you would from your local folder.\n",
    "\n",
    "- The StarCluster AMI comes with `tmux` installed by default.\n",
    "\n",
    "It is possible to ssh into other cluster nodes from the master using local DNS aliases such as:\n",
    "\n",
    "    [myuser@master]% ssh node001\n",
    "\n",
    "### Dynamically Resizing the Cluster\n",
    "\n",
    "When using the `LoadBalancedView` API of `IPython.parallel.Client` is it possible to dynamically grow the cluster to shorten the duration of the processing of a queue of task without having to restart from scratch.\n",
    "\n",
    "This can be achieved using the `addnode` command, for instance to add 3 more nodes using \\$0.50 bid price on the Spot Instance market:\n",
    "    \n",
    "    [laptop]% starcluster addnode -s 3 -b 0.5 mycluster\n",
    "    \n",
    "Each node will automatically run the `IPCluster` plugin and register new `IPEngine` processes to the existing `IPController` process running on master.\n",
    "\n",
    "It is also possible to terminate individual running nodes of the cluster with `removenode` command but this will kill any task running on that node and IPython.parallel will **not** restart the failed task automatically.\n",
    "\n",
    "### Terminating a Cluster\n",
    "\n",
    "Once your are done with your computation, don't forget to shutdown the whole cluster and EBS volume so as to only pay for the resources you used.\n",
    "\n",
    "Before doing so, don't forget to backup any result file you would like to keep, by either pushing them to the S3 storage service (recommended for large files that you would want to reuse on EC2 later) or fetching them locally using the `starcluster get` command.\n",
    "\n",
    "The cluster shutdown itself can be achieved with a single command:\n",
    "\n",
    "    [laptop]% starcluster terminate mycluster\n",
    "\n",
    "Alternatively to can also keep your data by preserving the EBS volume attached to the master node by remplacing the `terminate` command with the `stop` command:\n",
    "\n",
    "    [laptop]% starcluster stop mycluster\n",
    "\n",
    "You can then later restart the same cluster again with the `start` command to automatically remount the EBS volume.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `ipcluster` plugin if you haven't already.\n",
    "\n",
    "Near the bottom of `.starcluster/config`:\n",
    "```bash\n",
    "######################\n",
    "## Built-in Plugins ##\n",
    "######################\n",
    "# The following plugins ship with StarCluster and should work out-of-the-box.\n",
    "# Uncomment as needed. Don't forget to update your PLUGINS list!\n",
    "# See http://star.mit.edu/cluster/docs/latest/plugins for plugin details.\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "[plugin ipcluster]\n",
    "SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster\n",
    "# Enable the IPython notebook server (optional)\n",
    "ENABLE_NOTEBOOK = True\n",
    "# Set a password for the notebook for increased security\n",
    "# This is optional but *highly* recommended\n",
    "NOTEBOOK_PASSWD = a-secret-password\n",
    "```\n",
    "\n",
    "Set `CLUSTER_SIZE` to `3` for more memory *(see [aws.amazon.com/ec2/instance-types](http://aws.amazon.com/ec2/instance-types/) for details)*:\n",
    "```bash\n",
    "[cluster smallcluster]\n",
    "# number of ec2 instances to launch\n",
    "CLUSTER_SIZE = 3\n",
    "NODE_IMAGE_ID = ami-6b211202\n",
    "PLUGINS = ipcluster\n",
    "SPOT_BID = 0.10\n",
    "```\n",
    "Also set `SPOT_BID` to `0.10` (or less?) to save \\$\\$\\$ *(see [aws.amazon.com/ec2/purchasing-options/spot-instances](http://aws.amazon.com/ec2/purchasing-options/spot-instances/) for details)*\n",
    "\n",
    "Start your new cluster:\n",
    "\n",
    "`$ starcluster start my_cluster`\n",
    "\n",
    "Copy your credentials to your cluster:\n",
    "\n",
    "`$ starcluster put my_cluster --user sgeadmin ~/Downloads/credentials.csv /home/sgeadmin/`\n",
    "\n",
    "This should, as a side effect, add your cluster to the list of known hosts on your machine. \n",
    "In my experience, it often doesn't, however. \n",
    "Therefore, **you will want to:**\n",
    "\n",
    "```bash\n",
    "starcluster sshmaster my_cluster\n",
    "```\n",
    "**before you do the following (or `Client` will hang forever):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "url_file = expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_medium_cluster-us-east-1.json')\n",
    "sshkey = expanduser('~/.ssh/mykey.pem')  # replace with your pem\n",
    "client = Client(url_file, sshkey = sshkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how many engines you have running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview = client.direct_view()\n",
    "len(client.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are fewer engines running than expected, that's probably because they didn't start on some of the instances. We can use the following to see which instances have engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "%%bash\n",
    "ec2metadata --local-ipv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "starcluster put dat12 --user sgeadmin digits_cv_00* /mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up each of our engines individually. We can do this quickly with the `%%px` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px -t0\n",
    "%%bash\n",
    "scp /mnt/sgeadmin/digits_cv_00* node001:/mnt/sgeadmin/\n",
    "scp /mnt/sgeadmin/digits_cv_00* node002:/mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remote_filenames = ['/mnt/sgeadmin/' + filename.split('/')[-1] for filename in digits_split_filenames]\n",
    "remote_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "\n",
    "all_parameters, all_tasks = grid_search(\n",
    "   lb_view, model, remote_filenames, svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "# Large Scale Text Classification for Sentiment Analysis\n",
    "\n",
    "## Outline of the Section\n",
    "\n",
    "- Limitations of the Vocabulary-Based Vectorizer\n",
    "- The **Hashing Trick**\n",
    "- **Online / Streaming** Text Feature Extraction and Classification\n",
    "- **Parallel** Text Feature Extraction and Classification\n",
    "\n",
    "## Scalability Issues\n",
    "\n",
    "The `sklearn.feature_extraction.text.CountVectorizer` and `sklearn.feature_extraction.text.TfidfVectorizer` classes suffer from a number of scalability issues that all stem from the internal usage of the `vocabulary_` attribute (a Python dictionary) used to map the unicode string feature names to the integer feature indices.\n",
    "\n",
    "The main scalability issues are:\n",
    "\n",
    "- **Memory usage of the text vectorizer**: the all the string representations of the features are loaded in memory\n",
    "- **Parallelization problems for text feature extraction**: the `vocabulary_` would be a shared state: complex synchronization and overhead\n",
    "- **Impossibility to do online or out-of-core / streaming learning**: the `vocabulary_` needs to be learned from the data: its size cannot be known before making one pass over the full dataset\n",
    "    \n",
    "    \n",
    "To better understand the issue let's have a look at how the `vocabulary_` attribute work. At `fit` time the tokens of the corpus are uniquely indentified by a integer index and this mapping stored in the vocabulary:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "vectorizer.fit([\n",
    "    \"The cat sat on the mat.\",\n",
    "])\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "The vocabulary is used at `transform` time to build the occurence matrix:\n",
    "\n",
    "X = vectorizer.transform([\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"This cat is a nice cat.\",\n",
    "]).toarray()\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)\n",
    "\n",
    "Let's refit with a slightly larger corpus:\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "vectorizer.fit([\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "])\n",
    "vectorizer.vocabulary_\n",
    "\n",
    "The `vocabulary_` is the (logarithmically) growing with the size of the training corpus. Note that we could not have built the vocabularies in parallel on the 2 text documents as they share some words hence would require some kind of shared datastructure or synchronization barrier which is complicated to setup, especially if we want to distribute the processing on a cluster.\n",
    "\n",
    "With this new vocabulary, the dimensionality of the output space is now larger:\n",
    "\n",
    "X = vectorizer.transform([\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"This cat is a nice cat.\",\n",
    "]).toarray()\n",
    "\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)\n",
    "\n",
    "## The Sentiment 140 Dataset\n",
    "\n",
    "To illustrate the scalabitiy issues of the vocabulary-based vectorizers, let's load a more reallistic dataset for a classical text classification task: sentiment analysis on tweets. The goald is to tell appart negative from positive tweets on a variety of topics.\n",
    "\n",
    "Assuming that the `../fetch_data.py` script was run successfully the following files should be available:\n",
    "\n",
    "import os\n",
    "\n",
    "sentiment140_folder = os.path.join('datasets', 'sentiment140')\n",
    "training_csv_file = os.path.join(sentiment140_folder, 'training.1600000.processed.noemoticon.csv')\n",
    "testing_csv_file = os.path.join(sentiment140_folder, 'testdata.manual.2009.06.14.csv')\n",
    "\n",
    "Those files were downloaded from the research archive of the http://www.sentiment140.com/ project. The first file was gathered using the twitter streaming API by running stream queries for the positive \":)\" and negative \":(\" emoticons to collect tweets that are explicitly positive or negative. To make the classification problem non-trivial, the emoticons were stripped out of the text in the CSV files:\n",
    "\n",
    "!ls -lh datasets/sentiment140/training.1600000.processed.noemoticon.csv\n",
    "\n",
    "!wc -l datasets/sentiment140/training.1600000.processed.noemoticon.csv\n",
    "\n",
    "Let's parse the CSV files and load everything in memory. As loading everything can take up to 2GB, let's limit the collection to 100K tweets of each (positive and negative) out of the total of 1.6M tweets.\n",
    "\n",
    "FIELDNAMES = ('polarity', 'id', 'date', 'query', 'author', 'text')\n",
    "\n",
    "def read_csv(csv_file, fieldnames=FIELDNAMES, max_count=None,\n",
    "             n_partitions=1, partition_id=0):\n",
    "    \n",
    "    import csv  # put the import inside for use in IPython.parallel\n",
    "    \n",
    "    texts = []\n",
    "    targets = []\n",
    "    with open(csv_file, 'rb') as f:\n",
    "        reader = csv.DictReader(f, fieldnames=fieldnames,\n",
    "                                delimiter=',', quotechar='\"')\n",
    "        pos_count, neg_count = 0, 0\n",
    "        for i, d in enumerate(reader):\n",
    "            if i % n_partitions != partition_id:\n",
    "                # Skip entry if not in the requested partition\n",
    "                continue\n",
    "                \n",
    "            # only include strong positives (4) and negatives (0) for now\n",
    "            if d['polarity'] == '4':\n",
    "                if max_count and pos_count >= max_count / 2:\n",
    "                    # we have enough positive tweets, don't collect any more\n",
    "                    continue\n",
    "                pos_count += 1\n",
    "                texts.append(d['text'])\n",
    "                targets.append(1)  # 4 becomes 1 for positive\n",
    "\n",
    "            elif d['polarity'] == '0':\n",
    "                if max_count and neg_count >= max_count / 2:\n",
    "                    # we have enough negative tweets, don't collect any more\n",
    "                    continue\n",
    "                neg_count += 1\n",
    "                texts.append(d['text'])\n",
    "                targets.append(-1)  # 0 becomes -1 for negative\n",
    "\n",
    "    return texts, targets  # include only text and new targets (ignore id, etc.)\n",
    "\n",
    "%time text_train_all, target_train_all = read_csv(training_csv_file, max_count=200000)\n",
    "\n",
    "len(text_train_all), len(target_train_all)\n",
    "\n",
    "Let's display the first samples:\n",
    "\n",
    "for text in text_train_all[:3]:\n",
    "    print(text + \"\\n\")\n",
    "\n",
    "print(target_train_all[:3])\n",
    "\n",
    "A polarity of \"0\" means negative while a polarity of \"4\" means positive. All the positive tweets are at the end of the file:\n",
    "\n",
    "for text in text_train_all[-3:]:\n",
    "    print(text + \"\\n\")\n",
    "\n",
    "print(target_train_all[-3:])\n",
    "\n",
    "Let's split the training CSV file into a smaller training set and a validation set with 100k random tweets each:\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "text_train_small, text_validation, target_train_small, target_validation = train_test_split(\n",
    "    text_train_all, np.array(target_train_all), test_size=.5, random_state=42)\n",
    "\n",
    "len(text_train_small)\n",
    "\n",
    "(target_train_small == -1).sum(), (target_train_small == 1).sum()\n",
    "\n",
    "len(text_validation)\n",
    "\n",
    "(target_validation == -1).sum(), (target_validation == 1).sum()\n",
    "\n",
    "Let's open the manually annotated tweet files. The evaluation set also has neutral tweets with a polarity of \"2\" which we ignore. We can build the final evaluation set with only the positive and negative tweets of the evaluation CSV file:\n",
    "\n",
    "text_test_all, target_test_all = read_csv(testing_csv_file)\n",
    "\n",
    "len(text_test_all), len(target_test_all)\n",
    "\n",
    "## The Hashing Trick\n",
    "\n",
    "To workaround the limitations of the vocabulary-based vectorizers, one can use the hashing trick. Instead of building and storing an explicit mapping from the feature names to the feature indices in a Python dict, we can just use a hash function and a modulus operation:\n",
    "\n",
    "from sklearn.utils.murmurhash import murmurhash3_bytes_u32\n",
    "\n",
    "for word in \"the cat sat on the mat\".split():\n",
    "    print(\"{0} => {1}\".format(\n",
    "        word, murmurhash3_bytes_u32(word, 0) % 2 ** 20))\n",
    "\n",
    "This mapping is completly stateless and the dimensionality of the output space is explicitly fixed in advance (here we use a modulo `2 ** 20` which means roughly 1M dimensions). The makes it possible to workaround the limitations of the vocabulary based vectorizer both for parallelizability and online / out-of-core learning.\n",
    "\n",
    "The `HashingVectorizer` class is an alternative to the `TfidfVectorizer` class with `use_idf=False` that internally uses the murmurhash hash function:\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "h_vectorizer = HashingVectorizer(encoding='latin-1')\n",
    "h_vectorizer\n",
    "\n",
    "It shares the same \"preprocessor\", \"tokenizer\" and \"analyzer\" infrastructure:\n",
    "\n",
    "analyzer = h_vectorizer.build_analyzer()\n",
    "analyzer('This is a test sentence.')\n",
    "\n",
    "We can vectorize our datasets into a scipy sparse matrix exactly as we would have done with the `CountVectorizer` or `TfidfVectorizer`, except that we can directly call the `transform` method: there is no need to `fit` as `HashingVectorizer` is a stateless transformer:\n",
    "\n",
    "%time X_train_small = h_vectorizer.transform(text_train_small)\n",
    "\n",
    "The dimension of the output is fixed ahead of time to `n_features=2 ** 20` by default (nearly 1M features) to minimize the rate of collision on most classification problem while having reasonably sized linear models (1M weights in the `coef_` attribute):\n",
    "\n",
    "X_train_small\n",
    "\n",
    "As only the non-zero elements are stored, `n_features` has little impact on the actual size of the data in memory. We can combine the hashing vectorizer with a Passive-Aggressive linear model in a pipeline:\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "h_pipeline = Pipeline((\n",
    "    ('vec', HashingVectorizer(encoding='latin-1')),\n",
    "    ('clf', PassiveAggressiveClassifier(C=1, n_iter=1)),\n",
    "))\n",
    "\n",
    "%time h_pipeline.fit(text_train_small, target_train_small).score(text_validation, target_validation)\n",
    "\n",
    "Let's check that the score on the validation set is reasonably in line with the set of manually annotated tweets:\n",
    "\n",
    "h_pipeline.score(text_test_all, target_test_all)\n",
    "\n",
    "As the `text_train_small` dataset is not that big we can still use a vocabulary based vectorizer to check that the hashing collisions are not causing any significative performance drop on the validation set (**WARNING** this is twice as slow as the hashing vectorizer version, skip this cell if your computer is too slow):\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vocabulary_vec = TfidfVectorizer(encoding='latin-1', use_idf=False)\n",
    "vocabulary_pipeline = Pipeline((\n",
    "    ('vec', vocabulary_vec),\n",
    "    ('clf', PassiveAggressiveClassifier(C=1, n_iter=1)),\n",
    "))\n",
    "\n",
    "%time vocabulary_pipeline.fit(text_train_small, target_train_small).score(text_validation, target_validation)\n",
    "\n",
    "We get almost the same score but almost twice as slower with also a big, slow to (un)pickle datastructure in memory:\n",
    "\n",
    "len(vocabulary_vec.vocabulary_)\n",
    "\n",
    "More info and reference for the original papers on the Hashing Trick in the answers to this http://metaoptimize.com/qa question: [What is the Hashing Trick?](http://metaoptimize.com/qa/questions/6943/what-is-the-hashing-trick).\n",
    "\n",
    "## Out-of-Core learning\n",
    "\n",
    "Out-of-Core learning is the task of training a machine learning model on a dataset that does not fit in the main memory. This requires the following conditions:\n",
    "    \n",
    "- a **feature extraction** layer with **fixed output dimensionality**\n",
    "- knowing the list of all classes in advance (in this case we only have positive and negative tweets)\n",
    "- a machine learning **algorithm that supports incremental learning** (the `partial_fit` method in scikit-learn).\n",
    "\n",
    "Let us simulate an infinite tweeter stream that can generate batches of annotated tweet texts and there polarity. We can do this by recombining randomly pairs of positive or negative tweets from our fixed dataset:\n",
    "\n",
    "from random import Random\n",
    "\n",
    "\n",
    "class InfiniteStreamGenerator(object):\n",
    "    \"\"\"Simulate random polarity queries on the twitter streaming API\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, targets, seed=0, batchsize=100):\n",
    "        self.texts_pos = [text for text, target in zip(texts, targets)\n",
    "                               if target > 0]\n",
    "        self.texts_neg = [text for text, target in zip(texts, targets)\n",
    "                               if target <= 0]\n",
    "        self.rng = Random(seed)\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "    def next_batch(self, batchsize=None):\n",
    "        batchsize = self.batchsize if batchsize is None else batchsize\n",
    "        texts, targets = [], []\n",
    "        for i in range(batchsize):\n",
    "            # Select the polarity randomly\n",
    "            target = self.rng.choice((-1, 1))\n",
    "            targets.append(target)\n",
    "            \n",
    "            # Combine 2 random texts of the right polarity\n",
    "            pool = self.texts_pos if target > 0 else self.texts_neg\n",
    "            text = self.rng.choice(pool) + \" \" + self.rng.choice(pool)\n",
    "            texts.append(text)\n",
    "        return texts, targets\n",
    "\n",
    "infinite_stream = InfiniteStreamGenerator(text_train_small, target_train_small)\n",
    "\n",
    "texts_in_batch, targets_in_batch = infinite_stream.next_batch(batchsize=3)\n",
    "\n",
    "for t in texts_in_batch:\n",
    "    print(t + \"\\n\")\n",
    "\n",
    "targets_in_batch\n",
    "\n",
    "We can now use our infinte tweet source to train an online machine learning algorithm using the hashing vectorizer. Note the use of the `partial_fit` method of the `PassiveAggressiveClassifier` instance in place of the traditional call to the `fit` method that needs access to the full training set.\n",
    "\n",
    "From time to time, we evaluate the current predictive performance of the model on our validation set that is guaranteed to not overlap with the infinite training set source:\n",
    "\n",
    "n_batches = 1000\n",
    "validation_scores = []\n",
    "training_set_size = []\n",
    "\n",
    "# Build the vectorizer and the classifier\n",
    "h_vectorizer = HashingVectorizer(encoding='latin-1')\n",
    "clf = PassiveAggressiveClassifier(C=1)\n",
    "\n",
    "# Extract the features for the validation once and for all\n",
    "X_validation = h_vectorizer.transform(text_validation)\n",
    "classes = np.array([-1, 1])\n",
    "\n",
    "n_samples = 0\n",
    "for i in range(n_batches):\n",
    "    \n",
    "    texts_in_batch, targets_in_batch = infinite_stream.next_batch()    \n",
    "    n_samples += len(texts_in_batch)\n",
    "\n",
    "    # Vectorize the text documents in the batch\n",
    "    X_batch = h_vectorizer.transform(texts_in_batch)\n",
    "    \n",
    "    # Incrementally train the model on the new batch\n",
    "    clf.partial_fit(X_batch, targets_in_batch, classes=classes)\n",
    "    \n",
    "    if n_samples % 100 == 0:\n",
    "        # Compute the validation score of the current state of the model\n",
    "        score = clf.score(X_validation, target_validation)\n",
    "        validation_scores.append(score)\n",
    "        training_set_size.append(n_samples)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"n_samples: {0}, score: {1:.4f}\".format(n_samples, score))\n",
    "\n",
    "We can now plot the collected validation score values, versus the number of samples generated by the infinite source and feed to the model:\n",
    "\n",
    "plt.plot(training_set_size, validation_scores)\n",
    "plt.ylim(0.5, 1)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Validation score\")\n",
    "\n",
    "## Parallelizing Text Classification\n",
    "\n",
    "### Partitioning the Data and Training in Parallel\n",
    "\n",
    "As the `HashingVectorizer` is stateless, one can use a separate instance (with the same parameters) in parallel or distributed processes to extract the features on independant partitions of a big text dataset. Each partition of extracted features can then be fed to independent instances of a linear classifier model on each computing node:\n",
    "\n",
    "<img src=\"parallel_text_clf.png\" style=\"width: 500px\" />\n",
    "\n",
    "### Final Linear Model Averaging\n",
    "\n",
    "Once all the nodes are ready we can average the linear models:\n",
    "    \n",
    "<img src=\"parallel_text_clf_average.png\" style=\"width: 500px\" />\n",
    "\n",
    "### Sample Implementation on the Tweet Data\n",
    "\n",
    "Let's use IPython parallel to read partitions of the train CSV in different Python processes using the interactive IPython.parallel interface:\n",
    "\n",
    "```bash\n",
    "starcluster put dat12 --user sgeadmin fetch_data.py /mnt/sgeadmin/\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "starcluster sn dat12 master 'cd /mnt/sgeadmin/ && python fetch_data.py sentiment140'\n",
    "starcluster sn dat12 node001 'cd /mnt/sgeadmin/ && python fetch_data.py sentiment140'\n",
    "starcluster sn dat12 node002 'cd /mnt/sgeadmin/ && python fetch_data.py sentiment140'\n",
    "```\n",
    "\n",
    "Let's tell each engine which partition of the data it will have to handle:\n",
    "\n",
    "dv = client.direct_view()\n",
    "\n",
    "dv.scatter('partition_ids', range(len(client)), block=True)\n",
    "\n",
    "%px print(partition_ids)\n",
    "\n",
    "%px partition_id = partition_ids[0]\n",
    "\n",
    "%px print(partition_id)\n",
    "\n",
    "Let's send all we need to the engines\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "h_vectorizer = HashingVectorizer(encoding='latin-1')\n",
    "dv['h_vectorizer'] = h_vectorizer\n",
    "dv['read_csv'] = read_csv\n",
    "dv['training_csv_file'] = '/mnt/sgeadmin/' + training_csv_file\n",
    "dv['n_partitions'] = len(client)\n",
    "\n",
    "%px print(training_csv_file)\n",
    "%px print(n_partitions)\n",
    "\n",
    "We are now ready to read the data partition from the CSV file, vectorize it, and train an indepenent model on each IPython.parallel engine:\n",
    "\n",
    "%%px\n",
    "\n",
    "max_count = 50000\n",
    "print(\"Parsing %d items for partition %d...\" % (max_count, partition_id))\n",
    "\n",
    "texts, targets = read_csv(training_csv_file, n_partitions=n_partitions,\n",
    "                         partition_id=partition_id, max_count=50000)\n",
    "\n",
    "%%px\n",
    "print(\"Shuffling the positive and negative examples...\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "texts, targets = shuffle(texts, targets, random_state=1)\n",
    "\n",
    "%%px\n",
    "print(\"Vectorizing text data...\")\n",
    "\n",
    "vectors = h_vectorizer.transform(texts)\n",
    "\n",
    "%%px\n",
    "print(\"Fitting a linear model...\")\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(n_iter=1).fit(vectors, targets)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "classifiers = dv.gather('clf', block=True)\n",
    "classifiers\n",
    "\n",
    "We can now compute the average linear model:\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "def average_linear_model(models):\n",
    "    \"\"\"Compute a linear model that is the average of the others\"\"\"\n",
    "    avg = copy(models[0])\n",
    "\n",
    "    avg.coef_ = np.sum([m.coef_ for m in models], axis=0)\n",
    "    avg.coef_ /= len(models)\n",
    "    \n",
    "    avg.intercept_ = np.sum([m.intercept_ for m in models], axis=0)\n",
    "    avg.intercept_ /= len(models)\n",
    "\n",
    "    return avg\n",
    "    \n",
    "\n",
    "clf = average_linear_model(classifiers)\n",
    "\n",
    "Let's compare the score of the average model with the scores of the individual classifiers. The average models can have a better generalization than the individual models being averaged:\n",
    "\n",
    "clf.score(h_vectorizer.transform(text_test_all), target_test_all)\n",
    "\n",
    "for c in classifiers:\n",
    "    print(c.score(h_vectorizer.transform(text_test_all), target_test_all))\n",
    "\n",
    "Averaging linear models learned on different datasets that follow the same distribution is a form of Ensemble method. Other Ensemble methods include:\n",
    "    \n",
    "- Boosted models (see [Gradient Tree Boosting](http://scikit-learn.org/dev/modules/ensemble.html#gradient-tree-boosting) available in 0.13 and [AdaBoost](http://scikit-learn.org/dev/modules/ensemble.html#adaboost) in master),\n",
    "- Bagging (Bootstrap Aggregating) as done in [Random Forests](http://scikit-learn.org/dev/modules/ensemble.html#random-forests). Decision Trees as the base model\n",
    "- Other non-bootstraped, randomized aggregate of Decision Trees such as [Extremely Randomized Trees](http://scikit-learn.org/dev/modules/ensemble.html#extremely-randomized-trees).\n",
    "- Averaging the probabilistic estimate of a library of randomized and / or heterogeneous linear or non-linear models.\n",
    "- Stacking, for instance: training a final Random Forest on the probabilistic class assignment output of a library of randomized and / or heterogeneous linear or non-linear models.\n",
    "\n",
    "### Limitations of the Hashing Vectorizer\n",
    "\n",
    "Using the Hashing Vectorizer makes it possible to implement streaming and parallel text classification but can also introduce some issues:\n",
    "    \n",
    "- The collisions can introduce too much noise in the data and degrade prediction quality,\n",
    "- The `HashingVectorizer` does not provide \"Inverse Document Frequency\" reweighting (lack of a `use_idf=True` option).\n",
    "- There is no easy way to inverse the mapping and find the feature names from the feature index.\n",
    "\n",
    "The collision issues can be controlled by increasing the `n_features` parameters.\n",
    "\n",
    "The IDF weighting might be reintroduced by appending a `TfidfTransformer` instance on the output of the vectorizer. However computing the `idf_` statistic used for the feature reweighting will require to do at least one additional pass over the training set before being able to start training the classifier: this breaks the online learning scheme.\n",
    "\n",
    "The lack of inverse mapping (the `get_feature_names()` method of `TfidfVectorizer`) is even harder to workaround. That would require extending the `HashingVectorizer` class to add a \"trace\" mode to record the mapping of the most important features to provide statistical debugging information.\n",
    "\n",
    "In the mean time to debug feature extraction issues, it is recommended to use `TfidfVectorizer(use_idf=False)` on a small-ish subset of the dataset to simulate a `HashingVectorizer()` instance that have the `get_feature_names()` method and no collision issues.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
