{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "This file was auto-generated from markdown using notedown.\n",
    "Instead of modifying the ipynb modify the markdown source. \n",
    "-->\n",
    "\n",
    "<h1 class=\"tocheading\">Spark Streaming</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Spark Streaming\n",
    "===============\n",
    "\n",
    "Why Spark Streaming\n",
    "-------------------\n",
    "\n",
    "Q: What problem does Spark Streaming solve?\n",
    "\n",
    "- Spark like MapReduce is designed to process data as a batch job.\n",
    "\n",
    "- Nightly batch jobs process large amounts of data and generate insights.\n",
    "\n",
    "- What if we want to react immediately instead of wait 24 hours.\n",
    "\n",
    "- Spark Streaming solves this problem.\n",
    "\n",
    "- It lets you process data immediately in near-realtime.\n",
    "\n",
    "Spark Streaming Applications\n",
    "----------------------------\n",
    "\n",
    "Q: What is an example scenario?\n",
    "\n",
    "- Suppose you have an intrusion detection system.\n",
    "\n",
    "- You process log files to determine if the system is under attack.\n",
    "\n",
    "- Batch processing will take 24 hours to raise an intrusion alert.\n",
    "\n",
    "- Spark Streaming can detect an intrustion in minutes or seconds.\n",
    "\n",
    "Micro-Batch Concept\n",
    "-------------------\n",
    "\n",
    "Q: How does Spark Streaming work?\n",
    "\n",
    "- Events are grouped into micro batched RDDs.\n",
    "\n",
    "- Each RDD contains events from the last few seconds.\n",
    "\n",
    "- Incoming event stream is turned into RDD stream.\n",
    "\n",
    "- These micro batched RDDs are joined with existing data to raise alerts.\n",
    "\n",
    "Spark Streaming RDDs\n",
    "--------------------\n",
    "\n",
    "Q: How does Spark Streaming integrate with Spark?\n",
    "\n",
    "- Spark Streaming converts incoming events into micro batched RDDs.\n",
    "\n",
    "- These are then processed by the regular Spark APIs.\n",
    "\n",
    "<img src=\"images/streaming-arch.png\">\n",
    "\n",
    "<img src=\"images/streaming-flow-micro-batches.png\">\n",
    "\n",
    "Spark Stack\n",
    "-----------\n",
    "\n",
    "Q: How does Spark Streaming fit into the rest of Spark?\n",
    "\n",
    "- Spark Streaming is a subsystem of Spark.\n",
    "\n",
    "- Spark Streaming enables handling realtime events.\n",
    "\n",
    "<img src=\"images/spark-stack.png\">\n",
    "\n",
    "Spark Streaming Big Picture\n",
    "---------------------------\n",
    "\n",
    "- Spark Streaming can consume events from multiple sources.\n",
    "\n",
    "- These are processed and written out to HDFS, databases, and other\n",
    "  systems.\n",
    "\n",
    "<img src=\"images/streaming-input-output-components.png\">\n",
    "\n",
    "\n",
    "DStream Concept\n",
    "---------------\n",
    "\n",
    "- A DStream is a stream of RDDs.\n",
    "\n",
    "- Think of a DStream as an infinite sequence of RDDs.\n",
    "\n",
    "<img src=\"images/streaming-dstream-as-rdds.png\">\n",
    "\n",
    "- The incoming events are batched together into RDDs.\n",
    "\n",
    "<img src=\"images/streaming-dstream-time-i.png\">\n",
    "\n",
    "\n",
    "Spark Streaming vs Storm\n",
    "------------------------\n",
    "\n",
    "Q: How does Spark Streaming compare with Storm?\n",
    "\n",
    "- Storm is another system for realtime processing of events.\n",
    "\n",
    "- Here is a comparison of Storm and Spark Streaming.\n",
    "\n",
    "Comparison           |Winner     |Spark Streaming      |Storm\n",
    "----------           |------     |---------------      |-----\n",
    "Processing Model     |  -        |Mini batches         |Record-at-a-time\n",
    "Latency              |Storm      |Few seconds          |Sub-second\n",
    "Fault tolerance      |Spark      |Exactly once         |At least once (may be duplicates)\n",
    "Batch integration    |Spark      |Spark                |Requires different framework\n",
    "API                  |Spark      |Simpler              |Complex\n",
    "Production use       |Storm      |2013                 |2011\n",
    "\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What happens to an event that is half in batch `time=1` and half in\n",
    "batch `time=2`? Which batch does it go to?\n",
    "</summary>\n",
    "1. It goes to batch `time=2`.<br>\n",
    "2. Incomplete events are meaningless.<br>\n",
    "3. RDDs are formed from fully-formed events.\n",
    "</details>\n",
    "\n",
    "Spark Streaming Code\n",
    "--------------------\n",
    "\n",
    "Q: How can I write a Spark Streaming app?\n",
    "\n",
    "- Here is an example of a Spark Streaming app.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create StreamingContext with 2 threads, and batch interval of 1 second\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create DStream to listen to hostname:port\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count words in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first 10 elements of each RDD in DStream \n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait streaming to terminate\n",
    "ssc.awaitTermination()\n",
    "```\n",
    "\n",
    "Network Source\n",
    "--------------\n",
    "\n",
    "Q: How can I create data to feed into this stream?\n",
    "\n",
    "- You need to write to the network socket 9999 on localhost for this\n",
    "  streaming app to pick up the events.\n",
    "\n",
    "- Here is some shell code to do this.\n",
    "\n",
    "```sh\n",
    "nc -lk 9999\n",
    "```\n",
    "\n",
    "- You can type words into this or write a script that pipes data into\n",
    "  this periodically.\n",
    "\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- The `StreamingContext` is stored in `ssc`.\n",
    "\n",
    "- `ssc.socketTextStream` creates a `DStream`.\n",
    "\n",
    "- DStreams transformations like `flatMap`, `map`, `reduceByKey` \n",
    "  create new DStreams.\n",
    "\n",
    "- DStreams output operations like `pprint` are like RDD actions.\n",
    "\n",
    "- Except DStream output operations do not cause execution.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: When you execute `pprint` on a DStream will anything be printed?\n",
    "</summary>\n",
    "1. Nothing is printed.<br>\n",
    "2. The printing happens when we call `ssc.start()` and when data flows in.\n",
    "</details>\n",
    "\n",
    "RDDs vs DStreams\n",
    "----------------\n",
    "\n",
    "Q: How are DStream different from RDDs?\n",
    "\n",
    "- DStream transformations and output operations define an assembly line.\n",
    "  \n",
    "- Nothing happens until data comes in.\n",
    "\n",
    "- When data comes in DStream output operations trigger the execution\n",
    "  of DStream transformations.\n",
    "\n",
    "<img src=\"images/donuts.jpg\">\n",
    "\n",
    "Transformations and Output Operations\n",
    "=====================================\n",
    "\n",
    "DStream Transformations\n",
    "-----------------------\n",
    "\n",
    "Q: How are DStream transformations different from RDD transformations?\n",
    "\n",
    "- DStream transformations define what will happen to RDDs when they\n",
    "  arrive.\n",
    "  \n",
    "- DStream transformations produce new DStreams that will contain \n",
    "  transformed RDDs.\n",
    "\n",
    "- Nothing happens until data arrives.\n",
    "\n",
    "<img src=\"images/streaming-dstream-ops.png\">\n",
    "\n",
    "Transforming DStreams\n",
    "---------------------\n",
    "\n",
    "Transformation                                 |For Each Incoming RDD\n",
    "--------------                                 |---------------------\n",
    "`ds.map(lambda line: line.upper())`            |Uppercase `line` \n",
    "`ds.flatMap(lambda line: line.split())`        |Split `line` into words\n",
    "`ds.filter(lambda line: line.strip() != '')`   |Exclude `line` if it is empty\n",
    "`ds.repartition(10)`                           |Repartition RDD into 10 partitions\n",
    "`ds.reduceByKey(lambda v1,v2: v1+v2)`          |For each key sum values \n",
    "`ds.groupByKey()`                              |For each key group values into iterable\n",
    "\n",
    "Generic Transformations\n",
    "-----------------------\n",
    "\n",
    "Q: How can I apply an arbitrary transformation on the incoming RDDs?\n",
    "\n",
    "- DStreams have some but not all of the transformations as RDDs.\n",
    "\n",
    "- For example, `sortByKey()` is not supported on DStreams.\n",
    "\n",
    "- Instead DStreams provide `transform()` \n",
    "\n",
    "- `transform()` lets you translate any RDD transformation to DStreams.\n",
    "\n",
    "- These two have the same effect.\n",
    "\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.flatMap(lambda line: line.split()))\n",
    "```\n",
    "\n",
    "```python\n",
    "ds.flatMap(lambda line: line.split())\n",
    "```\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can you write `sortByKey()` for DStreams?\n",
    "</summary>\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.sortByKey())\n",
    "```\n",
    "</details>\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "Consider this code:\n",
    "\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.flatMap(lambda line: line.split()))\n",
    "```\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `lambda line: ...` execute? \n",
    "</summary>\n",
    "On the executors.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `lambda rdd: ...` execute? \n",
    "</summary>\n",
    "On the driver.\n",
    "</details>\n",
    "\n",
    "\n",
    "DStream Output Operations\n",
    "-------------------------\n",
    "\n",
    "Expression                                     |Meaning\n",
    "----------                                     |-------\n",
    "`ds.foreachRDD(lambda rdd: func(rdd.first()))` |Call `func()` on `first()` of each incoming RDD\n",
    "`ds.pprint(num=10)`                            |Print first 10 elements of each incoming RDD\n",
    "`ds.saveAsTextFiles('foo',suffix=None)`        |Save each incoming RDD's partitions to disk\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- These output operations only execute when RDDs start arriving.\n",
    "\n",
    "- `foreachRDD` is a generic output operation.\n",
    "\n",
    "- `foreachRDD` lets you define arbitrary output operations on incoming RDDs.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Print the count of incoming RDDs.\n",
    "</summary>\n",
    "```python\n",
    "# Enable print as a function\n",
    "from __future__ import print_function\n",
    "\n",
    "# Define the output operation\n",
    "ds.foreachRDD(lambda rdd: print(rdd.count()))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Where will the lambda inside the `foreachRDD` execute?\n",
    "</summary>\n",
    "1. It will execute on the driver.<br>\n",
    "2. This is because RDDs are defined on the driver, not on the executors.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Testing Streaming Apps Using QueueStream\n",
    "----------------------------------------\n",
    "\n",
    "Q: Manually testing apps using `nc` is quite tedious. Is there an\n",
    "easier more automatable way to do this?\n",
    "\n",
    "- *Queue streams* enable you to create preprogrammed streams perfect\n",
    "  for automated testing and test-driven development.\n",
    "\n",
    "Counting Event Types\n",
    "--------------------\n",
    "\n",
    "Q: Count how many events of different types are in incoming stream in\n",
    "each micro-batch.\n",
    "\n",
    "- Here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file test_queue_stream.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "print 'Initializing event_rdd_queue'\n",
    "event_rdd_queue = []\n",
    "for i in xrange(5):\n",
    "    events = range(10) * 10\n",
    "    event_rdd = ssc.sparkContext.parallelize(events)\n",
    "    event_rdd_queue.append(event_rdd)\n",
    "pprint(event_rdd_queue)\n",
    "\n",
    "print 'Building DStream pipeline'\n",
    "ds = ssc\\\n",
    "    .queueStream(event_rdd_queue) \\\n",
    "    .map(lambda event: (event, 1)) \\\n",
    "    .reduceByKey(lambda v1,v2: v1+v2)\n",
    "ds.pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "\n",
    "print 'Stopping ssc'\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_queue_stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating RDD\n",
    "===============\n",
    "\n",
    "Merging DStreams\n",
    "----------------\n",
    "\n",
    "Transformation      |Effect\n",
    "--------------      |------\n",
    "`ds1.union(ds2)`    |Combine RDD in `ds1` with RDD in same batch in `ds2`\n",
    "`ds1.join(ds2)`     |Join RDD in `ds1` with RDD in same batch in `ds2`\n",
    "\n",
    "Note\n",
    "----\n",
    "\n",
    "- For `union` or `join` the DStreams must have identical batch\n",
    "  durations.\n",
    "\n",
    "- The batches are matched up based on timestamps.\n",
    "\n",
    "\n",
    "Windowing Operations\n",
    "--------------------\n",
    "\n",
    "Q: How can I process multiple RDDs within a window of time?\n",
    "\n",
    "```python\n",
    "ds2 = ds1.window(windowDuration=30, slideDuration=10)\n",
    "```\n",
    "\n",
    "- Batches RDDs into 30-second windows \n",
    "\n",
    "- Produces new window every 10 seconds\n",
    "\n",
    "<img src=\"images/streaming-dstream-window.png\">\n",
    "\n",
    "Windowing Operations\n",
    "--------------------\n",
    "\n",
    "Q: Calculate the average of a series of heads and tails using a\n",
    "window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file test_window.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "print 'Initializing rdd_queue'\n",
    "rdd_queue = []\n",
    "for i in xrange(5): \n",
    "    rdd_data = xrange(1000)\n",
    "    rdd = ssc.sparkContext.parallelize(rdd_data)\n",
    "    rdd_queue.append(rdd)\n",
    "pprint(rdd_queue)\n",
    "\n",
    "print 'Creating queue stream'\n",
    "ds = ssc\\\n",
    "    .queueStream(rdd_queue)\\\n",
    "    .map(lambda x: (x % 10, 1))\\\n",
    "    .window(windowDuration=4,slideDuration=2)\\\n",
    "    .reduceByKey(lambda v1,v2:v1+v2)\n",
    "ds.pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "time.sleep(20)\n",
    "\n",
    "print 'Stopping ssc'\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_window.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowing Operations With Inverse\n",
    "---------------------------------\n",
    "\n",
    "Q: How can I avoid the overhead of adding or averaging over the same\n",
    "values in a window?\n",
    "\n",
    "```python\n",
    "windows_word_counts = pair_ds.reduceByKeyAndWindow(\n",
    "    func=lambda x, y: x + y,\n",
    "    invFunc=lambda x, y: x - y, \n",
    "    windowDuration=30,\n",
    "    slideDuration=10)\n",
    "```\n",
    "\n",
    "- Creates window of length `windowDuration` (30 seconds)\n",
    "\n",
    "- Moves window every `slideDuration` (10 seconds)\n",
    "\n",
    "- Merges incoming values using `func`\n",
    "\n",
    "- Eliminates outgoing values using `invFunc`\n",
    "\n",
    "- `windowDuration` and `slideDuration` are in seconds\n",
    "\n",
    "- These must be multiples of the `batchDuration` of the DStream\n",
    "\n",
    "- This requires that *checkpointing* is enabled on the StreamingContext.\n",
    "\n",
    "<img src=\"images/streaming-windowed-stream.png\">\n",
    "\n",
    "<img src=\"images/streaming-windowed-stream-with-inv.png\">\n",
    "\n",
    "Streaming Durations\n",
    "-------------------\n",
    "\n",
    "Q: What are the different durations in a DStream and which one should\n",
    "I use?\n",
    "\n",
    "Type               |Meaning\n",
    "----               |-------\n",
    "Batch Duration     |How many seconds until next incoming RDD\n",
    "Slide Duration     |How many seconds until next window RDD\n",
    "Window Duration    |How many seconds to include in window RDD\n",
    "\n",
    "Duration Impact\n",
    "---------------\n",
    "\n",
    "Q: What is the impact of increasing these durations?\n",
    "\n",
    "Type                 |Increase                                   |Effect \n",
    "----                 |--------                                   |------ \n",
    "Batch Duration       |Larger but less frequent incoming RDDs     |Less Processing \n",
    "Slide Duration       |Less frequent window RDDs                  |Less Processing\n",
    "Window Duration      |Larger window RDDs                         |More Processing\n",
    "\n",
    "Duration Summary\n",
    "----------------\n",
    "\n",
    "- Batch and window duration control RDD size\n",
    "\n",
    "- Batch and slide duration control RDD frequency\n",
    "\n",
    "- Larger RDDs have more context and produce better insights.\n",
    "\n",
    "- Larger RDDs might require more processing.\n",
    "\n",
    "- Bundling frequent small RDDs into infrequent larger ones can reduce processing.\n",
    "\n",
    "State DStreams\n",
    "--------------\n",
    "\n",
    "Q: How can I aggregate a value over the lifetime of a streaming\n",
    "application?\n",
    "\n",
    "- You can do this with the `updateStateByKey` transform.\n",
    "\n",
    "```python\n",
    "# add new values with previous running count to get new count\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "\n",
    "runningCounts = pairs.updateStateByKey(updateFunction)\n",
    "```\n",
    "\n",
    "- This takes a DStream made up of key-value RDDs\n",
    "\n",
    "- For each incoming RDD for each key it aggregates the values with the\n",
    "  previous values seen for that key.\n",
    "\n",
    "- Like the windowing transformations, this requires that checkpointing\n",
    "  be enabled on the StreamingContext.\n",
    "\n",
    "Testing Streaming Apps Using TextFileStream\n",
    "-------------------------------------------\n",
    "\n",
    "Q: The QueueStream does not work with windowing operations or any\n",
    "other operations that require checkpointing. How can code that uses\n",
    "`updateStateByKey` be tested? \n",
    "\n",
    "- We can use TextFileStream instead.\n",
    "- Lets define a function `xrange_write` which we will use for the following examples.\n",
    "- This will write numbers 0, 1, 2, ... to directory `input`.\n",
    "- It will write 5 numbers per second, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file text_file_util.py\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from distutils import dir_util \n",
    "\n",
    "# Every batch_duration write a file with batch_size numbers, forever.\n",
    "# Start at 0 and keep incrementing. (For testing.)\n",
    "\n",
    "def xrange_write(\n",
    "        batch_size = 5,\n",
    "        batch_dir = 'input',\n",
    "        batch_duration = 1):\n",
    "    dir_util.mkpath('./input')\n",
    "    \n",
    "    # Repeat forever\n",
    "    for i in itertools.count():\n",
    "        # Generate data\n",
    "        min = batch_size * i \n",
    "        max = batch_size * (i + 1)\n",
    "        batch_data = xrange(min,max)\n",
    "      \n",
    "        # Write to the file\n",
    "        unique_file_name = str(uuid.uuid4())\n",
    "        file_path = batch_dir + '/' + unique_file_name\n",
    "        with open(file_path,'w') as batch_file: \n",
    "            for element in batch_data:\n",
    "                line = str(element) + \"\\n\"\n",
    "                batch_file.write(line)\n",
    "    \n",
    "        # Give streaming app time to catch up\n",
    "        time.sleep(batch_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Events\n",
    "---------------\n",
    "\n",
    "Q: How can I count a certain type of event in incoming data?\n",
    "\n",
    "- You can use state DStreams.\n",
    "\n",
    "- This code takes a mod by 10 of the incoming numbers.\n",
    "\n",
    "- Then it counts how many times each number between 0 and 9 is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file test_count.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from text_file_util import xrange_write\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# add new values with previous running count to get new count\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "ssc.checkpoint('ckpt')\n",
    "\n",
    "ds = ssc.textFileStream('input') \\\n",
    "    .map(lambda x: int(x) % 10) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .updateStateByKey(updateFunction)\n",
    "\n",
    "ds.pprint()\n",
    "ds.count().pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "\n",
    "# Write data to textFileStream\n",
    "xrange_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The program will run forever. To terminate hit `Ctrl-C`.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can you calculate a running average using a state DStream?\n",
    "</summary>\n",
    "1. In the above example, for the RDD key-value pair, replace `value`\n",
    "with `(sum,count)`. <br>\n",
    "2. In `updateStateByKey` add both to `sum` and `count`.<br>\n",
    "3. Use `map` to calculate `sum/count` which is the average.<br>\n",
    "</details>\n",
    "\n",
    "<!--\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can you calculate a running standard deviation using a state DStream?\n",
    "</summary>\n",
    "1. See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm<br>  <-- Wrong!\n",
    "</details>\n",
    "-->\n",
    "\n",
    "\n",
    "\n",
    "Join \n",
    "----\n",
    "\n",
    "Q: How can I detect if an incoming credit card transaction is from a\n",
    "canceled card?\n",
    "\n",
    "- You can join DStreams against a batch RDD.\n",
    "- Store the historical data in the batch RDD.\n",
    "- Join it with the incoming DStream RDDs to determine next action.\n",
    "- Note: You must get the batch RDD using the `ssc.SparkContext`.\n",
    "\n",
    "```python\n",
    "dataset = ... # some RDD\n",
    "windowedStream = stream.window(20)\n",
    "joinedStream = windowedStream.transform(lambda rdd: rdd.join(dataset))\n",
    "```\n",
    "\n",
    "Detecting Bad Customers\n",
    "-----------------------\n",
    "Q: Create a streaming app that can join the incoming orders with our\n",
    "previous knowledge of whether this customer is good or bad.\n",
    "\n",
    "- Create the streaming app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file test_join.py\n",
    "# Import modules.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "\n",
    "# Create the StreamingContext.\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "\n",
    "# For testing create prepopulated QueueStream of streaming customer orders. \n",
    "\n",
    "print 'Initializing queue of customer transactions'\n",
    "transaction_rdd_queue = []\n",
    "for i in xrange(5): \n",
    "    transactions = [(customer_id, None) for customer_id in xrange(10)]\n",
    "    transaction_rdd = ssc.sparkContext.parallelize(transactions)\n",
    "    transaction_rdd_queue.append(transaction_rdd)\n",
    "pprint(transaction_rdd_queue)\n",
    "\n",
    "# Batch RDD of whether customers are good or bad. \n",
    "\n",
    "print 'Initializing bad customer rdd from batch sources'\n",
    "# (customer_id, is_good_customer)\n",
    "customers = [\n",
    "        (0,True),\n",
    "        (1,False),\n",
    "        (2,True),\n",
    "        (3,False),\n",
    "        (4,True),\n",
    "        (5,False),\n",
    "        (6,True),\n",
    "        (7,False),\n",
    "        (8,True),\n",
    "        (9,False) ]\n",
    "customer_rdd = ssc.sparkContext.parallelize(customers)\n",
    "\n",
    "# Join the streaming RDD and batch RDDs to filter out bad customers.\n",
    "print 'Creating queue stream'\n",
    "ds = ssc\\\n",
    "    .queueStream(transaction_rdd_queue)\\\n",
    "    .transform(lambda rdd: rdd.join(customer_rdd))\\\n",
    "    .filter(lambda (customer_id, (customer_data, is_good_customer)): is_good_customer)\n",
    "\n",
    "ds.pprint()\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# $SPARK_HOME/bin/spark-submit\n",
    "python test_join.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: If you are joining with a large batch RDD how can you minimize the\n",
    "shuffling of the records?\n",
    "</summary>\n",
    "1. Use `partitionBy` on the incoming RDDs as well as on the batch\n",
    "RDD.<br>\n",
    "2. This will ensure that records are partitioned by their keys.<br>\n",
    "3. This can make a real difference in the performance of your Big Data\n",
    "streaming app.<br>\n",
    "</details>\n",
    "\n",
    "Cluster View\n",
    "------------\n",
    "\n",
    "<img src=\"images/streaming-daemons.png\">\n",
    "\n",
    "Checkpointing\n",
    "-------------\n",
    "\n",
    "Q: How can I protect my streaming app against failure?\n",
    "\n",
    "- Streaming apps run for much longer than batch apps.\n",
    "\n",
    "- They can run for days and weeks.\n",
    "\n",
    "- So fault-tolerance is important for them.\n",
    "\n",
    "- To enable recovery from failure you must enable checkpointing.\n",
    "\n",
    "- If a checkpointed application crashes, you restart it and it\n",
    "  recovers the state of the RDDs when it crashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%file test_checkpointing.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from text_file_util import xrange_write\n",
    "    \n",
    "from pprint import pprint\n",
    "    \n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "    \n",
    "checkpointDir = 'ckpt'\n",
    "    \n",
    "def functionToCreateContext():\n",
    "    ssc = StreamingContext(SparkContext(), batchDuration=2)\n",
    "    \n",
    "    # Add new values with previous running count to get new count\n",
    "    ds = ssc.textFileStream('input') \\\n",
    "        .map(lambda x: int(x) % 10) \\\n",
    "        .map(lambda x: (x,1)) \\\n",
    "        .updateStateByKey(updateFunction)\n",
    "    ds.pprint()\n",
    "    ds.count().pprint()\n",
    "    \n",
    "    # Set up checkpoint\n",
    "    ssc.checkpoint(checkpointDir)\n",
    "    return ssc\n",
    "    \n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext.getOrCreate(\n",
    "    checkpointDir, functionToCreateContext)\n",
    "    \n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "    \n",
    "# Write data to textFileStream\n",
    "xrange_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_checkpointing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The program will run forever. To terminate hit `Ctrl-C`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
