{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"tocheading\">Serving Layer</h1>\n",
    "\n",
    "> The serving layer consists of databases that index and serve the results of the batch layer.\n",
    "\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives\n",
    "---------------------------------------------------------------------\n",
    "By the end of today's lesson, you will be able to:  \n",
    "* Tailor batch views to the queries they serve\n",
    "* Provide a new answer to the data-normalization versus\n",
    "denormalization debate\n",
    "* Discuss the advantages of batch-writable, random-read, and no random-write databases\n",
    "* Contrast a Lambda Architecture solution with a fully incremental solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing on the batch layer\n",
    "---------------------------------------------------------------------\n",
    "**The serving layer provides low-latency access to the results of calculations performed on the master dataset.**  \n",
    "The serving layer views are slightly out of date due to the time required for batch computation.\n",
    "![Figure 10.1](images/10fig01_alt.jpg)\n",
    "\n",
    "While investigating the serving layer, you’ll learn the following:\n",
    "\n",
    "* Indexing strategies to minimize latency, resource usage, and variance\n",
    "* The requirements for the serving layer in the Lambda Architecture\n",
    "* How the serving layer solves the long-debated normalization versus denormal-\n",
    "ization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics for the serving layer\n",
    "\n",
    "* _latency_ - the time required to answer a single query\n",
    "* _throughput_ - the number of queries that can be served within a given period of time\n",
    "\n",
    "### Scatter/Gather\n",
    "\n",
    "Suppose a query requires fetching data from three servers...\n",
    "![Figure 10.3](images/10fig03_alt.jpg)\n",
    "**When distributing a task over multiple servers, the overall latency is determined by the slowest server response time.**\n",
    "\n",
    "In general, the more servers a query touches, the higher the overall latency of the query.  \n",
    "![Figure 10.4](images/10fig04_alt.jpg)\n",
    "**If you increase the number of servers involved in a distributed task, you also increase the likelihood that at least one will respond slowly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A different indexing strategy\n",
    "Collocate the pageview information for a sin- gle URL on the same partition and store it sequentially. Fetching the pageviews will then only require a single seek and scan rather than numerous seeks.  \n",
    "![Figure 10.5](images/10fig05_alt.jpg)\n",
    "**A sorted index promotes scans and limits disk seeks to improve both latency and throughput.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The serving layer solution to the normalization/ denormalization problem\n",
    "### A normalized schema uses multiple independent datasets with little or no redundant data.\n",
    "\n",
    "| User ID | Name | Location ID |\n",
    "|:-:| ------ |:-:|\n",
    "| 1 | Sally  | 3 |\n",
    "| 2 | George | 1 |\n",
    "| 3 | Bob    | 3 |\n",
    "\n",
    "| Location ID | City | State | Population |\n",
    "|:------:| ---- | -- | ----:|\n",
    "| 1 | New York  | NY | 8.2M |\n",
    "| 2 | San Diego | CA | 1.3M |\n",
    "| 3 | Chicago   | IL | 2.7M |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denormalized tables store data redundantly to improve query performance.\n",
    "\n",
    "| User ID | Name | Location ID | City | State |\n",
    "|:-----:| ------ |:--:| ---- | -- |\n",
    "| 1 | Sally  | 3 | Chicago   | IL | \n",
    "| 2 | George | 1 | New York  | NY |\n",
    "| 3 | Bob    | 3 | Chicago   | IL | \n",
    "\n",
    "| Location ID | City | State | Population |\n",
    "|:------:| ---- | -- | ----:|\n",
    "| 1 | New York  | NY | 8.2M |\n",
    "| 2 | San Diego | CA | 1.3M |\n",
    "| 3 | Chicago   | IL | 2.7M |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The master dataset should be normalized to maintain consistency but the batch views may be denormalized for query efficiency. Since the batch views are _views_, this does not violate the principles of normaliziation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for a serving layer database\n",
    "* *Batch writable*—The batch views for a serving layer are produced from scratch. When a new version of a view becomes available, it must be possible to completely swap out the older version with the updated view.\n",
    "* *Scalable*—A serving layer database must be capable of handling views of arbitrary size. As with the distributed filesystems and batch computation framework previously discussed, this requires it to be distributed across multiple machines.\n",
    "* *Random reads*—A serving layer database must support random reads, with indexes providing direct access to small portions of the view. This requirement is necessary to have low latency on queries.\n",
    "* *Fault-tolerant*—Because a serving layer database is distributed, it must be tolerant of machine failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a serving layer for SuperWebAnalytics.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import simplejson as json\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import approxCountDistinct\n",
    "\n",
    "try:\n",
    "    if not sc:\n",
    "        sc = pyspark.SparkContext()\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "\n",
    "sqlContext = pyspark.HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_normalized_page_views = sc.pickleFile(\"../SuperWebAnalytics/batch/distinct_normalized_page_views/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pageviews over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d': 86400, 'h': 3600, 'm': 2419200, 'w': 604800}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granularity = {'h': 60 * 60}\n",
    "granularity['d'] = granularity['h'] * 24 # days in UTC (GMT) timezone\n",
    "granularity['w'] = granularity['d'] * 7\n",
    "granularity['m'] = granularity['w'] * 4  # 1 month = 28 days, not 1 calendar month\n",
    "granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_granular(granual):\n",
    "    def func(datum): \n",
    "        url = datum['dataunit']['page_view']['page']['url']\n",
    "        true_as_of_secs = datum['pedigree']['true_as_of_secs']\n",
    "        return (url, granual, true_as_of_secs - true_as_of_secs % granularity[granual])\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_view_schema = StructType([\n",
    "    StructField('url', StringType() ,True),\n",
    "    StructField('granularity', StringType(), True),\n",
    "    StructField('ts', IntegerType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_view_by_hour = sqlContext.createDataFrame(distinct_normalized_page_views.map(gen_granular('h')), \n",
    "                                               page_view_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_page_views = page_view_by_hour.groupBy(['url', 'granularity', 'ts']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_page_views.registerTempTable('hourly_page_views')\n",
    "sqlContext.cacheTable('hourly_page_views')\n",
    "\n",
    "# N.B. This won't work if you want your days to be in any timezone other than UTC\n",
    "daily_page_views = sqlContext.sql(\"\"\"SELECT url, 'd' AS granularity, ts - ts % {} AS ts, sum(count) AS count \n",
    "                                FROM hourly_page_views \n",
    "                                GROUP BY url, ts - ts % {}\"\"\".format(granularity['d'], granularity['d']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+------+\n",
      "|            url|granularity|        ts| count|\n",
      "+---------------+-----------+----------+------+\n",
      "|    mysite.com/|          h|1438912800|300054|\n",
      "|    mysite.com/|          h|1438624800|101212|\n",
      "|    mysite.com/|          d|1438905600|300054|\n",
      "|    mysite.com/|          d|1438560000|101212|\n",
      "|mysite.com/blog|          h|1438912800|899778|\n",
      "|mysite.com/blog|          h|1438624800|101732|\n",
      "|mysite.com/blog|          d|1438560000|101732|\n",
      "|mysite.com/blog|          d|1438905600|899778|\n",
      "+---------------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_view_union = hourly_page_views.unionAll(daily_page_views).orderBy('url')\n",
    "page_view_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_view_union.write.parquet(\"../SuperWebAnalytics/batch_views/page_views\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniques over time\n",
    "\n",
    "![Figure 10.8](images/10fig08_alt.jpg)\n",
    "\n",
    "**Index design for uniques over time. Although the index keys are a compound of URL and granularity, indexes are partitioned between servers solely by the URL.**\n",
    "\n",
    "* key is a compound key of URL and granularity\n",
    "* indices are partitioned solely by  URL  \n",
    "\n",
    "To retrieve a range of values for a URL and granularity, you’d use the URL to find the server containing the information you need, and then use both the URL and granularity to look up the values you’re interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def user_row(granual):\n",
    "    import simplejson as json\n",
    "    granularity = {'h': 60 * 60}\n",
    "    granularity['d'] = granularity['h'] * 24 # days in UTC (GMT) timezone\n",
    "    granularity['w'] = granularity['d'] * 7\n",
    "    granularity['m'] = granularity['w'] * 4  # 1 month = 28 days, not 1 calendar month\n",
    "    def func(datum):\n",
    "        url = datum['dataunit']['page_view']['page']['url']\n",
    "        true_as_of_secs = datum['pedigree']['true_as_of_secs']\n",
    "        person = json.dumps(datum['dataunit']['page_view']['person'])\n",
    "        return (url, granual, true_as_of_secs - true_as_of_secs % granularity[granual], person)\n",
    "    return func\n",
    "\n",
    "visit_schema = StructType([\n",
    "    StructField('url', StringType() ,True),\n",
    "    StructField('granularity', StringType(), True),\n",
    "    StructField('ts', IntegerType(), True),\n",
    "    StructField('person', StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(url=u'mysite.com/blog', granularity=u'h', ts=1438912800, person=u'{\"cookie\": \"PQRST\"}'),\n",
       " Row(url=u'mysite.com/blog', granularity=u'h', ts=1438912800, person=u'{\"user_id\": 5693}'),\n",
       " Row(url=u'mysite.com/blog', granularity=u'h', ts=1438912800, person=u'{\"user_id\": 6257}'),\n",
       " Row(url=u'mysite.com/', granularity=u'h', ts=1438912800, person=u'{\"user_id\": 765}'),\n",
       " Row(url=u'mysite.com/blog', granularity=u'h', ts=1438912800, person=u'{\"user_id\": 7132}')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_visits = sqlContext.createDataFrame(distinct_normalized_page_views.map(user_row('h')),\n",
    "                                           visit_schema)\n",
    "hourly_visits.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_visits.write.parquet('../SuperWebAnalytics/batch/hourly_visits/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visits_by_hour = hourly_visits.groupBy(['url', 'granularity', 'ts'])\\\n",
    "                              .agg(approxCountDistinct('person').alias('approx_uniques'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily_visits = sqlContext.createDataFrame(distinct_normalized_page_views.map(user_row('d')),\n",
    "                                           visit_schema)\n",
    "daily_visits.write.parquet('../SuperWebAnalytics/batch/daily_visits/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits_by_day = daily_visits.groupBy(['url', 'granularity', 'ts'])\\\n",
    "                              .agg(approxCountDistinct('person').alias('approx_uniques'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+--------------+\n",
      "|            url|granularity|        ts|approx_uniques|\n",
      "+---------------+-----------+----------+--------------+\n",
      "|    mysite.com/|          d|1438560000|         10874|\n",
      "|    mysite.com/|          d|1438905600|         10893|\n",
      "|    mysite.com/|          h|1438624800|         10874|\n",
      "|    mysite.com/|          h|1438912800|         10893|\n",
      "|mysite.com/blog|          h|1438912800|         10893|\n",
      "|mysite.com/blog|          h|1438624800|         10776|\n",
      "|mysite.com/blog|          d|1438560000|         10776|\n",
      "|mysite.com/blog|          d|1438905600|         10893|\n",
      "+---------------+-----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits_union = visits_by_hour.unionAll(visits_by_day).orderBy('url')\n",
    "visits_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits_union.write.parquet(\"../SuperWebAnalytics/batch_views/visits\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-identifier normalization\n",
    "![Figure 9.3](images/09fig03_alt.jpg)\n",
    "\n",
    "<!--\n",
    "**Equivs could affect any bucket in the database.**\n",
    "![Figure 10.10](images/10fig10.jpg)\n",
    "-->\n",
    "\n",
    "#### Handling equivs on the read-side workflow\n",
    "![Figure 10.11](images/10fig11_alt.jpg)\n",
    "\n",
    "1. First, retrieve every UserID set for every hour in the range, and merge them.\n",
    "2. Convert the set of UserIDs to a set of PersonIDs by using the UserID-to-PersonID index.\n",
    "3. Return the count of the PersonID set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<!--\n",
    "### Bounce-rate analysis\n",
    "\n",
    "![Figure 10.9](images/10fig09_alt.jpg)\n",
    "\n",
    "**Implementing a bounce-rates view using a key/value index**\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|                 dt|approx_uniques|\n",
      "+-------------------+--------------+\n",
      "|2015-08-02 17:00:00|         10874|\n",
      "|2015-08-06 17:00:00|         10893|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits = sqlContext.read.parquet(\"../SuperWebAnalytics/batch_views/visits\")\n",
    "visits.registerTempTable(\"visits\");\n",
    "sqlContext.sql(\"\"\"SELECT from_unixtime(ts) AS dt, approx_uniques FROM visits \n",
    "                   WHERE url = 'mysite.com/' AND granularity = 'd'\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "By this coming Monday:  \n",
    "\n",
    "1. Create batch views in tabular (not key-value) format.\n",
    "2. Serialize them using parquet.\n",
    "3. Load them and query them using SQL (_i.e._ HiveQL or SparkSQL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
