{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "This file was auto-generated from markdown using notedown.\n",
    "Instead of modifying the ipynb modify the markdown source. \n",
    "-->\n",
    "\n",
    "<h1 class=\"tocheading\">HDFS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Why Hadoop\n",
    "==========\n",
    "\n",
    "Big Data Problem\n",
    "----------------\n",
    "\n",
    "We have a 100 TB of sales data that looks like this:\n",
    "\n",
    "ID    |Date          |Store  |State |Product   |Amount\n",
    "--    |----          |-----  |----- |-------   |------\n",
    "101   |11/13/2014    |100    |WA    |331       |300.00\n",
    "104   |11/18/2014    |700    |OR    |329       |450.00\n",
    "\n",
    "What If\n",
    "-------\n",
    "\n",
    "What are some of the questions we could answer if we could process this huge data set?\n",
    "\n",
    "- How much revenue did we make by store, state?\n",
    "\n",
    "- How much revenue did we make by product?\n",
    "\n",
    "- How much revenue did we make by week, month, year?\n",
    "\n",
    "Statistical Uses\n",
    "----------------\n",
    "\n",
    "Why are these interesting?\n",
    "\n",
    "- These questions can help us figure out which products are selling\n",
    "  in which markets, at what time of the year.\n",
    "\n",
    "Engineering Problem\n",
    "-------------------\n",
    "\n",
    "To answer these questions we have to solve two problems:\n",
    "\n",
    "- Store 100 TB of data\n",
    "\n",
    "- Process 100 TB of data\n",
    "\n",
    "Here is our starting point:\n",
    "\n",
    "- To solve this problem we have been provided with 1000 commodity Linux servers.\n",
    "\n",
    "- How can we organize these machines to store and process this data.\n",
    "\n",
    "Objectives\n",
    "----------\n",
    "\n",
    "By the end of this class, we will be able to:\n",
    "\n",
    "- Explain how HDFS splits up large files into blocks and stores them\n",
    "  on a cluster.\n",
    "  \n",
    "- Explain how HDFS uses replication to ensure fault tolerance of the\n",
    "  data.\n",
    "\n",
    "- Explain how HDFS uses `fsimage` and `edits` files to ensure fault\n",
    "  tolerance of the metadata.\n",
    "\n",
    "<!--TODO: Sync up objectives with actual content-->\n",
    "\n",
    "Hadoop Intro\n",
    "============\n",
    "\n",
    "Hadoop\n",
    "------\n",
    "\n",
    "Hadoop is a cluster operating system. It is made up of:\n",
    "\n",
    "- HDFS, which coordinates storing large amounts of data on a\n",
    "  cluster.\n",
    "\n",
    "- MapReduce which coordinates processing data across a cluster of\n",
    "  machines.\n",
    "\n",
    "Google Papers\n",
    "-------------\n",
    "\n",
    "Hadoop, HDFS, and MapReduce are open source implementations of the\n",
    "ideas in these papers from Google and Stanford.\n",
    "\n",
    "- Paper #1: [2003] The Google File System     \n",
    "    <http://research.google.com/archive/gfs-sosp2003.pdf>\n",
    "\n",
    "- Paper #2: [2004] MapReduce: Simplified Data Processing on Large Clusters    \n",
    "    <http://research.google.com/archive/mapreduce-osdi04.pdf>\n",
    "\n",
    "- Paper #3: [2006] Bigtable: A Distributed Storage System for Structured Data\n",
    "    <http://static.googleusercontent.com/media/research.google.com/en/us/archive/bigtable-osdi06.pdf>\n",
    "\n",
    "\n",
    "Doug Cutting\n",
    "------------\n",
    "\n",
    "<img src=\"images/doug-cutting.png\">\n",
    "\n",
    "Hadoop\n",
    "------\n",
    "\n",
    "<img style=\"width:50%\" src=\"images/yellow-elephant-hadoop.jpg\">\n",
    "\n",
    "Hadoop Analogy\n",
    "--------------\n",
    "\n",
    "<img src=\"images/devastator-transformer.jpg\">\n",
    "\n",
    "Hadoop Analogy\n",
    "--------------\n",
    "\n",
    "System     |Analogy\n",
    "------     |-------\n",
    "Hadoop     |Cluster Operating System\n",
    "HDFS       |Cluster Disk Drive\n",
    "MapReduce  |Cluster CPU\n",
    "\n",
    "- Hadoop clusters are made up of commodity Linux machines.\n",
    "\n",
    "- Each machine is weak and limited.\n",
    "\n",
    "- Hadoop combines these machines.\n",
    "\n",
    "- The Hadoop cluster is bigger and more powerful than the individual\n",
    "  machines.\n",
    "\n",
    "HDFS Daemons\n",
    "------------\n",
    "\n",
    "Daemon Name          |Role                              |Number Deployed\n",
    "-----------          |-----------                       |---------------\n",
    "NameNode             |Manages DataNodes and metadata    |1 per cluster\n",
    "Secondary NameNode   |Compacts recovery metadata        |1 per cluster\n",
    "Standby NameNode     |NameNode backup                   |1 per cluster\n",
    "DataNode             |Stores/processes file parts       |1 per worker machine\n",
    "\n",
    "HDFS Cluster\n",
    "------------\n",
    "\n",
    "<img src=\"images/hdfs-cluster-arch.png\">\n",
    "\n",
    "HDFS Operation\n",
    "==============\n",
    "\n",
    "HDFS Command Line Access\n",
    "------------------------\n",
    "\n",
    "Command                                  |Meaning\n",
    "-------                                  |-------\n",
    "`hadoop fs -ls hdfs://nn:8020/user/jim`  |List home directory of user `jim`\n",
    "`hadoop fs -ls /user/jim`                |List home directory of user `jim`\n",
    "`hadoop fs -ls data`                     |List home directory of user `jim`\n",
    "`hadoop fs -ls `                         |List home directory of user `jim`\n",
    "`hadoop fs -mkdir dir`                   |Make new directory `/user/jim/dir`\n",
    "`hadoop fs -mkdir -p a/b/c/dir`          |Make new directory and all missing parents\n",
    "`hadoop fs -rm file`                     |Remove `/user/jim/file`\n",
    "`hadoop fs -rm -r dir`                   |Remove `/user/jim/dir` and all its contents\n",
    "`hadoop fs -rm dir`                      |Remove `/user/jim/dir` if it is empty\n",
    "`hadoop fs -put file1 file2`             |Copy local `file1` to `/user/jim/file2`\n",
    "`hadoop fs -put file1 dir/`              |Copy local `file1` to `/user/jim/dir/file2`\n",
    "`echo 'hi' `&#124;` hadoop fs -put - file.txt`|Put string `hi` into `/user/jim/file.txt`\n",
    "`hadoop fs -get /user/jim/file.txt`      |Copy `/user/jim/file.txt` to local `file.txt`\n",
    "`hadoop fs -get /user/jim/file1 file2`   |Copy `/user/jim/file1` to local `file2`\n",
    "`hadoop fs -cat /user/jim/file.txt`      |Cat `/user/jim/file.txt` to stdout\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of the streaming put?\n",
    "</summary>\n",
    "1. The data does not have to be staged anywhere.<br>\n",
    "2. You can put data into HDFS from a running program.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Can you access HDFS files on a remote cluster?\n",
    "</summary>\n",
    "1. Use `hadoop fs -ls hdfs://remote-nn:8020/user/jim/path`<br>\n",
    "2. Use `hadoop fs -Dfs.defaultFS=hdfs://remote-nn:8020 /user/jim/path`<br>\n",
    "3. Use `hadoop fs -fs hdfs://remote-nn:8020 /user/jim/path`<br>\n",
    "</details>\n",
    "\n",
    "Files Blocks Replicas\n",
    "---------------------\n",
    "\n",
    "<img src=\"images/hdfs-files-blocks-replicas.png\">\n",
    "\n",
    "- Each file is made up of blocks.\n",
    "\n",
    "- Each block has 3 copies or *replicas*.\n",
    "\n",
    "- The replicas are equivalent---there is no primary replica.\n",
    "\n",
    "- The block size for a file cannot be changed once a file is created.\n",
    "\n",
    "- The replication for a file *can* be changed dynamically.\n",
    "\n",
    "\n",
    "Custom Block Size and Replication\n",
    "---------------------------------\n",
    "\n",
    "Command                                          |Meaning\n",
    "-------                                          |-------\n",
    "`hadoop fs -D dfs.blocksize=67108864 -put file`  |Put `file` with block size 64MB \n",
    "`hadoop fs -D dfs.replication=1 -put file`       |Put `file` with replication 1\n",
    "`hadoop fs -setrep 1 file`                       |Change replication of `file` to 1\n",
    "`hadoop fs -setrep -R 1 dir`                     |Change replication of `dir` and contents to 1\n",
    "`hadoop fs -setrep -w 1 file`                    |Change replication and block till done\n",
    "\n",
    "File Security\n",
    "-------------\n",
    "\n",
    "Command                               |Meaning\n",
    "-------                               |-------\n",
    "`hadoop fs -chown jim file1`          |Change owner of `file1` to `jim`\n",
    "`hadoop fs -chgrp staff file1`        |Change group of `file1` to `staff`\n",
    "`hadoop fs -chown jim:staff file1`    |Change both owner and group\n",
    "`hadoop fs -chown -R jim:staff dir1`  |Change both for `dir1` and its contents\n",
    "`hadoop fs -chgrp -R staff dir1`      |Change group of `dir1` and its contents\n",
    "`hadoop fs -chmod 755 file1`          |Set `file1` permissions to `rwxr-xr-x`\n",
    "`hadoop fs -chmod 755 -R dir1`        |Set permissions for `dir1` and its contents\n",
    "\n",
    "Hadoop Security\n",
    "---------------\n",
    "\n",
    "Q: What is the primary HDFS security model?\n",
    "\n",
    "- HDFS uses the Unix file system security model.\n",
    "\n",
    "- Unix secures access through authentication and authorization.\n",
    "\n",
    "- Authentication: Who are you.\n",
    "\n",
    "- Authorization: Who has access to a file.\n",
    "\n",
    "- By default HDFS enforces authorization but not authentication.\n",
    "\n",
    "Q: How can I impersonate someone to hack their files in HDFS? \n",
    "\n",
    "- `sudo -u jim hadoop fs -cat /user/jim/deep-dark-secrets.txt`\n",
    "\n",
    "Q: How can I secure the system against impersonation?\n",
    "\n",
    "- To fix this you have to enable Kerberos.\n",
    "\n",
    "MapReduce\n",
    "=========\n",
    "\n",
    "MapReduce Intro\n",
    "---------------\n",
    "\n",
    "Q: What is MapReduce?\n",
    "\n",
    "- MapReduce is a system for processing data on HDFS.\n",
    "\n",
    "- In the *map* phase data is processed locally, with one *mapper* per\n",
    "  block.\n",
    "\n",
    "- In the *reduce* phase the results of the map phase are consolidated.\n",
    "\n",
    "Data Locality\n",
    "-------------\n",
    "\n",
    "Q: What is *data locality*?\n",
    "\n",
    "- Data locality is the secret sauce in HDFS and MapReduce. \n",
    "\n",
    "- Data locality means MapReduce runs mappers on locally machines with HDFS blocks.\n",
    "\n",
    "- When these machine are busy mappers may come up on other machines.\n",
    "\n",
    "\n",
    "Data Locality\n",
    "-------------\n",
    "\n",
    "<img src=\"images/hdfs-map-reduce-data-locality.png\">\n",
    "\n",
    "\n",
    "MapReduce Examples\n",
    "------------------\n",
    "\n",
    "Q: How can I run MapReduce programs?\n",
    "\n",
    "- Hadoop ships with some MapReduce example programs.\n",
    "\n",
    "- The jar files are located at `/usr/lib/hadoop-mapreduce`.\n",
    "\n",
    "- Here is how you can get more information about them.\n",
    "\n",
    "Command                                                    |Result\n",
    "-------                                                    |------\n",
    "`hadoop jar /path/hadoop-mapreduce-examples-VER.jar`       |Lists all programs\n",
    "`hadoop jar /path/hadoop-mapreduce-examples-VER.jar sleep` |List usage of `sleep`\n",
    "\n",
    "Q: What are some of the interesting example programs?\n",
    "\n",
    "Program                |Description\n",
    "-------                |-----------\n",
    "`aggregatewordcount`   |Counts words in input\n",
    "`grep`                 |Counts matches of regex in input\n",
    "`pi`                   |Estimates Pi using monte-carlo method\n",
    "`sleep`                |Sleeps \n",
    "`sudoku`               |Sudoku solver\n",
    "`teragen`              |Generate data for terasort\n",
    "`terasort`             |Run terasort\n",
    "`teravalidate`         |Checking results of terasort\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of running `grep` as a MapReduce program?\n",
    "</summary>\n",
    "`grep` on MapReduce will scan blocks in parallel and complete\n",
    "faster.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
