{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "This file was auto-generated from markdown using notedown.\n",
    "Instead of modifying the ipynb modify the markdown source. \n",
    "-->\n",
    "\n",
    "<h1 class=\"tocheading\">HDFS</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Why Hadoop\n",
    "==========\n",
    "\n",
    "Big Data Problem\n",
    "----------------\n",
    "\n",
    "We have a 100 TB of sales data that looks like this:\n",
    "\n",
    "ID    |Date          |Store  |State |Product   |Amount\n",
    "--    |----          |-----  |----- |-------   |------\n",
    "101   |11/13/2014    |100    |WA    |331       |300.00\n",
    "104   |11/18/2014    |700    |OR    |329       |450.00\n",
    "\n",
    "What If\n",
    "-------\n",
    "\n",
    "What are some of the questions we could answer if we could process this huge data set?\n",
    "\n",
    "- How much revenue did we make by store, state?\n",
    "\n",
    "- How much revenue did we make by product?\n",
    "\n",
    "- How much revenue did we make by week, month, year?\n",
    "\n",
    "Statistical Uses\n",
    "----------------\n",
    "\n",
    "Why are these interesting?\n",
    "\n",
    "- These questions can help us figure out which products are selling\n",
    "  in which markets, at what time of the year.\n",
    "\n",
    "Engineering Problem\n",
    "-------------------\n",
    "\n",
    "To answer these questions we have to solve two problems:\n",
    "\n",
    "- Store 100 TB of data\n",
    "\n",
    "- Process 100 TB of data\n",
    "\n",
    "Here is our starting point:\n",
    "\n",
    "- To solve this problem we have been provided with 1000 commodity Linux servers.\n",
    "\n",
    "- How can we organize these machines to store and process this data.\n",
    "\n",
    "Objectives\n",
    "----------\n",
    "\n",
    "By the end of this class, we will be able to:\n",
    "\n",
    "- Explain how HDFS splits up large files into blocks and stores them\n",
    "  on a cluster.\n",
    "  \n",
    "- Explain how HDFS uses replication to ensure fault tolerance of the\n",
    "  data.\n",
    "\n",
    "- Explain how HDFS uses `fsimage` and `edits` files to ensure fault\n",
    "  tolerance of the metadata.\n",
    "\n",
    "<!--TODO: Sync up objectives with actual content-->\n",
    "\n",
    "Hadoop Intro\n",
    "============\n",
    "\n",
    "Hadoop\n",
    "------\n",
    "\n",
    "Hadoop is a cluster operating system. It is made up of:\n",
    "\n",
    "- HDFS, which coordinates storing large amounts of data on a\n",
    "  cluster.\n",
    "\n",
    "- MapReduce which coordinates processing data across a cluster of\n",
    "  machines.\n",
    "\n",
    "Google Papers\n",
    "-------------\n",
    "\n",
    "Hadoop, HDFS, and MapReduce are open source implementations of the\n",
    "ideas in these papers from Google and Stanford.\n",
    "\n",
    "- Paper #1: [2003] The Google File System     \n",
    "    <http://research.google.com/archive/gfs-sosp2003.pdf>\n",
    "\n",
    "- Paper #2: [2004] MapReduce: Simplified Data Processing on Large Clusters    \n",
    "    <http://research.google.com/archive/mapreduce-osdi04.pdf>\n",
    "\n",
    "- Paper #3: [2006] Bigtable: A Distributed Storage System for Structured Data\n",
    "    <http://static.googleusercontent.com/media/research.google.com/en/us/archive/bigtable-osdi06.pdf>\n",
    "\n",
    "\n",
    "Doug Cutting\n",
    "------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/doug-cutting.png\">\n",
    "\n",
    "Hadoop\n",
    "------\n",
    "\n",
    "<img style=\"width:50%\" src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/yellow-elephant-hadoop.jpg\">\n",
    "\n",
    "Hadoop Analogy\n",
    "--------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/devastator-transformer.jpg\">\n",
    "\n",
    "Hadoop Analogy\n",
    "--------------\n",
    "\n",
    "System     |Analogy\n",
    "------     |-------\n",
    "Hadoop     |Cluster Operating System\n",
    "HDFS       |Cluster Disk Drive\n",
    "MapReduce  |Cluster CPU\n",
    "\n",
    "- Hadoop clusters are made up of commodity Linux machines.\n",
    "\n",
    "- Each machine is weak and limited.\n",
    "\n",
    "- Hadoop combines these machines.\n",
    "\n",
    "- The Hadoop cluster is bigger and more powerful than the individual\n",
    "  machines.\n",
    "\n",
    "HDFS Daemons\n",
    "------------\n",
    "\n",
    "Daemon Name          |Role                              |Number Deployed\n",
    "-----------          |-----------                       |---------------\n",
    "NameNode             |Manages DataNodes and metadata    |1 per cluster\n",
    "Secondary NameNode   |Compacts recovery metadata        |1 per cluster\n",
    "Standby NameNode     |NameNode backup                   |1 per cluster\n",
    "DataNode             |Stores/processes file parts       |1 per worker machine\n",
    "\n",
    "HDFS Cluster\n",
    "------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-cluster-arch.png\">\n",
    "\n",
    "HDFS Operation\n",
    "==============\n",
    "\n",
    "HDFS Command Line Access\n",
    "------------------------\n",
    "\n",
    "Command                                  |Meaning\n",
    "-------                                  |-------\n",
    "`hadoop fs -ls hdfs://nn:8020/user/jim`  |List home directory of user `jim`\n",
    "`hadoop fs -ls /user/jim`                |List home directory of user `jim`\n",
    "`hadoop fs -ls data`                     |List home directory of user `jim`\n",
    "`hadoop fs -ls `                         |List home directory of user `jim`\n",
    "`hadoop fs -mkdir dir`                   |Make new directory `/user/jim/dir`\n",
    "`hadoop fs -mkdir -p a/b/c/dir`          |Make new directory and all missing parents\n",
    "`hadoop fs -rm file`                     |Remove `/user/jim/file`\n",
    "`hadoop fs -rm -r dir`                   |Remove `/user/jim/dir` and all its contents\n",
    "`hadoop fs -rm dir`                      |Remove `/user/jim/dir` if it is empty\n",
    "`hadoop fs -put file1 file2`             |Copy local `file1` to `/user/jim/file2`\n",
    "`hadoop fs -put file1 dir/`              |Copy local `file1` to `/user/jim/dir/file2`\n",
    "`echo 'hi' `&#124;` hadoop fs -put - file.txt`|Put string `hi` into `/user/jim/file.txt`\n",
    "`hadoop fs -get /user/jim/file.txt`      |Copy `/user/jim/file.txt` to local `file.txt`\n",
    "`hadoop fs -get /user/jim/file1 file2`   |Copy `/user/jim/file1` to local `file2`\n",
    "`hadoop fs -cat /user/jim/file.txt`      |Cat `/user/jim/file.txt` to stdout\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of the streaming put?\n",
    "</summary>\n",
    "1. The data does not have to be staged anywhere.<br>\n",
    "2. You can put data into HDFS from a running program.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Can you access HDFS files on a remote cluster?\n",
    "</summary>\n",
    "1. Use `hadoop fs -ls hdfs://remote-nn:8020/user/jim/path`<br>\n",
    "2. Use `hadoop fs -Dfs.defaultFS=hdfs://remote-nn:8020 /user/jim/path`<br>\n",
    "3. Use `hadoop fs -fs hdfs://remote-nn:8020 /user/jim/path`<br>\n",
    "</details>\n",
    "\n",
    "Files Blocks Replicas\n",
    "---------------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-files-blocks-replicas.png\">\n",
    "\n",
    "- Each file is made up of blocks.\n",
    "\n",
    "- Each block has 3 copies or *replicas*.\n",
    "\n",
    "- The replicas are equivalent---there is no primary replica.\n",
    "\n",
    "- The block size for a file cannot be changed once a file is created.\n",
    "\n",
    "- The replication for a file *can* be changed dynamically.\n",
    "\n",
    "\n",
    "Custom Block Size and Replication\n",
    "---------------------------------\n",
    "\n",
    "Command                                          |Meaning\n",
    "-------                                          |-------\n",
    "`hadoop fs -D dfs.blocksize=67108864 -put file`  |Put `file` with block size 64MB \n",
    "`hadoop fs -D dfs.replication=1 -put file`       |Put `file` with replication 1\n",
    "`hadoop fs -setrep 1 file`                       |Change replication of `file` to 1\n",
    "`hadoop fs -setrep -R 1 dir`                     |Change replication of `dir` and contents to 1\n",
    "`hadoop fs -setrep -w 1 file`                    |Change replication and block till done\n",
    "\n",
    "File Security\n",
    "-------------\n",
    "\n",
    "Command                               |Meaning\n",
    "-------                               |-------\n",
    "`hadoop fs -chown jim file1`          |Change owner of `file1` to `jim`\n",
    "`hadoop fs -chgrp staff file1`        |Change group of `file1` to `staff`\n",
    "`hadoop fs -chown jim:staff file1`    |Change both owner and group\n",
    "`hadoop fs -chown -R jim:staff dir1`  |Change both for `dir1` and its contents\n",
    "`hadoop fs -chgrp -R staff dir1`      |Change group of `dir1` and its contents\n",
    "`hadoop fs -chmod 755 file1`          |Set `file1` permissions to `rwxr-xr-x`\n",
    "`hadoop fs -chmod 755 -R dir1`        |Set permissions for `dir1` and its contents\n",
    "\n",
    "Hadoop Security\n",
    "---------------\n",
    "\n",
    "Q: What is the primary HDFS security model?\n",
    "\n",
    "- HDFS uses the Unix file system security model.\n",
    "\n",
    "- Unix secures access through authentication and authorization.\n",
    "\n",
    "- Authentication: Who are you.\n",
    "\n",
    "- Authorization: Who has access to a file.\n",
    "\n",
    "- By default HDFS enforces authorization but not authentication.\n",
    "\n",
    "Q: How can I impersonate someone to hack their files in HDFS? \n",
    "\n",
    "- `sudo -u jim hadoop fs -cat /user/jim/deep-dark-secrets.txt`\n",
    "\n",
    "Q: How can I secure the system against impersonation?\n",
    "\n",
    "- To fix this you have to enable Kerberos.\n",
    "\n",
    "Hadoop Configuration\n",
    "====================\n",
    "\n",
    "Configuration Parameters\n",
    "------------------------\n",
    "\n",
    "Q: Why is configuration important?\n",
    "\n",
    "- Hadoop uses its configuration system for customizing defaults.\n",
    "\n",
    "- Almost everything in configurable in Hadoop.\n",
    "\n",
    "- The configuration system is like Hadoop's spinal cord.\n",
    "\n",
    "Q: How many configuration parameters are there?\n",
    "\n",
    "- Over 900.\n",
    "\n",
    "Q: Where can I get a list of all the configuration parameters?\n",
    "\n",
    "- Go to <https://hadoop.apache.org/docs/stable/>\n",
    "\n",
    "- Look at lower left corner for links to default configuration settings\n",
    "\n",
    "Changing Defaults\n",
    "-----------------\n",
    "\n",
    "Q: How can I change the default behavior of HDFS and MapReduce?\n",
    "\n",
    "- Hadoop uses *configuration* files for all changes to the default\n",
    "  behavior.\n",
    "\n",
    "- Hadoop configuration files are located at `/etc/hadoop/conf`.\n",
    "\n",
    "- This can be changed by adding a different location to the\n",
    "  `CLASSPATH` environment variable.\n",
    "\n",
    "Hadoop Configuration Files\n",
    "--------------------------\n",
    "\n",
    "Q: What are the different configuration files in `/etc/hadoop/conf`?\n",
    "\n",
    "Configuration File   |Purpose                    |Which daemons read\n",
    "------------------   |-------                    |------------------\n",
    "`hadoop-env.sh`      |Environment variables      |All daemons\n",
    "`core-site.xml`      |Core                       |All daemons\n",
    "`hdfs-site.xml`      |HDFS                       |HDFS daemons\n",
    "`yarn-site.xml`      |YARN                       |YARN daemons\n",
    "`mapred-site.xml`    |MapReduce                  |MapReduce daemons and processes\n",
    "`log4j.properties`   |Logging                    |All daemons\n",
    "`include`            |Whitelist of worker nodes  |Master daemons\n",
    "`exclude`            |Blacklist of worker nodes  |Master daemons\n",
    "`allocations.xml`    |Scheduling                 |Master daemons\n",
    "\n",
    "- `include`, `exclude`, and `allocations.xml` file names can be\n",
    "  changed to something else. \n",
    "\n",
    "Configuration Settings Per Job\n",
    "------------------------------\n",
    "\n",
    "Q: Can I change the configuration values when I run a command or\n",
    "submit a MapReduce job?\n",
    "\n",
    "- Users can override some configuration values.\n",
    "\n",
    "Q: What is the order of precedence in decreasing precedence?\n",
    "\n",
    "- Configuration defined in `Job` object in Hadoop app\n",
    "- Command-line `-D PARAM=VALUE`\n",
    "- Client `/etc/hadoop/conf/*`\n",
    "- Master or worker node `/etc/hadoop/conf/*`\n",
    "\n",
    "Q: How can I override configuration properties through the command\n",
    "line?\n",
    "\n",
    "Configuration Command Line Syntax\n",
    "---------------------------------\n",
    "\n",
    "Syntax                       |Meaning\n",
    "------                       |-------\n",
    "`-D PARAM=VALUE`             |Set value of parameter `PARAM` to `VALUE`\n",
    "`-fs hdfs://1.2.3.4:8020`    |Set NameNode to `hdfs://1.2.3.4:8020`\n",
    "`-jt 1.2.3.4:8088`           |Set JobTracker to `1.2.3.4:8088`\n",
    "\n",
    "Locking Configuration\n",
    "---------------------\n",
    "\n",
    "Q: How can an admin lock down configuration?\n",
    "\n",
    "```\n",
    "<property>\n",
    "<name>some.property.name</name>\n",
    "<value>some.value</value>\n",
    "<final>true</final>\n",
    "</property>\n",
    "```\n",
    "\n",
    "Deprecated Properties\n",
    "---------------------\n",
    "\n",
    "Property           |Example Value          |Meaning\n",
    "--------           |-------------          |-------\n",
    "`fs.defaultFS`     |`hdfs://1.2.3.4:8020`  |NameNode address and port\n",
    "`fs.default.name`  |`hdfs://1.2.3.4:8020`  |NameNode address and port (deprecated)\n",
    "\n",
    "- Deprecated properties are still supported.\n",
    "\n",
    "- Watch out for multiple properties affecting the same configuration\n",
    "  behavior.\n",
    "\n",
    "- After changing configuration you have to bounce the affected\n",
    "  daemons.\n",
    "\n",
    "Hadoop Architecture\n",
    "===================\n",
    "\n",
    "Problem: Large Files\n",
    "--------------------\n",
    "\n",
    "Q: How would you design a system that had to store files larger than\n",
    "what you can fit on a single machine?\n",
    "\n",
    "- Split the file into 128 MB chunks. We will call them *blocks*.\n",
    "\n",
    "- Spread them across the cluster on worker nodes called *DataNodes*.\n",
    "\n",
    "- Keep track of where each block is located on a *master node* called\n",
    "  the *NameNode*.\n",
    "\n",
    "- To read a file we ask the NameNode where all its blocks are located.\n",
    "\n",
    "Problem: DataNode Failure\n",
    "-------------------------\n",
    "\n",
    "Q: What if a machine holding a block fails?\n",
    "\n",
    "- Lets keep 3 copies or *replicas* of each block on different\n",
    "  DataNodes.\n",
    "\n",
    "- The NameNode has the metadata.\n",
    "\n",
    "- The NameNode knows what blocks make up a file, and where all the\n",
    "  replicas of the blocks are located.\n",
    "\n",
    "NameNode DataNode Communication\n",
    "-------------------------------\n",
    "\n",
    "- All DataNodes heartbeat into the NameNode every 3 seconds.\n",
    "\n",
    "- The heartbeat says that the DataNode is alive and how busy it is.\n",
    "\n",
    "- Every 6 hours each DataNode sends a block report to the NameNode.\n",
    "\n",
    "- The block report lists all the blocks on the DataNode.\n",
    "\n",
    "- The NameNode sends back any commands for the DataNode as a response.\n",
    "\n",
    "NameNode DataNode Communication\n",
    "-------------------------------\n",
    "\n",
    "- DataNodes initiate conversation.\n",
    "\n",
    "- NameNode responds with instructions.\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-nn-dns.png\">\n",
    "\n",
    "Heartbeats\n",
    "----------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-nn-dn-communication.png\">\n",
    "\n",
    "Block Reports\n",
    "-------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-block-report.png\">\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Does the NameNode ever initiate conversation with the DataNode? \n",
    "</summary>\n",
    "No. It only communicates in response to a heartbeat or a block report.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Why does the NameNode not initiate conversation with the DataNode? \n",
    "</summary>\n",
    "By waiting for heartbeats the NameNode ensures that it is only talking\n",
    "to DataNodes that are alive.\n",
    "</details>\n",
    "\n",
    "HDFS Read\n",
    "---------\n",
    "\n",
    "Q: How does a client read a file from HDFS?\n",
    "\n",
    "- The client requests the file from the NameNode.\n",
    "\n",
    "- The NameNode sends back the locations of the first 10 blocks.\n",
    "\n",
    "- The client reads the closest replica for each block.\n",
    "\n",
    "HDFS Closeness\n",
    "--------------\n",
    "\n",
    "Q: How does an HDFS client determine closeness?\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-closeness.png\">\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Can a DataNode be an HDFS client?\n",
    "</summary>\n",
    "1. Yes.<br>\n",
    "2. Any machine that wants to interact HDFS files is a client.<br>\n",
    "3. Closeness only makes sense for clients in the cluster.<br>\n",
    "</details>\n",
    "\n",
    "HDFS Read\n",
    "---------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-file-read.png\">\n",
    "\n",
    "HDFS Read Sequence\n",
    "------------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-file-read-seq.png\">\n",
    "\n",
    "HDFS Write \n",
    "----------\n",
    "\n",
    "Q: How does a client write a file to HDFS?\n",
    "\n",
    "- Client tells NameNode it wants to put file.\n",
    "\n",
    "- NameNode gives it list of 3 DataNodes for first block.\n",
    "\n",
    "- Client sends first chunk of first block to the DataNode 1.\n",
    "\n",
    "- DataNode 1 sends it to DataNode 2, which sends it to DataNode 3.\n",
    "\n",
    "- Each receiver acknowledges the chunk.\n",
    "\n",
    "- Client gets list of DataNodes for each block.\n",
    "\n",
    "Q: What if one or two DataNodes go down?\n",
    "\n",
    "- Client reconstitutes pipeline with surviving DataNodes. \n",
    "\n",
    "- After block uploaded NameNode tells new DataNode to replicate.\n",
    "\n",
    "HDFS Write \n",
    "----------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-file-write.png\">\n",
    "\n",
    "HDFS Write Sequence\n",
    "-------------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-file-write-seq.png\">\n",
    "\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Can the client read a file while it is being written?\n",
    "</summary>\n",
    "1. Yes.<br>\n",
    "2. The client will think it has read the file despite reading it partially.<br>\n",
    "3. Clients should coordinate to ensure writing is done before reading.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "Block Placement Policy\n",
    "----------------------\n",
    "\n",
    "Q: How does the NameNode decide where to place blocks?\n",
    "\n",
    "- If client is a DataNode then NameNode places the first replica on it.\n",
    "\n",
    "- Otherwise it places the first replica based on which DataNode has\n",
    "  heartbeated in and is least busy.\n",
    "\n",
    "- The second replica is placed on a different rack from the first\n",
    "  replica.\n",
    "\n",
    "- The third replica is placed in the same rack as the second but on a\n",
    "  different node.\n",
    "\n",
    "- Further replicas are placed on random nodes in the cluster.\n",
    "\n",
    "Block Placement Policy\n",
    "----------------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-block-placement-policy.png\">\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Suppose you notice that a particular DataNode keeps getting filled\n",
    "up with block replicas. What might be the problem?\n",
    "</summary>\n",
    "1. It is likely that there is a program on the DataNode that is\n",
    "writing files to HDFS.<br>\n",
    "2. If you run a program on the DataNode make sure that you round-robin\n",
    "it with all the DataNodes to ensure balancing.<br>\n",
    "</details>\n",
    "\n",
    "MapReduce\n",
    "=========\n",
    "\n",
    "MapReduce Intro\n",
    "---------------\n",
    "\n",
    "Q: What is MapReduce?\n",
    "\n",
    "- MapReduce is a system for processing data on HDFS.\n",
    "\n",
    "- In the *map* phase data is processed locally, with one *mapper* per\n",
    "  block.\n",
    "\n",
    "- In the *reduce* phase the results of the map phase are consolidated.\n",
    "\n",
    "Data Locality\n",
    "-------------\n",
    "\n",
    "Q: What is *data locality*?\n",
    "\n",
    "- Data locality is the secret sauce in HDFS and MapReduce. \n",
    "\n",
    "- Data locality means MapReduce runs mappers on locally machines with HDFS blocks.\n",
    "\n",
    "- When these machine are busy mappers may come up on other machines.\n",
    "\n",
    "\n",
    "Data Locality\n",
    "-------------\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/hdfs-map-reduce-data-locality.png\">\n",
    "\n",
    "\n",
    "MapReduce Examples\n",
    "------------------\n",
    "\n",
    "Q: How can I run MapReduce programs?\n",
    "\n",
    "- Hadoop ships with some MapReduce example programs.\n",
    "\n",
    "- The jar files are located at `/usr/lib/hadoop-mapreduce`.\n",
    "\n",
    "- Here is how you can get more information about them.\n",
    "\n",
    "Command                                                    |Result\n",
    "-------                                                    |------\n",
    "`hadoop jar /path/hadoop-mapreduce-examples-VER.jar`       |Lists all programs\n",
    "`hadoop jar /path/hadoop-mapreduce-examples-VER.jar sleep` |List usage of `sleep`\n",
    "\n",
    "Q: What are some of the interesting example programs?\n",
    "\n",
    "Program                |Description\n",
    "-------                |-----------\n",
    "`aggregatewordcount`   |Counts words in input\n",
    "`grep`                 |Counts matches of regex in input\n",
    "`pi`                   |Estimates Pi using monte-carlo method\n",
    "`sleep`                |Sleeps \n",
    "`sudoku`               |Sudoku solver\n",
    "`teragen`              |Generate data for terasort\n",
    "`terasort`             |Run terasort\n",
    "`teravalidate`         |Checking results of terasort\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of running `grep` as a MapReduce program?\n",
    "</summary>\n",
    "`grep` on MapReduce will scan blocks in parallel and complete\n",
    "faster.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
